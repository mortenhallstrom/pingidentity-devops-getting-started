{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Ping Identity DevOps \u00b6 We enable DevOps professionals, administrators and developers with tools, frameworks, blueprints and reference architectures to deploy Ping Identity software in the cloud. DevOps Resources \u00b6 Docker Images Github Repos Helm Charts Community Benefits of DevOps \u00b6 Streamlined Deployments Deploy and run workloads on our solutions without the need for additional hardware or VMs. Consistent and Flexible Maintain all configurations and dependencies, ensuring consistent environments. Containers are portable and can be used on nearly any machine. Optimized Sizing Orchestration of containers allows organizations to increase fault tolerance, availability, and better manage costs by auto-scaling to application demand. Overview \u00b6 See Overview for descriptions of the components of our DevOps architecture and repositories. Get Started \u00b6 See Get Started to quickly deploy a preconfigured DevOps image of a Ping Identity solution or integrated set of solutions. Contact Us \u00b6 If you find functionality missing that you believe may be of benefit to other Ping Identity customers: Log a GitHub Issue Ask a question on our Cloud DevOps Community Create a Ping Identity Support Case","title":"Home"},{"location":"#ping-identity-devops","text":"We enable DevOps professionals, administrators and developers with tools, frameworks, blueprints and reference architectures to deploy Ping Identity software in the cloud.","title":"Ping Identity DevOps"},{"location":"#devops-resources","text":"Docker Images Github Repos Helm Charts Community","title":"DevOps Resources"},{"location":"#benefits-of-devops","text":"Streamlined Deployments Deploy and run workloads on our solutions without the need for additional hardware or VMs. Consistent and Flexible Maintain all configurations and dependencies, ensuring consistent environments. Containers are portable and can be used on nearly any machine. Optimized Sizing Orchestration of containers allows organizations to increase fault tolerance, availability, and better manage costs by auto-scaling to application demand.","title":"Benefits of DevOps"},{"location":"#overview","text":"See Overview for descriptions of the components of our DevOps architecture and repositories.","title":"Overview"},{"location":"#get-started","text":"See Get Started to quickly deploy a preconfigured DevOps image of a Ping Identity solution or integrated set of solutions.","title":"Get Started"},{"location":"#contact-us","text":"If you find functionality missing that you believe may be of benefit to other Ping Identity customers: Log a GitHub Issue Ask a question on our Cloud DevOps Community Create a Ping Identity Support Case","title":"Contact Us"},{"location":"3rdPartySoftware/","text":"Third-Party Software \u00b6 Ping Identity Docker Images bundle various third-party tools to enable product functionality. Review the list below for references. OpenJDK . GNU General Public License version 2.0. OpenSSH . Based on BSD licensing. Git . GNU General Public License version 2.0. Gettext . GNU General Public License version 2.0. Curl . Based on MIT/X license. ca-certificates . GNU General Public License version 2.0. Jq . MIT licensing. Gnupg . GNU general Public License.","title":"Third-Party Software"},{"location":"3rdPartySoftware/#third-party-software","text":"Ping Identity Docker Images bundle various third-party tools to enable product functionality. Review the list below for references. OpenJDK . GNU General Public License version 2.0. OpenSSH . Based on BSD licensing. Git . GNU General Public License version 2.0. Gettext . GNU General Public License version 2.0. Curl . Based on MIT/X license. ca-certificates . GNU General Public License version 2.0. Jq . MIT licensing. Gnupg . GNU general Public License.","title":"Third-Party Software"},{"location":"contributing/","text":"Contributing \u00b6 Thanks for taking the time to contribute! How Can I Contribute? \u00b6 Reporting Bugs \u00b6 How Do I Submit a Bug Report? \u00b6 Bugs are tracked as GitHub Issues . You can report a bug by submitting an issue in the Ping Identity DevOps Issue Tracker . To help the maintainers understand and reproduce the problem, please try to provide information like the following: A clear and descriptive title. A description of what happened and a description of what you expected to happen. An example with the exact steps needed to reproduce the problem. If relevant, sample code is helpful. Please understand that bug reports are reviewed and prioritized internally, and we may not be able to address all bug reports or provide an estimated time for resolution. Suggesting Enhancements \u00b6 As with bugs, requests are tracked as GitHub Issues . You can suggest an enhancement by submitting an issue in the Ping Identity DevOps Issue Tracker . Please understand that enhancement requests are reviewed and prioritized internally, and we may not be able to address all requests or provide an estimated time for resolution. Alternate Routes for Submitting Bugs and Suggesting Enhancements \u00b6 If you would rather not have your issue discussed on the public repository, you can open an issue from Ping Identity's Support Portal . Contributing Code Changes \u00b6 Ping Identity does not accept third-party code submissions.","title":"Contributing"},{"location":"contributing/#contributing","text":"Thanks for taking the time to contribute!","title":"Contributing"},{"location":"contributing/#how-can-i-contribute","text":"","title":"How Can I Contribute?"},{"location":"contributing/#reporting-bugs","text":"","title":"Reporting Bugs"},{"location":"contributing/#how-do-i-submit-a-bug-report","text":"Bugs are tracked as GitHub Issues . You can report a bug by submitting an issue in the Ping Identity DevOps Issue Tracker . To help the maintainers understand and reproduce the problem, please try to provide information like the following: A clear and descriptive title. A description of what happened and a description of what you expected to happen. An example with the exact steps needed to reproduce the problem. If relevant, sample code is helpful. Please understand that bug reports are reviewed and prioritized internally, and we may not be able to address all bug reports or provide an estimated time for resolution.","title":"How Do I Submit a Bug Report?"},{"location":"contributing/#suggesting-enhancements","text":"As with bugs, requests are tracked as GitHub Issues . You can suggest an enhancement by submitting an issue in the Ping Identity DevOps Issue Tracker . Please understand that enhancement requests are reviewed and prioritized internally, and we may not be able to address all requests or provide an estimated time for resolution.","title":"Suggesting Enhancements"},{"location":"contributing/#alternate-routes-for-submitting-bugs-and-suggesting-enhancements","text":"If you would rather not have your issue discussed on the public repository, you can open an issue from Ping Identity's Support Portal .","title":"Alternate Routes for Submitting Bugs and Suggesting Enhancements"},{"location":"contributing/#contributing-code-changes","text":"Ping Identity does not accept third-party code submissions.","title":"Contributing Code Changes"},{"location":"disclaimer/","text":"Disclaimer \u00b6 Copyright (C) 2021 Ping Identity Corporation All rights reserved. Ping Identity Corporation 1099 18th St Suite 2950 Denver, CO 80202 303.468.2900 http://www.pingidentity.com Disclaimer Of Warranties \u00b6 THE SOFTWARE PROVIDED HEREUNDER IS PROVIDED ON AN \"AS IS\" BASIS, WITHOUT ANY WARRANTIES OR REPRESENTATIONS EXPRESS, IMPLIED OR STATUTORY; INCLUDING, WITHOUT LIMITATION, WARRANTIES OF QUALITY, PERFORMANCE, NONINFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE. NOR ARE THERE ANY WARRANTIES CREATED BY A COURSE OR DEALING, COURSE OF PERFORMANCE OR TRADE USAGE. FURTHERMORE, THERE ARE NO WARRANTIES THAT THE SOFTWARE WILL MEET YOUR NEEDS OR BE FREE FROM ERRORS, OR THAT THE OPERATION OF THE SOFTWARE WILL BE UNINTERRUPTED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"Disclaimer"},{"location":"disclaimer/#disclaimer","text":"Copyright (C) 2021 Ping Identity Corporation All rights reserved. Ping Identity Corporation 1099 18th St Suite 2950 Denver, CO 80202 303.468.2900 http://www.pingidentity.com","title":"Disclaimer"},{"location":"disclaimer/#disclaimer-of-warranties","text":"THE SOFTWARE PROVIDED HEREUNDER IS PROVIDED ON AN \"AS IS\" BASIS, WITHOUT ANY WARRANTIES OR REPRESENTATIONS EXPRESS, IMPLIED OR STATUTORY; INCLUDING, WITHOUT LIMITATION, WARRANTIES OF QUALITY, PERFORMANCE, NONINFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE. NOR ARE THERE ANY WARRANTIES CREATED BY A COURSE OR DEALING, COURSE OF PERFORMANCE OR TRADE USAGE. FURTHERMORE, THERE ARE NO WARRANTIES THAT THE SOFTWARE WILL MEET YOUR NEEDS OR BE FREE FROM ERRORS, OR THAT THE OPERATION OF THE SOFTWARE WILL BE UNINTERRUPTED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"Disclaimer Of Warranties"},{"location":"license/","text":"License \u00b6 Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"[]\" replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives. Copyright 2021 Ping Identity Corp. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"license/#license","text":"Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"[]\" replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives. Copyright 2021 Ping Identity Corp. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"overview/","text":"Overview \u00b6 The DevOps resources include Docker Images of Ping Identity products, deployment examples and configuration management tools. When you're ready, begin with our Get Started guide. Our documentation will help set you up and familiarize you with the use of the resources. DevOps Docker Images \u00b6 Docker Images Docker Builds We make available preconfigured Docker Images of our products in Docker containers. Each of our containers is a complete working product instance, immediately usable when deployed. Our Docker stacks are integrated collections of these containers, preconfigured to interoperate with the containers in the stack. You'll find information about our available Docker Images in the pingidentity-docker-builds repository or on our Docker Hub site. The Docker images are automatically pulled from our repository the first time you deploy a product container or orchestrated set of containers. Alternatively, you can pull the images from our Docker Hub site. Deployment Examples \u00b6 DevOps Getting Started We supply examples for deploying our products as standalone containers, as a Docker Compose stack, or as an orchestrated set using Kubernetes. Use Docker Compose for development, demonstrations, and lightweight orchestration. Use Kubernetes for enterprise-level orchestration. Configuration Management \u00b6 For configuration management, we use: Server profiles, for runtime configuration of containers. YAML files for runtime configuration of stacks. YAML file configuration settings complement that used for server profiles. Environment variables. These can be included in YAML files or called from external files. Shell scripts (hooks) to automate certain operations for a product. Release tags to give you a choice between stable builds or the current (potentially unstable) builds. By default, our Docker images run as root within the container. For instructions on how to change this, see Securing the Containers .","title":"Overview"},{"location":"overview/#overview","text":"The DevOps resources include Docker Images of Ping Identity products, deployment examples and configuration management tools. When you're ready, begin with our Get Started guide. Our documentation will help set you up and familiarize you with the use of the resources.","title":"Overview"},{"location":"overview/#devops-docker-images","text":"Docker Images Docker Builds We make available preconfigured Docker Images of our products in Docker containers. Each of our containers is a complete working product instance, immediately usable when deployed. Our Docker stacks are integrated collections of these containers, preconfigured to interoperate with the containers in the stack. You'll find information about our available Docker Images in the pingidentity-docker-builds repository or on our Docker Hub site. The Docker images are automatically pulled from our repository the first time you deploy a product container or orchestrated set of containers. Alternatively, you can pull the images from our Docker Hub site.","title":"DevOps Docker Images"},{"location":"overview/#deployment-examples","text":"DevOps Getting Started We supply examples for deploying our products as standalone containers, as a Docker Compose stack, or as an orchestrated set using Kubernetes. Use Docker Compose for development, demonstrations, and lightweight orchestration. Use Kubernetes for enterprise-level orchestration.","title":"Deployment Examples"},{"location":"overview/#configuration-management","text":"For configuration management, we use: Server profiles, for runtime configuration of containers. YAML files for runtime configuration of stacks. YAML file configuration settings complement that used for server profiles. Environment variables. These can be included in YAML files or called from external files. Shell scripts (hooks) to automate certain operations for a product. Release tags to give you a choice between stable builds or the current (potentially unstable) builds. By default, our Docker images run as root within the container. For instructions on how to change this, see Securing the Containers .","title":"Configuration Management"},{"location":"deployment/deploy/","text":"Working with DevOps Images \u00b6 After you've deployed a set of our DevOps Images using the full-stack server profile in Get Started , you're set up to move on to deployments using server profiles that may more closely reflect use cases you want to test out. Your choices at this point are: Continue working with the full-stack server profile in your local pingidentity-devops-getting-started/11-docker-compose/03-full-stack directory. Try our other server profiles in your local pingidentity-devops-getting-started directory to quickly deploy typical use cases. Clone the pingidentity-server-profiles repository to your local ${HOME}/projects/devops directory and discover the setup of specific product configurations.","title":"Introduction"},{"location":"deployment/deploy/#working-with-devops-images","text":"After you've deployed a set of our DevOps Images using the full-stack server profile in Get Started , you're set up to move on to deployments using server profiles that may more closely reflect use cases you want to test out. Your choices at this point are: Continue working with the full-stack server profile in your local pingidentity-devops-getting-started/11-docker-compose/03-full-stack directory. Try our other server profiles in your local pingidentity-devops-getting-started directory to quickly deploy typical use cases. Clone the pingidentity-server-profiles repository to your local ${HOME}/projects/devops directory and discover the setup of specific product configurations.","title":"Working with DevOps Images"},{"location":"deployment/deployCompose/","text":"Deploy with Docker-Compose \u00b6 We use Docker Compose for light orchestration of containers, deploying a stack of containers quickly, on a single host, based on configurations specified in YAML files. Try the examples for Docker Compose in your local pingidentity-devops-getting-started/11-docker-compose directory to easily deploy typical use cases.","title":"Introduction"},{"location":"deployment/deployCompose/#deploy-with-docker-compose","text":"We use Docker Compose for light orchestration of containers, deploying a stack of containers quickly, on a single host, based on configurations specified in YAML files. Try the examples for Docker Compose in your local pingidentity-devops-getting-started/11-docker-compose directory to easily deploy typical use cases.","title":"Deploy with Docker-Compose"},{"location":"deployment/deployHelm/","text":"Deploy Ping DevOps Charts using Helm \u00b6 Helm Charts Repo Helm is a package deployment tool for Kubernetes. It can be used with PingDevops to deploy all the components of the Solution with a simple command. To get started, complete the following steps: Inject your Ping DevOps secrets. There are a couple of options for injecting a license. Evaluation License - Use your PING_IDENTITY_DEVOPS_USER/PING_IDENTITY_DEVOPS_KEY credentials along with your PING_IDENTITY_ACCEPT_EULA setting. For more information on obtaining credentials click here . For more information on using ping-devops utility click here . ping-devops generate devops-secret | kubectl -apply -f - Install Helm Installing on MacOS (or linux with brew) brew install helm Installing on other OS - https://helm.sh/docs/intro/install/ Add Helm Ping DevOps Repo helm repo add pingidentity https://helm.pingidentity.com/devops/ List Ping DevOps Charts helm search repo pingidentity Update local machine with latest charts helm repo update Install a Ping DevOps Chart Install a chart using the helm install {release} {chart} ... using the example below. In this case, it is installing a pingfederate-admin chart with the release name of pf . helm install pf pingidentity/pingfederate-admin or, if you have a devops-values.yaml file to include: helm install pf ping-devops/pingfederate-admin -f devops-values.yaml Accessing Deployments \u00b6 Components of the release will be prefixed with pf . Use kubectl to access the workloads: View kubernetes resources installed kubectl get all # or get even more (ing, pvc) kubectl get pods,svc,deploy,rs,sts,job,ing,pvc View Logs: # kubectl logs -f pod/{release}-{product}-... kubectl logs -f pod/pf-pingfederate-admin-... Uninstalling Deployment \u00b6 To uninstall a release from helm, use the following helm uninstall command: helm uninstall pf","title":"Deploy with PingIdentiy Helm Charts"},{"location":"deployment/deployHelm/#deploy-ping-devops-charts-using-helm","text":"Helm Charts Repo Helm is a package deployment tool for Kubernetes. It can be used with PingDevops to deploy all the components of the Solution with a simple command. To get started, complete the following steps: Inject your Ping DevOps secrets. There are a couple of options for injecting a license. Evaluation License - Use your PING_IDENTITY_DEVOPS_USER/PING_IDENTITY_DEVOPS_KEY credentials along with your PING_IDENTITY_ACCEPT_EULA setting. For more information on obtaining credentials click here . For more information on using ping-devops utility click here . ping-devops generate devops-secret | kubectl -apply -f - Install Helm Installing on MacOS (or linux with brew) brew install helm Installing on other OS - https://helm.sh/docs/intro/install/ Add Helm Ping DevOps Repo helm repo add pingidentity https://helm.pingidentity.com/devops/ List Ping DevOps Charts helm search repo pingidentity Update local machine with latest charts helm repo update Install a Ping DevOps Chart Install a chart using the helm install {release} {chart} ... using the example below. In this case, it is installing a pingfederate-admin chart with the release name of pf . helm install pf pingidentity/pingfederate-admin or, if you have a devops-values.yaml file to include: helm install pf ping-devops/pingfederate-admin -f devops-values.yaml","title":"Deploy Ping DevOps Charts using Helm"},{"location":"deployment/deployHelm/#accessing-deployments","text":"Components of the release will be prefixed with pf . Use kubectl to access the workloads: View kubernetes resources installed kubectl get all # or get even more (ing, pvc) kubectl get pods,svc,deploy,rs,sts,job,ing,pvc View Logs: # kubectl logs -f pod/{release}-{product}-... kubectl logs -f pod/pf-pingfederate-admin-...","title":"Accessing Deployments"},{"location":"deployment/deployHelm/#uninstalling-deployment","text":"To uninstall a release from helm, use the following helm uninstall command: helm uninstall pf","title":"Uninstalling Deployment"},{"location":"deployment/deployK8s-AKS/","text":"Deploy to Azure Kubernetes Service \u00b6 This directory contains scripts and deployment files to help with the deployment, management and scaling of Ping Identity DevOps Docker Images to Microsoft Azure Kubernetes Service (AKS). Prerequisites \u00b6 You've already been through Get Started to set up your DevOps environment and run a test deployment of the products. You've created a Kubernetes cluster on AKS. You've created a Kubernetes secret using your DevOps credentials. See the For Kubernetes topic in Using your DevOps user and key . You've downloaded and installed the Azure CLI . We also highly recommend you are familiar with the information in these AKS articles: Azure Kubernetes Service Deploy our fullstack example in AKS \u00b6 Create an Azure Resource Group to put all resources into. Enter: az group create \\ --name ping-devops-rg \\ --location westus Create an Azure AKS cluster. You'll create a 2 node cluster. You need a public certificate, by default in ~/.ssh/id_rsa.pub. Enter: az aks create \\ --resource-group ping-devops-rg \\ --name ping-devops-cluster \\ --node-count 2 \\ --enable-addons monitoring \\ --ssh-key-value ~/.ssh/id_rsa.pub Get the AKS Credentials into .kube/config . Enter: az aks get-credentials \\ --resource-group ping-devops-rg \\ --name ping-devops-cluster From your local pingidentity-devops-getting-started/20-kubernetes/02-fullstack directory, start our fullstack example in AKS. Enter: kustomize build . | kubectl apply -f - To display the status of the environment, enter: kubectl get all To clean up the environment, enter: kustomize build . | kubectl delete -f - To clean up the Azure Resource Group and all associated resources, including the AKS cluster created, enter: Caution : This will remove everything you created that is associated with this resource group. az group delete \\ --name ping-devops-rg","title":"Deploy to Azure Kubernetes Service"},{"location":"deployment/deployK8s-AKS/#deploy-to-azure-kubernetes-service","text":"This directory contains scripts and deployment files to help with the deployment, management and scaling of Ping Identity DevOps Docker Images to Microsoft Azure Kubernetes Service (AKS).","title":"Deploy to Azure Kubernetes Service"},{"location":"deployment/deployK8s-AKS/#prerequisites","text":"You've already been through Get Started to set up your DevOps environment and run a test deployment of the products. You've created a Kubernetes cluster on AKS. You've created a Kubernetes secret using your DevOps credentials. See the For Kubernetes topic in Using your DevOps user and key . You've downloaded and installed the Azure CLI . We also highly recommend you are familiar with the information in these AKS articles: Azure Kubernetes Service","title":"Prerequisites"},{"location":"deployment/deployK8s-AKS/#deploy-our-fullstack-example-in-aks","text":"Create an Azure Resource Group to put all resources into. Enter: az group create \\ --name ping-devops-rg \\ --location westus Create an Azure AKS cluster. You'll create a 2 node cluster. You need a public certificate, by default in ~/.ssh/id_rsa.pub. Enter: az aks create \\ --resource-group ping-devops-rg \\ --name ping-devops-cluster \\ --node-count 2 \\ --enable-addons monitoring \\ --ssh-key-value ~/.ssh/id_rsa.pub Get the AKS Credentials into .kube/config . Enter: az aks get-credentials \\ --resource-group ping-devops-rg \\ --name ping-devops-cluster From your local pingidentity-devops-getting-started/20-kubernetes/02-fullstack directory, start our fullstack example in AKS. Enter: kustomize build . | kubectl apply -f - To display the status of the environment, enter: kubectl get all To clean up the environment, enter: kustomize build . | kubectl delete -f - To clean up the Azure Resource Group and all associated resources, including the AKS cluster created, enter: Caution : This will remove everything you created that is associated with this resource group. az group delete \\ --name ping-devops-rg","title":"Deploy our fullstack example in AKS"},{"location":"deployment/deployK8s-AWS/","text":"Prepare AWS EKS for Multi-Region Deployments \u00b6 In this example, we'll deploy 2 Kubernetes clusters, each in a different Amazon Web Services (AWS) region. An AWS virtual private cloud (VPC) is assigned and dedicated to each cluster. Throughout this document, \"VPC\" is synonymous with \"cluster\". Prerequisites \u00b6 AWS CLI . eksctl , the current version. AWS account permissions to create clusters. Configure the AWS CLI \u00b6 If you've not already done so, configure the AWS CLI to use your profile and credentials: Assign your profile and supply your aws_access_key_id and aws_secret_access_key . Enter: aws configure --profile = <aws-profile> Then enter your aws_access_key_id and aws_secret_access_key . Open your ~/.aws/credentials file in a text editor and add your AWS role_arn . For example: \u201crole_arn = arn:aws:iam::xxxxxxxx4146:role/xxx\u201d Create the multi-region clusters \u00b6 Create the YAML files to configure the the clusters. You'll create the clusters in different AWS regions. We'll be using the ca-central-1 region and the us-west-2 region in this document. a. Configure the first cluster. For example, using the ca-central-1 region and the reserved CIDR 172.16.0.0: apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : pingfed-ca-central-1 region : ca-central-1 version : \"1.17\" vpc : cidr : 172.16.0.0/16 managedNodeGroups : - name : us-west-2a-worker-nodes instanceType : t3a.2xlarge labels : {} tags : {} minSize : 1 maxSize : 2 desiredCapacity : 1 volumeSize : 12 privateNetworking : true ssh : publicKeyPath : ~/.ssh/id_rsa.pub iam : withAddonPolicies : imageBuilder : true autoScaler : true externalDNS : true certManager : true appMesh : true ebs : true fsx : true efs : true albIngress : true xRay : true cloudWatch : true - name : us-west-2b-worker-nodes instanceType : t3a.2xlarge labels : {} tags : {} minSize : 1 maxSize : 2 desiredCapacity : 1 volumeSize : 12 privateNetworking : true ssh : publicKeyPath : ~/.ssh/id_rsa.pub iam : withAddonPolicies : imageBuilder : true autoScaler : true externalDNS : true certManager : true appMesh : true ebs : true fsx : true efs : true albIngress : true xRay : true cloudWatch : true For production purposes, select a VPC with a private IP. The ssh entry is optional, allowing you to SSH in to your cluster. b. Configure the second cluster. For example, using the us-west-2 region and the reserved CIDR 10.0.0.0: apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : pingfed-us-west-2 region : us-west-2 version : \"1.17\" vpc : cidr : 10.0.0.0/16 managedNodeGroups : - name : us-west-2a-worker-nodes instanceType : t3a.2xlarge labels : {} tags : {} minSize : 1 maxSize : 2 desiredCapacity : 1 volumeSize : 12 privateNetworking : true ssh : publicKeyPath : ~/.ssh/id_rsa.pub iam : withAddonPolicies : imageBuilder : true autoScaler : true externalDNS : true certManager : true appMesh : true ebs : true fsx : true efs : true albIngress : true xRay : true cloudWatch : true - name : us-west-2b-worker-nodes instanceType : t3a.2xlarge labels : {} tags : {} minSize : 1 maxSize : 2 desiredCapacity : 1 volumeSize : 12 privateNetworking : true ssh : publicKeyPath : ~/.ssh/id_rsa.pub iam : withAddonPolicies : imageBuilder : true autoScaler : true externalDNS : true certManager : true appMesh : true ebs : true fsx : true efs : true albIngress : true xRay : true cloudWatch : true For production purposes, select a VPC with a private IP. The ssh entry is optional, allowing you to SSH in to your cluster. Create the clusters using eksctl . a. Create the first cluster. For example: eksctl create cluster -f ca-central-1.yaml --profile <aws-profile> b. Create the second cluster. For example: eksctl create cluster -f us-west-2.yaml --profile <aws-profile> Log in to the AWS console, go to the VPC service, select Your VPCs (under Virtual Private Cloud), and note the VPC details for the clusters you've created. Retain the VpcId values for the ca-central-1 and us-west-2 VPCs. You'll use these in subsequent steps. Set up VPC peering between the two clusters. You'll create a peering connection from the cluster in the us-west-2 region to the cluster in the ca-central-1 region. You'll do this from the VPC Dashboard as in the prior step. a. In the top right of the page, select the Oregon (us-west-2) region. b. Select Peering Connections , and click Create Peering Connection . c. Assign a unique name for the peering connection (for example, us-west-2-to-ca-central-1). d. Under Select a local VPC to peer with , enter the VpcId value for the us-west-2 VPC. e. Under Select another VPC to peer with , select My account --> Another region --> Canada Central (ca-central-1). f. Under VPC (Accepter) , enter the VpcId value for the ca-central-1 region. g. Click Create Peering Connection . When successful, a confirmation is displayed. Click OK to continue. h. In the top right of the page, change the region to Canada Central . i. Select Peering Connections . Notice that the peering connection status for us-west-2 shows as Pending Acceptance . j. Select the ca-central-1 connection, click the Actions dropdown list, and select Accept Request . You'll be prompted to confirm. The VPC peering connection status should now show as Active . Get the subnets information for each cluster node. Each cluster node uses a different subnet, so there'll be three subnets assigned to each VPC. The information displayed will contain the subnet ID for each subnet. You'll use the subnet IDs in the subsequent step to get the associated routing tables. a. In the top right of the page, change the region to Oregon . b. Go to the EC2 service, and select Instances . Apply a filter, if needed, to find your nodes for the cluster. c. Select each node, and record the Subnet ID of each. You'll use the subnet IDs in a subsequent step. d. In the top right of the page, change the region to Canada Central , and repeat the 2 previous steps to find and record the subnet IDs for this VPC. Get the routing table associated with the subnets for each VPC. a. Go to the VPC service. (You're still using the Canada Central region.) b. In the VPC Dashboard, select Subnets . c. For each subnet displayed, record the Routing Table value. You may have a single routing table for all of your subnets. You'll use the routing table ID or IDs in a subsequent step. d. In the top right of the page, change the region to Oregon , and repeat the 2 previous steps to find and record the routing table ID or IDs for this VPC. Modify the routing table or tables for each VPC to add a route to the other VPC using the peering connection you created. a. In the VPC Dashboard, select Route Tables . (You're still using the Oregon region.) b. Select the route table you recorded for the us-west-2 (Oregon) VPC, and click the Routes button. You should see 2 routes displayed. c. Click Edit Routes --> Add Route , and for Destination , enter the CIDR block for the ca-central-1 cluster (172.16.0.0/16). d. For Target , select the VPC peering connection you created in a prior step. Click Save Routes . A route for the ` ca - central - 1 ` cluster directed to the peering connection is displayed . e. If more than one routing table is used for the us-west-2 VPC, repeat the previous steps for each routing table. f. In the top right of the page, change the region to Canada Central . g. Select the route table you recorded for the ca-central-1 (Canada Central) VPC, and click the Routes button. You should see 2 routes displayed. h. Click Edit Routes --> Add Route , and for Destination , enter the CIDR block for the us-west-2 cluster (10.0.0.0/16). i. For Target , select the VPC peering connection you created in a prior step. Click Save Routes . A route for the ` us - west - 2 ` cluster directed to the peering connection is displayed . j. If more than one routing table is used for the ca-central-1 VPC, repeat the previous steps for each routing table. Update the Security Groups for each VPC. You'll get the Security Group IDs for each VPC, then add inbound and outbound rules for both the us-west-2 VPC, and the ca-central-1 VPC. a. In the VPC Dashboard, select Security Groups . (You're still using the Canada Central region.) b. Apply a filter to find the security groups for the ca-central-1 cluster, and select the security group with \u201c-nodegroup\u201d in the name. This is the security group used for the firewall settings for all the worker nodes in the ca-central-1 cluster. c. Click Inbound Rules --> Add Rule . d. Select these values for the rule: Type: Custom TCP Rule Protocol: TCP Port Range: 7600-7700 > These ports are for are specific to PingFederate Clustering, adjust based on your products. Source: Custom, and enter the CIDR block for the us-west-2 (10.0.0.0/16) cluster. e. Click Save Rules to save the inbound security group rule for the ca-central-1 cluster. f. Click Outbound Rules --> Add Rule . g. Select these values for the rule: Type: Custom TCP Rule Protocol: TCP Port Range: 7600-7700 > These ports are for are specific to PingFederate Clustering, adjust based on your products. Source: Custom, and enter the CIDR block for the us-west-2 (10.0.0.0/16) cluster. h. Click Save Rules to save the outbound security group rule for the ca-central-1 cluster. i. In the top right of the page, change the region to Oregon . You'll now repeat the previous steps to add inbound and outbound rules for the us-west-2 cluster. j. Apply a filter to find the security groups for the us-west-2 cluster, and select the security group with \u201c-nodegroup\u201d in the name. This is the security group used for the firewall settings for all the worker nodes in the us-west-2 cluster. k. Click Inbound Rules --> Add Rule . l. Select these values for the rule: Type: Custom TCP Rule Protocol: TCP Port Range: 7600-7700 > These ports are for are specific to PingFederate Clustering, adjust based on your products. Source: Custom, and enter the CIDR block for the ca-central-1 (172.16.0.0/16) cluster. m. Click Save Rules to save the inbound security group rule for the us-west-2 cluster. n. Click Outbound Rules --> Add Rule . o. Select these values for the rule: Type: Custom TCP Rule Protocol: TCP Port Range: 7600-7700 > These ports are for are specific to PingFederate Clustering, adjust based on your products. Source: Custom, and enter the CIDR block for the ca-central-1 (172.16.0.0/16) cluster. p. Click Save Rules to save the outbound security group rule for the us-west-2 cluster.","title":"Deploy Peered EKS Clusters"},{"location":"deployment/deployK8s-AWS/#prepare-aws-eks-for-multi-region-deployments","text":"In this example, we'll deploy 2 Kubernetes clusters, each in a different Amazon Web Services (AWS) region. An AWS virtual private cloud (VPC) is assigned and dedicated to each cluster. Throughout this document, \"VPC\" is synonymous with \"cluster\".","title":"Prepare AWS EKS for Multi-Region Deployments"},{"location":"deployment/deployK8s-AWS/#prerequisites","text":"AWS CLI . eksctl , the current version. AWS account permissions to create clusters.","title":"Prerequisites"},{"location":"deployment/deployK8s-AWS/#configure-the-aws-cli","text":"If you've not already done so, configure the AWS CLI to use your profile and credentials: Assign your profile and supply your aws_access_key_id and aws_secret_access_key . Enter: aws configure --profile = <aws-profile> Then enter your aws_access_key_id and aws_secret_access_key . Open your ~/.aws/credentials file in a text editor and add your AWS role_arn . For example: \u201crole_arn = arn:aws:iam::xxxxxxxx4146:role/xxx\u201d","title":"Configure the AWS CLI"},{"location":"deployment/deployK8s-AWS/#create-the-multi-region-clusters","text":"Create the YAML files to configure the the clusters. You'll create the clusters in different AWS regions. We'll be using the ca-central-1 region and the us-west-2 region in this document. a. Configure the first cluster. For example, using the ca-central-1 region and the reserved CIDR 172.16.0.0: apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : pingfed-ca-central-1 region : ca-central-1 version : \"1.17\" vpc : cidr : 172.16.0.0/16 managedNodeGroups : - name : us-west-2a-worker-nodes instanceType : t3a.2xlarge labels : {} tags : {} minSize : 1 maxSize : 2 desiredCapacity : 1 volumeSize : 12 privateNetworking : true ssh : publicKeyPath : ~/.ssh/id_rsa.pub iam : withAddonPolicies : imageBuilder : true autoScaler : true externalDNS : true certManager : true appMesh : true ebs : true fsx : true efs : true albIngress : true xRay : true cloudWatch : true - name : us-west-2b-worker-nodes instanceType : t3a.2xlarge labels : {} tags : {} minSize : 1 maxSize : 2 desiredCapacity : 1 volumeSize : 12 privateNetworking : true ssh : publicKeyPath : ~/.ssh/id_rsa.pub iam : withAddonPolicies : imageBuilder : true autoScaler : true externalDNS : true certManager : true appMesh : true ebs : true fsx : true efs : true albIngress : true xRay : true cloudWatch : true For production purposes, select a VPC with a private IP. The ssh entry is optional, allowing you to SSH in to your cluster. b. Configure the second cluster. For example, using the us-west-2 region and the reserved CIDR 10.0.0.0: apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : pingfed-us-west-2 region : us-west-2 version : \"1.17\" vpc : cidr : 10.0.0.0/16 managedNodeGroups : - name : us-west-2a-worker-nodes instanceType : t3a.2xlarge labels : {} tags : {} minSize : 1 maxSize : 2 desiredCapacity : 1 volumeSize : 12 privateNetworking : true ssh : publicKeyPath : ~/.ssh/id_rsa.pub iam : withAddonPolicies : imageBuilder : true autoScaler : true externalDNS : true certManager : true appMesh : true ebs : true fsx : true efs : true albIngress : true xRay : true cloudWatch : true - name : us-west-2b-worker-nodes instanceType : t3a.2xlarge labels : {} tags : {} minSize : 1 maxSize : 2 desiredCapacity : 1 volumeSize : 12 privateNetworking : true ssh : publicKeyPath : ~/.ssh/id_rsa.pub iam : withAddonPolicies : imageBuilder : true autoScaler : true externalDNS : true certManager : true appMesh : true ebs : true fsx : true efs : true albIngress : true xRay : true cloudWatch : true For production purposes, select a VPC with a private IP. The ssh entry is optional, allowing you to SSH in to your cluster. Create the clusters using eksctl . a. Create the first cluster. For example: eksctl create cluster -f ca-central-1.yaml --profile <aws-profile> b. Create the second cluster. For example: eksctl create cluster -f us-west-2.yaml --profile <aws-profile> Log in to the AWS console, go to the VPC service, select Your VPCs (under Virtual Private Cloud), and note the VPC details for the clusters you've created. Retain the VpcId values for the ca-central-1 and us-west-2 VPCs. You'll use these in subsequent steps. Set up VPC peering between the two clusters. You'll create a peering connection from the cluster in the us-west-2 region to the cluster in the ca-central-1 region. You'll do this from the VPC Dashboard as in the prior step. a. In the top right of the page, select the Oregon (us-west-2) region. b. Select Peering Connections , and click Create Peering Connection . c. Assign a unique name for the peering connection (for example, us-west-2-to-ca-central-1). d. Under Select a local VPC to peer with , enter the VpcId value for the us-west-2 VPC. e. Under Select another VPC to peer with , select My account --> Another region --> Canada Central (ca-central-1). f. Under VPC (Accepter) , enter the VpcId value for the ca-central-1 region. g. Click Create Peering Connection . When successful, a confirmation is displayed. Click OK to continue. h. In the top right of the page, change the region to Canada Central . i. Select Peering Connections . Notice that the peering connection status for us-west-2 shows as Pending Acceptance . j. Select the ca-central-1 connection, click the Actions dropdown list, and select Accept Request . You'll be prompted to confirm. The VPC peering connection status should now show as Active . Get the subnets information for each cluster node. Each cluster node uses a different subnet, so there'll be three subnets assigned to each VPC. The information displayed will contain the subnet ID for each subnet. You'll use the subnet IDs in the subsequent step to get the associated routing tables. a. In the top right of the page, change the region to Oregon . b. Go to the EC2 service, and select Instances . Apply a filter, if needed, to find your nodes for the cluster. c. Select each node, and record the Subnet ID of each. You'll use the subnet IDs in a subsequent step. d. In the top right of the page, change the region to Canada Central , and repeat the 2 previous steps to find and record the subnet IDs for this VPC. Get the routing table associated with the subnets for each VPC. a. Go to the VPC service. (You're still using the Canada Central region.) b. In the VPC Dashboard, select Subnets . c. For each subnet displayed, record the Routing Table value. You may have a single routing table for all of your subnets. You'll use the routing table ID or IDs in a subsequent step. d. In the top right of the page, change the region to Oregon , and repeat the 2 previous steps to find and record the routing table ID or IDs for this VPC. Modify the routing table or tables for each VPC to add a route to the other VPC using the peering connection you created. a. In the VPC Dashboard, select Route Tables . (You're still using the Oregon region.) b. Select the route table you recorded for the us-west-2 (Oregon) VPC, and click the Routes button. You should see 2 routes displayed. c. Click Edit Routes --> Add Route , and for Destination , enter the CIDR block for the ca-central-1 cluster (172.16.0.0/16). d. For Target , select the VPC peering connection you created in a prior step. Click Save Routes . A route for the ` ca - central - 1 ` cluster directed to the peering connection is displayed . e. If more than one routing table is used for the us-west-2 VPC, repeat the previous steps for each routing table. f. In the top right of the page, change the region to Canada Central . g. Select the route table you recorded for the ca-central-1 (Canada Central) VPC, and click the Routes button. You should see 2 routes displayed. h. Click Edit Routes --> Add Route , and for Destination , enter the CIDR block for the us-west-2 cluster (10.0.0.0/16). i. For Target , select the VPC peering connection you created in a prior step. Click Save Routes . A route for the ` us - west - 2 ` cluster directed to the peering connection is displayed . j. If more than one routing table is used for the ca-central-1 VPC, repeat the previous steps for each routing table. Update the Security Groups for each VPC. You'll get the Security Group IDs for each VPC, then add inbound and outbound rules for both the us-west-2 VPC, and the ca-central-1 VPC. a. In the VPC Dashboard, select Security Groups . (You're still using the Canada Central region.) b. Apply a filter to find the security groups for the ca-central-1 cluster, and select the security group with \u201c-nodegroup\u201d in the name. This is the security group used for the firewall settings for all the worker nodes in the ca-central-1 cluster. c. Click Inbound Rules --> Add Rule . d. Select these values for the rule: Type: Custom TCP Rule Protocol: TCP Port Range: 7600-7700 > These ports are for are specific to PingFederate Clustering, adjust based on your products. Source: Custom, and enter the CIDR block for the us-west-2 (10.0.0.0/16) cluster. e. Click Save Rules to save the inbound security group rule for the ca-central-1 cluster. f. Click Outbound Rules --> Add Rule . g. Select these values for the rule: Type: Custom TCP Rule Protocol: TCP Port Range: 7600-7700 > These ports are for are specific to PingFederate Clustering, adjust based on your products. Source: Custom, and enter the CIDR block for the us-west-2 (10.0.0.0/16) cluster. h. Click Save Rules to save the outbound security group rule for the ca-central-1 cluster. i. In the top right of the page, change the region to Oregon . You'll now repeat the previous steps to add inbound and outbound rules for the us-west-2 cluster. j. Apply a filter to find the security groups for the us-west-2 cluster, and select the security group with \u201c-nodegroup\u201d in the name. This is the security group used for the firewall settings for all the worker nodes in the us-west-2 cluster. k. Click Inbound Rules --> Add Rule . l. Select these values for the rule: Type: Custom TCP Rule Protocol: TCP Port Range: 7600-7700 > These ports are for are specific to PingFederate Clustering, adjust based on your products. Source: Custom, and enter the CIDR block for the ca-central-1 (172.16.0.0/16) cluster. m. Click Save Rules to save the inbound security group rule for the us-west-2 cluster. n. Click Outbound Rules --> Add Rule . o. Select these values for the rule: Type: Custom TCP Rule Protocol: TCP Port Range: 7600-7700 > These ports are for are specific to PingFederate Clustering, adjust based on your products. Source: Custom, and enter the CIDR block for the ca-central-1 (172.16.0.0/16) cluster. p. Click Save Rules to save the outbound security group rule for the us-west-2 cluster.","title":"Create the multi-region clusters"},{"location":"deployment/deployK8s/","text":"Orchestrate Deployments with Kubernetes \u00b6 The Kubernetes examples contain configurations and scripts to orchestrate deployments of our DevOps Images for: Kubernetes For General Use . Kubernetes For Cloud Platforms (such as, Amazon Web Services).","title":"Introduction"},{"location":"deployment/deployK8s/#orchestrate-deployments-with-kubernetes","text":"The Kubernetes examples contain configurations and scripts to orchestrate deployments of our DevOps Images for: Kubernetes For General Use . Kubernetes For Cloud Platforms (such as, Amazon Web Services).","title":"Orchestrate Deployments with Kubernetes"},{"location":"deployment/deployK8sCloud/","text":"Kubernetes deployments for cloud platforms \u00b6 We currently have instructions and examples for deploying our product containers using Kubernetes on these platforms: Amazon Web Services (AWS) Elastic Kubernetes Service (EKS). Microsoft Azure Kubernetes Service (AKS). Each hosting platform supports and manages Kubernetes differently. Prerequisites \u00b6 You've already been through Get Started to set up your DevOps environment and run a test deployment of the products. You've created a Kubernetes cluster on one of these platforms: Amazon's AWS EKS. Microsoft's AKS. You've created a Kubernetes secret using your DevOps credentials. See the For Kubernetes topic in Using your DevOps user and key . AWS EKS \u00b6 See Deploy Peered EKS Clusters . AKS \u00b6 See Deploy to Azure Kubernetes Service .","title":"Introduction"},{"location":"deployment/deployK8sCloud/#kubernetes-deployments-for-cloud-platforms","text":"We currently have instructions and examples for deploying our product containers using Kubernetes on these platforms: Amazon Web Services (AWS) Elastic Kubernetes Service (EKS). Microsoft Azure Kubernetes Service (AKS). Each hosting platform supports and manages Kubernetes differently.","title":"Kubernetes deployments for cloud platforms"},{"location":"deployment/deployK8sCloud/#prerequisites","text":"You've already been through Get Started to set up your DevOps environment and run a test deployment of the products. You've created a Kubernetes cluster on one of these platforms: Amazon's AWS EKS. Microsoft's AKS. You've created a Kubernetes secret using your DevOps credentials. See the For Kubernetes topic in Using your DevOps user and key .","title":"Prerequisites"},{"location":"deployment/deployK8sCloud/#aws-eks","text":"See Deploy Peered EKS Clusters .","title":"AWS EKS"},{"location":"deployment/deployK8sCloud/#aks","text":"See Deploy to Azure Kubernetes Service .","title":"AKS"},{"location":"deployment/deployK8sFullstack/","text":"Orchestrate a Full Stack Deployment \u00b6 You'll use kustomize for the full stack deployment from your local pingidentity-devops-getting-started/20-kubernetes/02-fullstack directory (the location of the YAML files), and call into your local pingidentity-devops-getting-started/20-kubernetes/01-standalone directory for the base product configurations. However, this time we'll use the server profiles in our pingidentity-server-profiles/baseline repository, rather than the pingidentity-server-profiles/getting-started repository, as we did for the standalone deployments. The env_vars.* files contain the environment variables for pingidentity-server-profiles/baseline . For example: SERVER_PROFILE_URL=https://www.github.com/pingidentity/pingidentity-server-profiles.git SERVER_PROFILE_PATH=baseline/pingaccess PING_IDENTITY_ACCEPT_EULA=YES kustomization.yaml does the following: References your local pingidentity-devops-getting-started/20-kubernetes/01-standalone directory for the base product configurations. Replaces the environment variables in the parent configMap with those in the specified env_vars.* files. Deploy the Stack \u00b6 To orchestrate the full stack deployment, from your local pingidentity-devops-getting-started/20-kubernetes directory, enter: kustomize build . | kubectl apply -f - Optionally, if you don't want to deploy everything, first comment out what you don't want in the kustomization.yaml file. Clean Up \u00b6 To clean up when you're finished, enter: kustomize build . | kubectl delete -f -","title":"Orchestrate a Full Stack Deployment"},{"location":"deployment/deployK8sFullstack/#orchestrate-a-full-stack-deployment","text":"You'll use kustomize for the full stack deployment from your local pingidentity-devops-getting-started/20-kubernetes/02-fullstack directory (the location of the YAML files), and call into your local pingidentity-devops-getting-started/20-kubernetes/01-standalone directory for the base product configurations. However, this time we'll use the server profiles in our pingidentity-server-profiles/baseline repository, rather than the pingidentity-server-profiles/getting-started repository, as we did for the standalone deployments. The env_vars.* files contain the environment variables for pingidentity-server-profiles/baseline . For example: SERVER_PROFILE_URL=https://www.github.com/pingidentity/pingidentity-server-profiles.git SERVER_PROFILE_PATH=baseline/pingaccess PING_IDENTITY_ACCEPT_EULA=YES kustomization.yaml does the following: References your local pingidentity-devops-getting-started/20-kubernetes/01-standalone directory for the base product configurations. Replaces the environment variables in the parent configMap with those in the specified env_vars.* files.","title":"Orchestrate a Full Stack Deployment"},{"location":"deployment/deployK8sFullstack/#deploy-the-stack","text":"To orchestrate the full stack deployment, from your local pingidentity-devops-getting-started/20-kubernetes directory, enter: kustomize build . | kubectl apply -f - Optionally, if you don't want to deploy everything, first comment out what you don't want in the kustomization.yaml file.","title":"Deploy the Stack"},{"location":"deployment/deployK8sFullstack/#clean-up","text":"To clean up when you're finished, enter: kustomize build . | kubectl delete -f -","title":"Clean Up"},{"location":"deployment/deployK8sGeneral/","text":"Kubernetes Deployments for General Use \u00b6 In all of these examples, we'll use the standalone configurations in your local pingidentity-devops-getting-started/20-kubernetes/01-standalone directory to supply the base product configurations. To minimize repetitive and irrelevant information, we'll also use kustomize . For effective use of the examples, we recommended that you be familiar with concepts such as \"resources\" and \"patches\" in kustomize . You'll find useful comments in the kustomization.yaml files in your local pingidentity-devops-getting-started/20-kubernetes example directories. Prerequisites \u00b6 You've already been through Get Started to set up your DevOps environment and run a test deployment of the products. You've created a Kubernetes secret using your DevOps credentials. See the For Kubernetes topic in Using your DevOps user and key . For the PingFederate cluster: envsubst . Substitutes shell format strings with environment variables. See envsubst if your OS doesn't have this utility. PingFederate build image for version 10 or greater. (The DNS Discovery feature first available in version 10 is needed.) What you'll do \u00b6 Install kustomize. Choose one or more of these examples to deploy: Orchestrate standalone deployments. Orchestrate a full stack deployment. Orchestrate a replicated PingDirectory deployment. Orchestrate a clustered PingAccess deployment. Orchestrate a clustered PingFederate deployment. Expose the applications Install kustomize \u00b6 Install kustomize . To view standard YAML outputs in each directory, run commands like: kustomize build <path/to/directory> For the fullstack configuration (pingidentity-devops-getting-started/20-kubernetes/02-fullstack), for example, you might use: kustomize build ./20-kubernetes/02-fullstack > ./output.yaml This builds and redirects the output to the output.yaml file. Expose the Applications \u00b6 There are multiple ways to expose applications outside of a cluster. The main ways are Service with type: Loadbalancer , Service with type: Nodeport , and with an Ingress Controller and Ingresses. Most of our examples will use Ingresses with Nginx as the ingress controller. This is for the following reasons: Prevalence of this pattern Cost efficiency - Cheaper than a load balancer per service. Reliability and scenario coverage - vs. NodePort: less chance of contention on cluster ports, reduction of need to hard-code ports, easier hostname and DNS management. Prerequisite \u00b6 Before deploying ingresses for the products, there must be an Ingress Controller available. An Nginx ingress-controller example with an AWS Network Load Balancer example can be found here like this: kustomize build https://github.com/pingidentity/ping-cloud-base/k8s-configs/cluster-tools/ingress/nginx/public > output.yaml Use of Nginx We use the Nginx ingress controller for reasons similar to why we chose exposing via Ingresses over Services. In addition to Nginx's prevalence in Kubernetes, it is preferred over the AWS ALB ingress controller (another common ingress controller) because: 1. Each Ingress with ALB ingress controller triggers creation of an ALB 2. Network Load Balancers allow us to do TCP traffic rather than just Layer 7 (HTTP(s). You'll also need a public certificate and private key to use as a tls secret to be presented by the ingress. You can generate this tls secret in kubernetes yaml format with the ping-devops tool ping-devops generate devops-secret | kubectl apply -f - Instructions \u00b6 You can find sample yaml files for ingresses on products that is makes sense for in 20-kubernetes/10-ingress These examples should be generally applicable, with the exception of metadata.annotations Deploy one of the examples with commands like: envsubst '${PING_IDENTITY_DEVOPS_DNS_ZONE}' \\ < 10 -ingress/pingfederate-standalone-ingress.yaml | \\ kubectl apply -f -","title":"Introduction"},{"location":"deployment/deployK8sGeneral/#kubernetes-deployments-for-general-use","text":"In all of these examples, we'll use the standalone configurations in your local pingidentity-devops-getting-started/20-kubernetes/01-standalone directory to supply the base product configurations. To minimize repetitive and irrelevant information, we'll also use kustomize . For effective use of the examples, we recommended that you be familiar with concepts such as \"resources\" and \"patches\" in kustomize . You'll find useful comments in the kustomization.yaml files in your local pingidentity-devops-getting-started/20-kubernetes example directories.","title":"Kubernetes Deployments for General Use"},{"location":"deployment/deployK8sGeneral/#prerequisites","text":"You've already been through Get Started to set up your DevOps environment and run a test deployment of the products. You've created a Kubernetes secret using your DevOps credentials. See the For Kubernetes topic in Using your DevOps user and key . For the PingFederate cluster: envsubst . Substitutes shell format strings with environment variables. See envsubst if your OS doesn't have this utility. PingFederate build image for version 10 or greater. (The DNS Discovery feature first available in version 10 is needed.)","title":"Prerequisites"},{"location":"deployment/deployK8sGeneral/#what-youll-do","text":"Install kustomize. Choose one or more of these examples to deploy: Orchestrate standalone deployments. Orchestrate a full stack deployment. Orchestrate a replicated PingDirectory deployment. Orchestrate a clustered PingAccess deployment. Orchestrate a clustered PingFederate deployment. Expose the applications","title":"What you'll do"},{"location":"deployment/deployK8sGeneral/#install-kustomize","text":"Install kustomize . To view standard YAML outputs in each directory, run commands like: kustomize build <path/to/directory> For the fullstack configuration (pingidentity-devops-getting-started/20-kubernetes/02-fullstack), for example, you might use: kustomize build ./20-kubernetes/02-fullstack > ./output.yaml This builds and redirects the output to the output.yaml file.","title":"Install kustomize"},{"location":"deployment/deployK8sGeneral/#expose-the-applications","text":"There are multiple ways to expose applications outside of a cluster. The main ways are Service with type: Loadbalancer , Service with type: Nodeport , and with an Ingress Controller and Ingresses. Most of our examples will use Ingresses with Nginx as the ingress controller. This is for the following reasons: Prevalence of this pattern Cost efficiency - Cheaper than a load balancer per service. Reliability and scenario coverage - vs. NodePort: less chance of contention on cluster ports, reduction of need to hard-code ports, easier hostname and DNS management.","title":"Expose the Applications"},{"location":"deployment/deployK8sGeneral/#prerequisite","text":"Before deploying ingresses for the products, there must be an Ingress Controller available. An Nginx ingress-controller example with an AWS Network Load Balancer example can be found here like this: kustomize build https://github.com/pingidentity/ping-cloud-base/k8s-configs/cluster-tools/ingress/nginx/public > output.yaml Use of Nginx We use the Nginx ingress controller for reasons similar to why we chose exposing via Ingresses over Services. In addition to Nginx's prevalence in Kubernetes, it is preferred over the AWS ALB ingress controller (another common ingress controller) because: 1. Each Ingress with ALB ingress controller triggers creation of an ALB 2. Network Load Balancers allow us to do TCP traffic rather than just Layer 7 (HTTP(s). You'll also need a public certificate and private key to use as a tls secret to be presented by the ingress. You can generate this tls secret in kubernetes yaml format with the ping-devops tool ping-devops generate devops-secret | kubectl apply -f -","title":"Prerequisite"},{"location":"deployment/deployK8sGeneral/#instructions","text":"You can find sample yaml files for ingresses on products that is makes sense for in 20-kubernetes/10-ingress These examples should be generally applicable, with the exception of metadata.annotations Deploy one of the examples with commands like: envsubst '${PING_IDENTITY_DEVOPS_DNS_ZONE}' \\ < 10 -ingress/pingfederate-standalone-ingress.yaml | \\ kubectl apply -f -","title":"Instructions"},{"location":"deployment/deployK8sPA-cluster/","text":"Orchestrate a PingAccess Cluster Deployment \u00b6 You'll use kustomize for the PingAccess cluster deployment from your local pingidentity-devops-getting-started/20-kubernetes/04-clustered-pingaccess directory (the location of the YAML files), and call into your local pingidentity-devops-getting-started/20-kubernetes/01-standalone directory for the base product configurations. You'll use the server profile in our pingidentity-server-profiles/pa-clustering repository. We use separate deployments for the PingAccess admin node ( env_vars.pingaccess ) and the PingAccess engine node ( env_vars.pingaccess-engine and pingaccess-engine.yaml ). To scale out replicas, use the PingAccess engine node. The env_vars.pingaccess and env_vars.pingaccess-engine files contain: The environment variables to use for pingidentity-server-profiles/pa-clustering . Sets the clustering (operational) mode for each deployment: CLUSTERED_CONSOLE for pingaccess and CLUSTERED_ENGINE for pingaccess-engine . kustomization.yaml does the following: References your local pingidentity-devops-getting-started/20-kubernetes/01-standalone/pingaccess directory for the base product configurations. Uses patches to remove the pingaccess engine port (3000). Replaces the environment variables in the parent configMap with those in the specified env_vars.pingaccess and env_vars.pingaccess-engine files. Deploy the Cluster \u00b6 To orchestrate the replicated PingAccess deployment, from your local pingidentity-devops-getting-started/20-kubernetes/04-clustered-pingaccess directory, enter: kustomize build . | kubectl apply -f - Scale up the engines: kubectl scale deployment pingaccess-engine --replicas = 2 Clean Up \u00b6 To clean up when you're finished, enter: kustomize build . | kubectl delete -f -","title":"Orchestrate a PingAccess Cluster Deployment"},{"location":"deployment/deployK8sPA-cluster/#orchestrate-a-pingaccess-cluster-deployment","text":"You'll use kustomize for the PingAccess cluster deployment from your local pingidentity-devops-getting-started/20-kubernetes/04-clustered-pingaccess directory (the location of the YAML files), and call into your local pingidentity-devops-getting-started/20-kubernetes/01-standalone directory for the base product configurations. You'll use the server profile in our pingidentity-server-profiles/pa-clustering repository. We use separate deployments for the PingAccess admin node ( env_vars.pingaccess ) and the PingAccess engine node ( env_vars.pingaccess-engine and pingaccess-engine.yaml ). To scale out replicas, use the PingAccess engine node. The env_vars.pingaccess and env_vars.pingaccess-engine files contain: The environment variables to use for pingidentity-server-profiles/pa-clustering . Sets the clustering (operational) mode for each deployment: CLUSTERED_CONSOLE for pingaccess and CLUSTERED_ENGINE for pingaccess-engine . kustomization.yaml does the following: References your local pingidentity-devops-getting-started/20-kubernetes/01-standalone/pingaccess directory for the base product configurations. Uses patches to remove the pingaccess engine port (3000). Replaces the environment variables in the parent configMap with those in the specified env_vars.pingaccess and env_vars.pingaccess-engine files.","title":"Orchestrate a PingAccess Cluster Deployment"},{"location":"deployment/deployK8sPA-cluster/#deploy-the-cluster","text":"To orchestrate the replicated PingAccess deployment, from your local pingidentity-devops-getting-started/20-kubernetes/04-clustered-pingaccess directory, enter: kustomize build . | kubectl apply -f - Scale up the engines: kubectl scale deployment pingaccess-engine --replicas = 2","title":"Deploy the Cluster"},{"location":"deployment/deployK8sPA-cluster/#clean-up","text":"To clean up when you're finished, enter: kustomize build . | kubectl delete -f -","title":"Clean Up"},{"location":"deployment/deployK8sPD-clusters/","text":"PingDirectory Deployments Across Kubernetes Clusters \u00b6 This example is an extension of the topic Orchestrate a replicated PingDirectory deployment in Kubernetes orchestration for general use . Here you'll deploy PingDirectory containers across multiple Kubernetes clusters. Overview \u00b6 Having a replicated PingDirectory topology across multiple kubernetes clusters is desired for highly-available active/active deployments as well as active/partial-active scenarios where a hot backup is expected. PingIdentity PingDirectory Docker Images abstract away much of the complexity of replication initialization scripts, even across clusters. Instead, the focus is on providing accessible DNS hostnames across clusters and environment variables to build ordinal hostnames for each Directory instance. What You'll Do \u00b6 PingDirectory Container Keys To Success Draft Hostnames - determine the variables needed to create your hostnames Additional Variables - for use-case flexibility Cluster Startup Background -Walk through what happens when a cluster starts Design Infrastructure to Match Hostnames - based on your infrastructure constraints, you may need to alter your hostname plans. Use these reference examples to help. Test the Deployment - use the 11-replication-timing folder to test replication speeds Because details within each Kubernetes cluster are well-hidden from outside the cluster, external access to each pod within the cluster is required. The PingDirectory images will set up access to each of the pods using load balancers from an external host, to allow each pod to communicate over the LDAP and replication protocols. PingDirectory Host Naming \u00b6 The most important aspect of a successful PingDirectory cross-cluster deployment assigning accessible and logical dns hostnames. Rules: Each pingdirectory needs it's own hostname avaialable in DNS hostname will have a space for the ordinal representing the instance in the statefulset all hostnames are accessible to all directory instances These rules still leave plenty of room for flexibility. Especially when accounting for cluster-native DNS names Kubernetes creates. Single-Cluster Multiple-Namespace \u00b6 For example, if you were to simulate a \"multi-cluster\" environment in a single cluster, you could just set up two namespaces and create a separate ClusterIP service for each directory. It would env up like so: Primary Cluster \u00b6 Pod Service Name Namespace Hostname pindirectory-0 pingdirectory-0 primary pingdirectory-0.primary pindirectory-1 pingdirectory-1 primary pingdirectory-1.primary pindirectory-2 pingdirectory-2 primary pingdirectory-2.primary Secondary Cluster \u00b6 Pod Service Name Namespace Hostname pindirectory-0 pingdirectory-0 secondary pingdirectory-0.secondary pindirectory-1 pingdirectory-1 secondary pingdirectory-1.secondary pindirectory-2 pingdirectory-2 secondary pingdirectory-2.secondary External DNS Names \u00b6 In a Prod Environment with external hostnames it may look more like: us-west cluster \u00b6 Pod Service Name DNS / Hostname pindirectory-0 pingdirectory-0 pingdirectory-0-us-west.ping-devops.com pindirectory-1 pingdirectory-1 pingdirectory-1-us-west.ping-devops.com pindirectory-2 pingdirectory-2 pingdirectory-2-us-west.ping-devops.com us-east cluster \u00b6 Pod Service Name DNS / Hostname pindirectory-0 pingdirectory-0 pingdirectory-0-us-east.ping-devops.com pindirectory-1 pingdirectory-1 pingdirectory-1-us-east.ping-devops.com pindirectory-2 pingdirectory-2 pingdirectory-2-us-east.ping-devops.com Variables to Create Hostnames \u00b6 To provide flexibility on how PingDirectory will find other instances, a full dns hostname is broken into multiple variables. Variable Description K8S_POD_HOSTNAME_PREFIX The string used as the prefix for all host names. Defaults to name of StatefulSet . K8S_POD_HOSTNAME_SUFFIX The string used as the suffix for all pod host names. Defaults to K8S_CLUSTER . K8S_SEED_HOSTNAME_SUFFIX The string used as the suffix for all seed host names. Defaults to K8S_SEED_CLUSTER (discussed later). A full hostname is created like: ${ K8S_POD_HOSTNAME_PREFIX } <instance-ordinal> ${ K8S_SEED_HOSTNAME_SUFFIX } Using Previous Hostname Examples \u00b6 hostname K8S_POD_HOSTNAME_PREFIX K8S_POD_HOSTNAME_SUFFIX K8S_SEED_HOSTNAME_SUFFIX pingdirectory-0.primary pingdirectory- .primary .primary pingdirectory-2-us-west.ping-devops.com pingdirectory- -us-west.ping-devops.com -us-west.ping-devops.com Environment Variables \u00b6 Variable Required Description K8S_CLUSTERS *** The total list of Kubernetes clusters that the StatefulSet will replicate to. K8S_CLUSTER *** The Kubernetes cluster the StatefulSet will be deployed to. K8S_SEED_CLUSTER *** The Kubernetes cluster that the seed server is deployed to. K8S_NUM_REPLICAS The number of replicas that make up the StatefulSet. K8S_POD_HOSTNAME_PREFIX The string used as the prefix for all host names. Defaults to StatefulSet . K8S_POD_HOSTNAME_SUFFIX The string used as the suffix for all pod host names. Defaults to K8S_CLUSTER . K8S_SEED_HOSTNAME_SUFFIX The string used as the suffix for all seed host names. Defaults to K8S_SEED_CLUSTER . K8S_INCREMENT_PORTS true or false . If true , each pod's port will be incremented by 1. An example of the YAML configuration for these environment variables: K8S_STATEFUL_SET_NAME=pingdirectory K8S_STATEFUL_SET_SERVICE_NAME=pingdirectory K8S_CLUSTERS=us-east-2 eu-west-1 K8S_CLUSTER=us-east-2 K8S_SEED_CLUSTER=us-east-2 K8S_NUM_REPLICAS=3 K8S_POD_HOSTNAME_PREFIX=pd- K8S_POD_HOSTNAME_SUFFIX=.us-cluster.ping-devops.com K8S_SEED_HOSTNAME_SUFFIX=.us-cluster.ping-devops.com K8S_INCREMENT_PORTS=true LDAPS_PORT=8600 REPLICATION_PORT=8700 These environment variable settings would map out like this: Seed Pod Instance Host name LDAP REPL CLUSTER: us-east-2 *** *** pingdirectory-0.us-east-2 pd-0.us-cluster.ping-devops.com 8600 8700 pingdirectory-1.us-east-2 pd-1.us-cluster.ping-devops.com 8601 8701 pingdirectory-2.us-east-2 pd-2.us-cluster.ping-devops.com 8602 8702 CLUSTER: eu-west-1 pingdirectory-0.eu-west-1 pd-0.eu-cluster.ping-devops.com 8600 8700 pingdirectory-1.eu-west-1 pd-1.eu-cluster.ping-devops.com 8601 8701 pingdirectory-2.eu-west-1 pd-2.eu-cluster.ping-devops.com 8602 8702 Cluster Startup Walkthrough \u00b6 Yes, that was a ton of variable conversation. This is done to for flexibility to accommodate various infrastructure constraints. For example, in some environments you cannot use the same port for each instance, so we must accommodate incrementing ports. Next, it's helpful to know what happens when a cluster starts, to understand why the initial creation of a cluster must be very prescriptive. The first pod must start on it's own and become healthy. This is critical to prevent replication islands. The very first time the very first pod starts, we call it \"GENESIS\". All other pods are dependent on this SEED_POD in the SEED_CLUSTER starting correctly on it's own. The entire purpose of defining SEED_POD and SEED_CLUSTER variables is avoid multiple genesis scenarios. Since we are deploying a statefulset, you can deploy the entire first cluster at once. Statefulsets create one pod in the number of replicas at a time. once the first pod is healthy, it begins dns querying combinations of hostnames at their LDAPS port to find another Directory instance. In our first cluster, this would be the hostname of pingdirectory-1. but it could also be pingdirectory-0 of another cluster. Once the query returns successful, creation of the replication topology automatically begins. From this point onward, the order in which instances start is less important. Note on Replication Traffic \u00b6 It may not always be clear what truly happens during \"replication\". Though it is partially proprietary, you can think of it like so: A modify request arrives at one pod. The corresponding Directory instance, say pingdirectory-0, makes the change locally, and tracks the change in the changelog. Then it broadcasts the change out to all other instances with the time of change. This request is added to a message queue of sorts on the other instances and processed in order. If you think about this, it starts to make sense why horizontal scaling of directories isn't something to be taken lightly. Reference Modes of Deployment \u00b6 There are multiple types of deployments that have been tested because of various infrastructure constraints. There will be discussed here. The example files in 09-multi-k8s-pingdirectory show that the biggest key is having a way to provide an accessible hostname to all the other pods. In most examples this is done by creating a service name and using statefulset.kubernetes.io/pod-name as the selector to isolate one pod. Multi-cluster Simulations \u00b6 These are examples for demo environments to get a feel for what a multi-region deployment looks like. Single Namespace \u00b6 20-kubernetes/09-multi-k8s-pingdirectory/01-single-namespace This is the least constrained example. It's good to just see what logs on a cross-cluster topology look likes relies only on dns names that kubernetes provides. All traffic is in one namespace so it should have no network constraints. kubectl apply -f 01 -west.yaml ...wait for pingdirectory-0 to be healthy... kubectl apply -f 02 -east.yaml note, logs with stern look better. brew install stern watch the logs stern pingdirectory or kubectl logs -f -l role = pingdirectory Once the all the instances are up and running, you should see something similar to: Single Cluster Multiple Namespaces \u00b6 20-kubernetes/09-multi-k8s-pingdirectory/02-single-cluster-two-namespaces This example can be used when you only have one cluster available for testing The files in this example are templates and expect namespaces to be added. export NAMESPACE_1 = <west-namespace> export NAMESPACE_2 = <east-namespace> envsubst < 03 -multi-cluster-dns/01-west.yaml | kubectl apply -f - envsubst < 03 -multi-cluster-dns/02-east.yaml | kubectl apply -f - Then watch logs. VPC Peered K8s Clusters \u00b6 These example should be possible in most kubernetes providers as long as you can: give external dns names to clusterIP services have replication and ldaps ports peered (open) between clusters Consider the EKS Peering Config example if you want to test this. Using External DNS Names \u00b6 20-kubernetes/09-multi-k8s-pingdirectory/03-multi-cluster-dns This example uses Headless Services instead of regular clusterIp services. This is necessary in a VPC peered environment because typically the route-tables and ip ranges you've peered correspond to container ip addresses, not service addresses. If you were to use clusterIp addresses, the instances may, unexpectedly, not have network connectivity to each other. The headless services use externalDNS to dynamically add records to the DNS provider (example, Route53) This example likely requires some more care, you'll want to sift through the yamls to understand what is going on. Once the example is stood up, you will see logs similar to: Without VPC Peering \u00b6 Some organizations do not allow VPC peering, or similar networking functions. Or, there may be no way to create external hostnames on clusterIp services. Here are some examples that may help. Using NodePorts \u00b6 In a scenario where you do not have VPC peering, or must create external DNS names manually, it may be beneficial to use NodePorts. To do this: Use the 20-kubernetes/09-multi-k8s-pingdirectory/03-multi-cluster-dns example as reference Make the pod-selector services NodePort services instead of clusterIp. Optionally, remove the external name, and create a routable dns name. If these names are being manually, then create the services and assign names before starting the statefulset. Single Load Balancer \u00b6 Caution The following examples are for extremely constrained environments, where traffic must go out through an external load balancer. For many purposes, these can be considered deprecated. Doing replication through load balancers should be avoided when possible. Here's a diagram of how a single load balancer can be used: Advantages Decreased cost of a single load balancer Single IP required Easier DNS management Wildcard DNS domain Or separate hosts pointing to load balancer Disadvantages More port mapping requirements Many external ports to manage and track Multiple Load Balancers \u00b6 Here's a diagram of how a single load balancer can be used: Advantages Use the same well-known port (such as, 636/8989) Separate IP addresses per instance Disadvantages DNS management Separate hostname required per instance StatefulSet Pod Services \u00b6 The StatefulSet service manages stateful objects for each pod. An example of the StatefulSet service configuration (one pod): kind : Service apiVersion : v1 metadata : name : pingdirectory-0-service spec : type : ClusterIP selector : statefulset.kubernetes.io/pod-name : pingdirectory-0 ports : - protocol : TCP port : 8600 targetPort : 8600 name : ldaps - protocol : TCP port : 8700 targetPort : 8700 name : repl Additional Kubernetes Resources Required \u00b6 In addition to the StatefulSet, other resources are required to properly map the load balancers to the pods. This diagram shows each of those resources: DNS \u00b6 A DNS entry will be required at the load balancer to direct a wildcard domain or individual host names to the load balancer created by the NGINX Ingress Service or Controller. For AWS, this can simply be an A record alias for each host, or a wildcard A record for any host in that domain. NGINX Ingress Service and Controller \u00b6 Several components map the ports from the external load balancer through the NGINX Service and Controller: External load balancer Provides an external IP and obtains definitions from the Ingress NGINX Service. Ingress NGINX Service Maps all port ranges (SEED_LDAPS_PORT, SEED_REPLICATION_PORT) to the same target port range. NGINX Ingress Controller Maps all port ranges to stateful set pods. Warning Typically, the NGINX Service and TCP services (see the following NGINX TCP services topic) require additional namespace access (such as, ingress-nginx-public ). Any additional applications using this service or controller will generally require additional privileges to manage this resource. An example of the NGINX Service configuration: kind : Service apiVersion : v1 metadata : name : ingress-nginx labels : app.kubernetes.io/name : ingress-nginx app.kubernetes.io/part-of : ingress-nginx app.kubernetes.io/role : ingress-nginx-public namespace : ingress-nginx-public annotations : service.beta.kubernetes.io/aws-load-balancer-type : nlb spec : selector : app.kubernetes.io/name : ingress-nginx app.kubernetes.io/part-of : ingress-nginx app.kubernetes.io/role : ingress-nginx-public externalTrafficPolicy : Local type : load-balancer ports : - name : http port : 80 targetPort : http - name : https port : 443 targetPort : https - name : ldaps-pingdiretory-0 port : 8600 targetPort : 8600 - name : ldaps-pingdiretory-1 port : 8601 targetPort : 8601 - name : ldaps-pingdiretory-2 port : 8602 targetPort : 8602 - name : repl-pingdiretory-0 port : 8700 targetPort : 8700 - name : repl-pingdiretory-1 port : 8701 targetPort : 8701 - name : repl-pingdiretory-2 port : 8702 targetPort : 8702 NGINX TCP Services \u00b6 The ConfigMap for TCP services ( tcp-services ) provides the mappings from the target ports on the NGINX Controller to the associated pod service. You'll need to replace the variable ${PING_IDENTITY_K8S_NAMESPACE} with the namespace that your StatefulSet and Services are deployed into. An example of the ConfigMap for the NGINX TCP services configuration: apiVersion : v1 kind : ConfigMap metadata : name : tcp-services namespace : ingress-nginx-public labels : app.kubernetes.io/name : ingress-nginx app.kubernetes.io/part-of : ingress-nginx app.kubernetes.io/role : ingress-nginx-public data : 8600 : \"${PING_IDENTITY_K8S_NAMESPACE}/pingdirectory-0-service:8600\" 8601 : \"${PING_IDENTITY_K8S_NAMESPACE}/pingdirectory-1-service:8601\" 8602 : \"${PING_IDENTITY_K8S_NAMESPACE}/pingdirectory-2-service:8602\" 8700 : \"${PING_IDENTITY_K8S_NAMESPACE}/pingdirectory-0-service:8700\" 8701 : \"${PING_IDENTITY_K8S_NAMESPACE}/pingdirectory-1-service:8701\" 8702 : \"${PING_IDENTITY_K8S_NAMESPACE}/pingdirectory-2-service:8702\" Deployment Example \u00b6 The examples in 20-kubernetes/05-multi-k8s-cluster-pingdirectory will create an example deployment across 3 clusters in AWS EKS: us-east-2 (SEED Cluster) eu-west-1 First, deploy the nginx services and configmap for the example. This will allow the services to be reached by the nginx controller via an AWS Network Load Balancer (nlb). You must run these with a aws/kubernetes profile allowing for apply into the ingress-nginx-public namespace. Also, be aware that there may already be other definitions found. You may need to merge. kubectl apply -f nginx-service.yaml kubectl apply -f nginx-tcp-services.yaml The cluster.sh script will create the .yaml necessary to deploy a set of Ping Directory instances in each cluster and replication between. Usage: cluster.sh OPERATION { options } where OPERATION in : create apply delete where options in : --cluster { cluster } - Cluster name used to identify different env_vars.pingdirecory files --context { context } - Name of Kubernetes context. Defaults to current context: jsmith.ping-dev-aws-us-east-2 -d,--dry-run - Provides the commands Example: cluster.sh create --cluster us-east-2 Steps To Create .yaml For us-east-2 \u00b6 Replace your-cluster-name with the name use are using. Using the cluster name us-east-2 , the script will generate a .yaml using kustomize, using the files: multi-cluster kustomization.yaml pingdirectory-service-clusterip.yaml env_vars.pingdirectory (built from env_vars.pingdirectory.multi-cluster and us-east-2) base kustomization.yaml https://github.com/pingidentity/pingidentity-devops-getting-started/20-kubernetes/03-replicated-pingdirectory env_vars.pingdirectory limits.yaml ./cluster.sh delete \\ --cluster us-east-2 \\ --context <your-cluster-name> \\ --dry-run This will create a .yaml called ouptut-us-east-2.yaml . Next, ensure that your devops-secret and tls-secret are created. ping-devops generate devops-secret | kubectl apply -f - ping-devops generate tls-secret ping-devops.com | kubectl create -f - and create the instances using the generated output-us-east-2.yaml . kubectl create -f output-us-east-2.yaml","title":"PingDirectory Deployments Across Kubernetes Clusters"},{"location":"deployment/deployK8sPD-clusters/#pingdirectory-deployments-across-kubernetes-clusters","text":"This example is an extension of the topic Orchestrate a replicated PingDirectory deployment in Kubernetes orchestration for general use . Here you'll deploy PingDirectory containers across multiple Kubernetes clusters.","title":"PingDirectory Deployments Across Kubernetes Clusters"},{"location":"deployment/deployK8sPD-clusters/#overview","text":"Having a replicated PingDirectory topology across multiple kubernetes clusters is desired for highly-available active/active deployments as well as active/partial-active scenarios where a hot backup is expected. PingIdentity PingDirectory Docker Images abstract away much of the complexity of replication initialization scripts, even across clusters. Instead, the focus is on providing accessible DNS hostnames across clusters and environment variables to build ordinal hostnames for each Directory instance.","title":"Overview"},{"location":"deployment/deployK8sPD-clusters/#what-youll-do","text":"PingDirectory Container Keys To Success Draft Hostnames - determine the variables needed to create your hostnames Additional Variables - for use-case flexibility Cluster Startup Background -Walk through what happens when a cluster starts Design Infrastructure to Match Hostnames - based on your infrastructure constraints, you may need to alter your hostname plans. Use these reference examples to help. Test the Deployment - use the 11-replication-timing folder to test replication speeds Because details within each Kubernetes cluster are well-hidden from outside the cluster, external access to each pod within the cluster is required. The PingDirectory images will set up access to each of the pods using load balancers from an external host, to allow each pod to communicate over the LDAP and replication protocols.","title":"What You'll Do"},{"location":"deployment/deployK8sPD-clusters/#pingdirectory-host-naming","text":"The most important aspect of a successful PingDirectory cross-cluster deployment assigning accessible and logical dns hostnames. Rules: Each pingdirectory needs it's own hostname avaialable in DNS hostname will have a space for the ordinal representing the instance in the statefulset all hostnames are accessible to all directory instances These rules still leave plenty of room for flexibility. Especially when accounting for cluster-native DNS names Kubernetes creates.","title":"PingDirectory Host Naming"},{"location":"deployment/deployK8sPD-clusters/#single-cluster-multiple-namespace","text":"For example, if you were to simulate a \"multi-cluster\" environment in a single cluster, you could just set up two namespaces and create a separate ClusterIP service for each directory. It would env up like so:","title":"Single-Cluster Multiple-Namespace"},{"location":"deployment/deployK8sPD-clusters/#primary-cluster","text":"Pod Service Name Namespace Hostname pindirectory-0 pingdirectory-0 primary pingdirectory-0.primary pindirectory-1 pingdirectory-1 primary pingdirectory-1.primary pindirectory-2 pingdirectory-2 primary pingdirectory-2.primary","title":"Primary Cluster"},{"location":"deployment/deployK8sPD-clusters/#secondary-cluster","text":"Pod Service Name Namespace Hostname pindirectory-0 pingdirectory-0 secondary pingdirectory-0.secondary pindirectory-1 pingdirectory-1 secondary pingdirectory-1.secondary pindirectory-2 pingdirectory-2 secondary pingdirectory-2.secondary","title":"Secondary Cluster"},{"location":"deployment/deployK8sPD-clusters/#external-dns-names","text":"In a Prod Environment with external hostnames it may look more like:","title":"External DNS Names"},{"location":"deployment/deployK8sPD-clusters/#us-west-cluster","text":"Pod Service Name DNS / Hostname pindirectory-0 pingdirectory-0 pingdirectory-0-us-west.ping-devops.com pindirectory-1 pingdirectory-1 pingdirectory-1-us-west.ping-devops.com pindirectory-2 pingdirectory-2 pingdirectory-2-us-west.ping-devops.com","title":"us-west cluster"},{"location":"deployment/deployK8sPD-clusters/#us-east-cluster","text":"Pod Service Name DNS / Hostname pindirectory-0 pingdirectory-0 pingdirectory-0-us-east.ping-devops.com pindirectory-1 pingdirectory-1 pingdirectory-1-us-east.ping-devops.com pindirectory-2 pingdirectory-2 pingdirectory-2-us-east.ping-devops.com","title":"us-east cluster"},{"location":"deployment/deployK8sPD-clusters/#variables-to-create-hostnames","text":"To provide flexibility on how PingDirectory will find other instances, a full dns hostname is broken into multiple variables. Variable Description K8S_POD_HOSTNAME_PREFIX The string used as the prefix for all host names. Defaults to name of StatefulSet . K8S_POD_HOSTNAME_SUFFIX The string used as the suffix for all pod host names. Defaults to K8S_CLUSTER . K8S_SEED_HOSTNAME_SUFFIX The string used as the suffix for all seed host names. Defaults to K8S_SEED_CLUSTER (discussed later). A full hostname is created like: ${ K8S_POD_HOSTNAME_PREFIX } <instance-ordinal> ${ K8S_SEED_HOSTNAME_SUFFIX }","title":"Variables to Create Hostnames"},{"location":"deployment/deployK8sPD-clusters/#using-previous-hostname-examples","text":"hostname K8S_POD_HOSTNAME_PREFIX K8S_POD_HOSTNAME_SUFFIX K8S_SEED_HOSTNAME_SUFFIX pingdirectory-0.primary pingdirectory- .primary .primary pingdirectory-2-us-west.ping-devops.com pingdirectory- -us-west.ping-devops.com -us-west.ping-devops.com","title":"Using Previous Hostname Examples"},{"location":"deployment/deployK8sPD-clusters/#environment-variables","text":"Variable Required Description K8S_CLUSTERS *** The total list of Kubernetes clusters that the StatefulSet will replicate to. K8S_CLUSTER *** The Kubernetes cluster the StatefulSet will be deployed to. K8S_SEED_CLUSTER *** The Kubernetes cluster that the seed server is deployed to. K8S_NUM_REPLICAS The number of replicas that make up the StatefulSet. K8S_POD_HOSTNAME_PREFIX The string used as the prefix for all host names. Defaults to StatefulSet . K8S_POD_HOSTNAME_SUFFIX The string used as the suffix for all pod host names. Defaults to K8S_CLUSTER . K8S_SEED_HOSTNAME_SUFFIX The string used as the suffix for all seed host names. Defaults to K8S_SEED_CLUSTER . K8S_INCREMENT_PORTS true or false . If true , each pod's port will be incremented by 1. An example of the YAML configuration for these environment variables: K8S_STATEFUL_SET_NAME=pingdirectory K8S_STATEFUL_SET_SERVICE_NAME=pingdirectory K8S_CLUSTERS=us-east-2 eu-west-1 K8S_CLUSTER=us-east-2 K8S_SEED_CLUSTER=us-east-2 K8S_NUM_REPLICAS=3 K8S_POD_HOSTNAME_PREFIX=pd- K8S_POD_HOSTNAME_SUFFIX=.us-cluster.ping-devops.com K8S_SEED_HOSTNAME_SUFFIX=.us-cluster.ping-devops.com K8S_INCREMENT_PORTS=true LDAPS_PORT=8600 REPLICATION_PORT=8700 These environment variable settings would map out like this: Seed Pod Instance Host name LDAP REPL CLUSTER: us-east-2 *** *** pingdirectory-0.us-east-2 pd-0.us-cluster.ping-devops.com 8600 8700 pingdirectory-1.us-east-2 pd-1.us-cluster.ping-devops.com 8601 8701 pingdirectory-2.us-east-2 pd-2.us-cluster.ping-devops.com 8602 8702 CLUSTER: eu-west-1 pingdirectory-0.eu-west-1 pd-0.eu-cluster.ping-devops.com 8600 8700 pingdirectory-1.eu-west-1 pd-1.eu-cluster.ping-devops.com 8601 8701 pingdirectory-2.eu-west-1 pd-2.eu-cluster.ping-devops.com 8602 8702","title":"Environment Variables"},{"location":"deployment/deployK8sPD-clusters/#cluster-startup-walkthrough","text":"Yes, that was a ton of variable conversation. This is done to for flexibility to accommodate various infrastructure constraints. For example, in some environments you cannot use the same port for each instance, so we must accommodate incrementing ports. Next, it's helpful to know what happens when a cluster starts, to understand why the initial creation of a cluster must be very prescriptive. The first pod must start on it's own and become healthy. This is critical to prevent replication islands. The very first time the very first pod starts, we call it \"GENESIS\". All other pods are dependent on this SEED_POD in the SEED_CLUSTER starting correctly on it's own. The entire purpose of defining SEED_POD and SEED_CLUSTER variables is avoid multiple genesis scenarios. Since we are deploying a statefulset, you can deploy the entire first cluster at once. Statefulsets create one pod in the number of replicas at a time. once the first pod is healthy, it begins dns querying combinations of hostnames at their LDAPS port to find another Directory instance. In our first cluster, this would be the hostname of pingdirectory-1. but it could also be pingdirectory-0 of another cluster. Once the query returns successful, creation of the replication topology automatically begins. From this point onward, the order in which instances start is less important.","title":"Cluster Startup Walkthrough"},{"location":"deployment/deployK8sPD-clusters/#note-on-replication-traffic","text":"It may not always be clear what truly happens during \"replication\". Though it is partially proprietary, you can think of it like so: A modify request arrives at one pod. The corresponding Directory instance, say pingdirectory-0, makes the change locally, and tracks the change in the changelog. Then it broadcasts the change out to all other instances with the time of change. This request is added to a message queue of sorts on the other instances and processed in order. If you think about this, it starts to make sense why horizontal scaling of directories isn't something to be taken lightly.","title":"Note on Replication Traffic"},{"location":"deployment/deployK8sPD-clusters/#reference-modes-of-deployment","text":"There are multiple types of deployments that have been tested because of various infrastructure constraints. There will be discussed here. The example files in 09-multi-k8s-pingdirectory show that the biggest key is having a way to provide an accessible hostname to all the other pods. In most examples this is done by creating a service name and using statefulset.kubernetes.io/pod-name as the selector to isolate one pod.","title":"Reference Modes of Deployment"},{"location":"deployment/deployK8sPD-clusters/#multi-cluster-simulations","text":"These are examples for demo environments to get a feel for what a multi-region deployment looks like.","title":"Multi-cluster Simulations"},{"location":"deployment/deployK8sPD-clusters/#single-namespace","text":"20-kubernetes/09-multi-k8s-pingdirectory/01-single-namespace This is the least constrained example. It's good to just see what logs on a cross-cluster topology look likes relies only on dns names that kubernetes provides. All traffic is in one namespace so it should have no network constraints. kubectl apply -f 01 -west.yaml ...wait for pingdirectory-0 to be healthy... kubectl apply -f 02 -east.yaml note, logs with stern look better. brew install stern watch the logs stern pingdirectory or kubectl logs -f -l role = pingdirectory Once the all the instances are up and running, you should see something similar to:","title":"Single Namespace"},{"location":"deployment/deployK8sPD-clusters/#single-cluster-multiple-namespaces","text":"20-kubernetes/09-multi-k8s-pingdirectory/02-single-cluster-two-namespaces This example can be used when you only have one cluster available for testing The files in this example are templates and expect namespaces to be added. export NAMESPACE_1 = <west-namespace> export NAMESPACE_2 = <east-namespace> envsubst < 03 -multi-cluster-dns/01-west.yaml | kubectl apply -f - envsubst < 03 -multi-cluster-dns/02-east.yaml | kubectl apply -f - Then watch logs.","title":"Single Cluster Multiple Namespaces"},{"location":"deployment/deployK8sPD-clusters/#vpc-peered-k8s-clusters","text":"These example should be possible in most kubernetes providers as long as you can: give external dns names to clusterIP services have replication and ldaps ports peered (open) between clusters Consider the EKS Peering Config example if you want to test this.","title":"VPC Peered K8s Clusters"},{"location":"deployment/deployK8sPD-clusters/#using-external-dns-names","text":"20-kubernetes/09-multi-k8s-pingdirectory/03-multi-cluster-dns This example uses Headless Services instead of regular clusterIp services. This is necessary in a VPC peered environment because typically the route-tables and ip ranges you've peered correspond to container ip addresses, not service addresses. If you were to use clusterIp addresses, the instances may, unexpectedly, not have network connectivity to each other. The headless services use externalDNS to dynamically add records to the DNS provider (example, Route53) This example likely requires some more care, you'll want to sift through the yamls to understand what is going on. Once the example is stood up, you will see logs similar to:","title":"Using External DNS Names"},{"location":"deployment/deployK8sPD-clusters/#without-vpc-peering","text":"Some organizations do not allow VPC peering, or similar networking functions. Or, there may be no way to create external hostnames on clusterIp services. Here are some examples that may help.","title":"Without VPC Peering"},{"location":"deployment/deployK8sPD-clusters/#using-nodeports","text":"In a scenario where you do not have VPC peering, or must create external DNS names manually, it may be beneficial to use NodePorts. To do this: Use the 20-kubernetes/09-multi-k8s-pingdirectory/03-multi-cluster-dns example as reference Make the pod-selector services NodePort services instead of clusterIp. Optionally, remove the external name, and create a routable dns name. If these names are being manually, then create the services and assign names before starting the statefulset.","title":"Using NodePorts"},{"location":"deployment/deployK8sPD-clusters/#single-load-balancer","text":"Caution The following examples are for extremely constrained environments, where traffic must go out through an external load balancer. For many purposes, these can be considered deprecated. Doing replication through load balancers should be avoided when possible. Here's a diagram of how a single load balancer can be used: Advantages Decreased cost of a single load balancer Single IP required Easier DNS management Wildcard DNS domain Or separate hosts pointing to load balancer Disadvantages More port mapping requirements Many external ports to manage and track","title":"Single Load Balancer"},{"location":"deployment/deployK8sPD-clusters/#multiple-load-balancers","text":"Here's a diagram of how a single load balancer can be used: Advantages Use the same well-known port (such as, 636/8989) Separate IP addresses per instance Disadvantages DNS management Separate hostname required per instance","title":"Multiple Load Balancers"},{"location":"deployment/deployK8sPD-clusters/#statefulset-pod-services","text":"The StatefulSet service manages stateful objects for each pod. An example of the StatefulSet service configuration (one pod): kind : Service apiVersion : v1 metadata : name : pingdirectory-0-service spec : type : ClusterIP selector : statefulset.kubernetes.io/pod-name : pingdirectory-0 ports : - protocol : TCP port : 8600 targetPort : 8600 name : ldaps - protocol : TCP port : 8700 targetPort : 8700 name : repl","title":"StatefulSet Pod Services"},{"location":"deployment/deployK8sPD-clusters/#additional-kubernetes-resources-required","text":"In addition to the StatefulSet, other resources are required to properly map the load balancers to the pods. This diagram shows each of those resources:","title":"Additional Kubernetes Resources Required"},{"location":"deployment/deployK8sPD-clusters/#dns","text":"A DNS entry will be required at the load balancer to direct a wildcard domain or individual host names to the load balancer created by the NGINX Ingress Service or Controller. For AWS, this can simply be an A record alias for each host, or a wildcard A record for any host in that domain.","title":"DNS"},{"location":"deployment/deployK8sPD-clusters/#nginx-ingress-service-and-controller","text":"Several components map the ports from the external load balancer through the NGINX Service and Controller: External load balancer Provides an external IP and obtains definitions from the Ingress NGINX Service. Ingress NGINX Service Maps all port ranges (SEED_LDAPS_PORT, SEED_REPLICATION_PORT) to the same target port range. NGINX Ingress Controller Maps all port ranges to stateful set pods. Warning Typically, the NGINX Service and TCP services (see the following NGINX TCP services topic) require additional namespace access (such as, ingress-nginx-public ). Any additional applications using this service or controller will generally require additional privileges to manage this resource. An example of the NGINX Service configuration: kind : Service apiVersion : v1 metadata : name : ingress-nginx labels : app.kubernetes.io/name : ingress-nginx app.kubernetes.io/part-of : ingress-nginx app.kubernetes.io/role : ingress-nginx-public namespace : ingress-nginx-public annotations : service.beta.kubernetes.io/aws-load-balancer-type : nlb spec : selector : app.kubernetes.io/name : ingress-nginx app.kubernetes.io/part-of : ingress-nginx app.kubernetes.io/role : ingress-nginx-public externalTrafficPolicy : Local type : load-balancer ports : - name : http port : 80 targetPort : http - name : https port : 443 targetPort : https - name : ldaps-pingdiretory-0 port : 8600 targetPort : 8600 - name : ldaps-pingdiretory-1 port : 8601 targetPort : 8601 - name : ldaps-pingdiretory-2 port : 8602 targetPort : 8602 - name : repl-pingdiretory-0 port : 8700 targetPort : 8700 - name : repl-pingdiretory-1 port : 8701 targetPort : 8701 - name : repl-pingdiretory-2 port : 8702 targetPort : 8702","title":"NGINX Ingress Service and Controller"},{"location":"deployment/deployK8sPD-clusters/#nginx-tcp-services","text":"The ConfigMap for TCP services ( tcp-services ) provides the mappings from the target ports on the NGINX Controller to the associated pod service. You'll need to replace the variable ${PING_IDENTITY_K8S_NAMESPACE} with the namespace that your StatefulSet and Services are deployed into. An example of the ConfigMap for the NGINX TCP services configuration: apiVersion : v1 kind : ConfigMap metadata : name : tcp-services namespace : ingress-nginx-public labels : app.kubernetes.io/name : ingress-nginx app.kubernetes.io/part-of : ingress-nginx app.kubernetes.io/role : ingress-nginx-public data : 8600 : \"${PING_IDENTITY_K8S_NAMESPACE}/pingdirectory-0-service:8600\" 8601 : \"${PING_IDENTITY_K8S_NAMESPACE}/pingdirectory-1-service:8601\" 8602 : \"${PING_IDENTITY_K8S_NAMESPACE}/pingdirectory-2-service:8602\" 8700 : \"${PING_IDENTITY_K8S_NAMESPACE}/pingdirectory-0-service:8700\" 8701 : \"${PING_IDENTITY_K8S_NAMESPACE}/pingdirectory-1-service:8701\" 8702 : \"${PING_IDENTITY_K8S_NAMESPACE}/pingdirectory-2-service:8702\"","title":"NGINX TCP Services"},{"location":"deployment/deployK8sPD-clusters/#deployment-example","text":"The examples in 20-kubernetes/05-multi-k8s-cluster-pingdirectory will create an example deployment across 3 clusters in AWS EKS: us-east-2 (SEED Cluster) eu-west-1 First, deploy the nginx services and configmap for the example. This will allow the services to be reached by the nginx controller via an AWS Network Load Balancer (nlb). You must run these with a aws/kubernetes profile allowing for apply into the ingress-nginx-public namespace. Also, be aware that there may already be other definitions found. You may need to merge. kubectl apply -f nginx-service.yaml kubectl apply -f nginx-tcp-services.yaml The cluster.sh script will create the .yaml necessary to deploy a set of Ping Directory instances in each cluster and replication between. Usage: cluster.sh OPERATION { options } where OPERATION in : create apply delete where options in : --cluster { cluster } - Cluster name used to identify different env_vars.pingdirecory files --context { context } - Name of Kubernetes context. Defaults to current context: jsmith.ping-dev-aws-us-east-2 -d,--dry-run - Provides the commands Example: cluster.sh create --cluster us-east-2","title":"Deployment Example"},{"location":"deployment/deployK8sPD-clusters/#steps-to-create-yaml-for-us-east-2","text":"Replace your-cluster-name with the name use are using. Using the cluster name us-east-2 , the script will generate a .yaml using kustomize, using the files: multi-cluster kustomization.yaml pingdirectory-service-clusterip.yaml env_vars.pingdirectory (built from env_vars.pingdirectory.multi-cluster and us-east-2) base kustomization.yaml https://github.com/pingidentity/pingidentity-devops-getting-started/20-kubernetes/03-replicated-pingdirectory env_vars.pingdirectory limits.yaml ./cluster.sh delete \\ --cluster us-east-2 \\ --context <your-cluster-name> \\ --dry-run This will create a .yaml called ouptut-us-east-2.yaml . Next, ensure that your devops-secret and tls-secret are created. ping-devops generate devops-secret | kubectl apply -f - ping-devops generate tls-secret ping-devops.com | kubectl create -f - and create the instances using the generated output-us-east-2.yaml . kubectl create -f output-us-east-2.yaml","title":"Steps To Create .yaml For us-east-2"},{"location":"deployment/deployK8sPF-cluster/","text":"Orchestrate a PingFederate Cluster Deployment \u00b6 You'll use kustomize for the PingFederate cluster deployment from your local pingidentity-devops-getting-started/20-kubernetes/06-clustered-pingfederate directory (the location of the YAML files), and call into your local pingidentity-devops-getting-started/20-kubernetes/01-standalone directory for the base product configurations. We use layered server profiles: The server profile in our pingidentity-server-profiles/pf-dns-ping-clustering repository. The server profile in our pingidentity-server-profiles/getting-started/pingfederate repository. See Layering Server Profiles for more information. We use separate deployments for the PingFederate admin node ( env_vars.pingfederate ) and the PingFederate engine node ( env_vars.pingfederate-engine and pingfederate-engine.yaml ). To scale out replicas, use the PingFederate engine node. The env_vars.pingfederate and env_vars.pingfederate-engine files contain: The environment variables to use for pingidentity-server-profiles/pf-dns-ping-clustering and pingidentity-server-profiles/getting-started/pingfederate . Sets the clustering (operational) mode for each deployment: CLUSTERED_CONSOLE for pingfederate and CLUSTERED_ENGINE for pingfederate-engine . Assigns the Kubernetes variable DNS_QUERY_LOCATION . kustomization.yaml does the following: References your local pingidentity-devops-getting-started/20-kubernetes/01-standalone/pingfederate directory for the base product configurations. Uses patches to remove the pingfederate engine port (9031). Adds a pingfederate cluster port (7600). Replaces the environment variables in the parent configMap with those in the specified env_vars.pingfederate and env_vars.pingfederate-engine files. Prerequisites \u00b6 envsubst . Substitutes shell format strings with environment variables. See envsubst if your OS doesn't have this utility. PingFederate build image for version 10 or greater. (The DNS Discovery feature first available in version 10 is needed.) Deploy the Cluster \u00b6 Set the environment variable that we assign to the Kubernetes variable DNS_QUERY_LOCATION . Either: Add PING_IDENTITY_K8S_NAMESPACE=<your-k8s-namespace> to your ~/.pingidentity/devops file. Run export PING_IDENTITY_K8S_NAMESPACE=<your-k8s-namespace> . To orchestrate the clustered PingFederate deployment, from your local pingidentity-devops-getting-started/20-kubernetes directory, enter: kustomize build . | \\ envsubst '${PING_IDENTITY_K8S_NAMESPACE}' | \\ kubectl apply -f - Cluster Startup In some situations, the PingFederate engine deployment can create a cluster before the admin deployment, thereby creating cluster silos. This can be overcome by using an Init container. Wait for the pingfederate-engine pod to be running, then validate clustering has worked. You can port-forward the admin service and view the clustering using the admin console. For example: kubectl port-forward svc/pingfederate 9999 :9999 Scale up the engines: kubectl scale deployment pingfederate-engine --replicas = 2 Cleanup \u00b6 To clean up when you're finished, enter: kustomize build . | \\ envsubst '${PING_IDENTITY_K8S_NAMESPACE}' | \\ kubectl delete -f -","title":"Orchestrate a PingFederate Cluster Deployment"},{"location":"deployment/deployK8sPF-cluster/#orchestrate-a-pingfederate-cluster-deployment","text":"You'll use kustomize for the PingFederate cluster deployment from your local pingidentity-devops-getting-started/20-kubernetes/06-clustered-pingfederate directory (the location of the YAML files), and call into your local pingidentity-devops-getting-started/20-kubernetes/01-standalone directory for the base product configurations. We use layered server profiles: The server profile in our pingidentity-server-profiles/pf-dns-ping-clustering repository. The server profile in our pingidentity-server-profiles/getting-started/pingfederate repository. See Layering Server Profiles for more information. We use separate deployments for the PingFederate admin node ( env_vars.pingfederate ) and the PingFederate engine node ( env_vars.pingfederate-engine and pingfederate-engine.yaml ). To scale out replicas, use the PingFederate engine node. The env_vars.pingfederate and env_vars.pingfederate-engine files contain: The environment variables to use for pingidentity-server-profiles/pf-dns-ping-clustering and pingidentity-server-profiles/getting-started/pingfederate . Sets the clustering (operational) mode for each deployment: CLUSTERED_CONSOLE for pingfederate and CLUSTERED_ENGINE for pingfederate-engine . Assigns the Kubernetes variable DNS_QUERY_LOCATION . kustomization.yaml does the following: References your local pingidentity-devops-getting-started/20-kubernetes/01-standalone/pingfederate directory for the base product configurations. Uses patches to remove the pingfederate engine port (9031). Adds a pingfederate cluster port (7600). Replaces the environment variables in the parent configMap with those in the specified env_vars.pingfederate and env_vars.pingfederate-engine files.","title":"Orchestrate a PingFederate Cluster Deployment"},{"location":"deployment/deployK8sPF-cluster/#prerequisites","text":"envsubst . Substitutes shell format strings with environment variables. See envsubst if your OS doesn't have this utility. PingFederate build image for version 10 or greater. (The DNS Discovery feature first available in version 10 is needed.)","title":"Prerequisites"},{"location":"deployment/deployK8sPF-cluster/#deploy-the-cluster","text":"Set the environment variable that we assign to the Kubernetes variable DNS_QUERY_LOCATION . Either: Add PING_IDENTITY_K8S_NAMESPACE=<your-k8s-namespace> to your ~/.pingidentity/devops file. Run export PING_IDENTITY_K8S_NAMESPACE=<your-k8s-namespace> . To orchestrate the clustered PingFederate deployment, from your local pingidentity-devops-getting-started/20-kubernetes directory, enter: kustomize build . | \\ envsubst '${PING_IDENTITY_K8S_NAMESPACE}' | \\ kubectl apply -f - Cluster Startup In some situations, the PingFederate engine deployment can create a cluster before the admin deployment, thereby creating cluster silos. This can be overcome by using an Init container. Wait for the pingfederate-engine pod to be running, then validate clustering has worked. You can port-forward the admin service and view the clustering using the admin console. For example: kubectl port-forward svc/pingfederate 9999 :9999 Scale up the engines: kubectl scale deployment pingfederate-engine --replicas = 2","title":"Deploy the Cluster"},{"location":"deployment/deployK8sPF-cluster/#cleanup","text":"To clean up when you're finished, enter: kustomize build . | \\ envsubst '${PING_IDENTITY_K8S_NAMESPACE}' | \\ kubectl delete -f -","title":"Cleanup"},{"location":"deployment/deployK8sPFclusters/","text":"PingFederate Cluster Across Multiple Kubernetes Clusters \u00b6 This section will discuss deploying a single PingFederate cluster that spans across multiple Kubernetes clusters. Having PingFederate in multiple regions doesn't always mean that spanning a single PingFederate cluster across multiple Kubernetes clusters is necessary or optimal. This scenario makes sense when you have: Traffic that can cross between regions at any time. (us-west and us-east, and users may be routed to either) Configuration that needs to be the same in multiple regions and no reliable automation to ensure that If all configuration changes are delivered via pipeline, and traffic wouldn't cross-regions, having separate PingFederate clusters can work. Note The set of pre-requisites required for AWS Kubernetes multi-clustering to be successful is found Here Overview \u00b6 This section will focus on the optimal use of PingFederate features. Adaptive clustering and dynamic discovery of engines, to enable engine auto-scaling. Static engine lists, which may be used to extend traditional, on-premise PingFederate clusters is out of scope here. Discovery Options \u00b6 There are two main dynamic discovery options: DNS_PING (PF > 10.2) \u00b6 S3 (PF < 10.1.x) \u00b6 In either scenario, some prerequisites must be achieved Prerequisites \u00b6 Two Kubernetes clusters created with the following requirements: VPC IPs selected from RFC1918 CIDR blocks The two cluster VPCs peered together All appropriate routing tables modified in both clusters to send cross cluster traffic to the VPC peer connection Security groups on both clusters to allow traffic for ports 7600 and 7700 to pass Successfully verified that a pod in one cluster can connect to a pod in the second cluster on ports 7600 and 7700 (directly to the pods back-end IP, not an exposed service) See example \"AWS configuration\" instructions Here envsubst kustomize Constants \u00b6 In either scenario, some pieces will remain the same. Yaml files that include: Two deployments: pingfederate-admin represents the admin console. pingfederate represents the engine(s) Two Configmaps. One for each deployment. These configmaps are nearly identical, but define the operational mode separately. Profile Files: run.properties.subst cluster-adaptive.conf.subst tcp.xml.subst (using S3_PING, NATIVE_S3_PING, or DNS_PING) Two Services: One for each of the two deployments (9999 and 9031). Variables \u00b6 For items specific to using S3 or DNS_PING follow the respective docs. GA PF 10.2 onward, Using DNS_PING PF < 10.1.x Using S3 (referencing AWS S3, with possible application to Azure)","title":"Introduction"},{"location":"deployment/deployK8sPFclusters/#pingfederate-cluster-across-multiple-kubernetes-clusters","text":"This section will discuss deploying a single PingFederate cluster that spans across multiple Kubernetes clusters. Having PingFederate in multiple regions doesn't always mean that spanning a single PingFederate cluster across multiple Kubernetes clusters is necessary or optimal. This scenario makes sense when you have: Traffic that can cross between regions at any time. (us-west and us-east, and users may be routed to either) Configuration that needs to be the same in multiple regions and no reliable automation to ensure that If all configuration changes are delivered via pipeline, and traffic wouldn't cross-regions, having separate PingFederate clusters can work. Note The set of pre-requisites required for AWS Kubernetes multi-clustering to be successful is found Here","title":"PingFederate Cluster Across Multiple Kubernetes Clusters"},{"location":"deployment/deployK8sPFclusters/#overview","text":"This section will focus on the optimal use of PingFederate features. Adaptive clustering and dynamic discovery of engines, to enable engine auto-scaling. Static engine lists, which may be used to extend traditional, on-premise PingFederate clusters is out of scope here.","title":"Overview"},{"location":"deployment/deployK8sPFclusters/#discovery-options","text":"There are two main dynamic discovery options:","title":"Discovery Options"},{"location":"deployment/deployK8sPFclusters/#dns_ping-pf-102","text":"","title":"DNS_PING (PF &gt; 10.2)"},{"location":"deployment/deployK8sPFclusters/#s3-pf-101x","text":"In either scenario, some prerequisites must be achieved","title":"S3 (PF &lt; 10.1.x)"},{"location":"deployment/deployK8sPFclusters/#prerequisites","text":"Two Kubernetes clusters created with the following requirements: VPC IPs selected from RFC1918 CIDR blocks The two cluster VPCs peered together All appropriate routing tables modified in both clusters to send cross cluster traffic to the VPC peer connection Security groups on both clusters to allow traffic for ports 7600 and 7700 to pass Successfully verified that a pod in one cluster can connect to a pod in the second cluster on ports 7600 and 7700 (directly to the pods back-end IP, not an exposed service) See example \"AWS configuration\" instructions Here envsubst kustomize","title":"Prerequisites"},{"location":"deployment/deployK8sPFclusters/#constants","text":"In either scenario, some pieces will remain the same. Yaml files that include: Two deployments: pingfederate-admin represents the admin console. pingfederate represents the engine(s) Two Configmaps. One for each deployment. These configmaps are nearly identical, but define the operational mode separately. Profile Files: run.properties.subst cluster-adaptive.conf.subst tcp.xml.subst (using S3_PING, NATIVE_S3_PING, or DNS_PING) Two Services: One for each of the two deployments (9999 and 9031).","title":"Constants"},{"location":"deployment/deployK8sPFclusters/#variables","text":"For items specific to using S3 or DNS_PING follow the respective docs. GA PF 10.2 onward, Using DNS_PING PF < 10.1.x Using S3 (referencing AWS S3, with possible application to Azure)","title":"Variables"},{"location":"deployment/deployK8sReplicated/","text":"Orchestrate a Replicated PingDirectory Deployment \u00b6 You'll use kustomize for the replicated deployment of PingDirectory from your local pingidentity-devops-getting-started/20-kubernetes/03-replicated-pingdirectory directory (the location of the YAML files), and call into your local pingidentity-devops-getting-started/20-kubernetes/01-standalone/pingdirectory and pingidentity-devops-getting-started/20-kubernetes/01-standalone/pingdataconsole directories for the base product configurations. You'll use the PingDirectory server profile in our pingidentity-server-profiles/baseline repository. The env_vars.pingdirectory file contains: The environment variables to use for pingidentity-server-profiles/baseline/pingdirectory . The Kubernetes declarations for PingDirectory. An initial creation of 1,000 PingDirectory users. A command to tail the log files for display. kustomization.yaml does the following: References your local pingidentity-devops-getting-started/20-kubernetes/01-standalone/pingdirectory and pingidentity-devops-getting-started/20-kubernetes/01-standalone/pingdataconsole directories for the base product configurations. References a mounted Kubernetes storage class volume for disaster recovery ( storage.yaml ). Replaces the environment variables in the parent configMap with those in the specified env_vars.pingdirectory file. See also Orchestrate PingDirectory Deployments Across Kubernetes Clusters . Deploy Stack \u00b6 To orchestrate the replicated PingDirectory deployment, from your local pingidentity-devops-getting-started/20-kubernetes/03-replicated-pingdirectory directory, enter: kustomize build . | kubectl apply -f - Clean Up \u00b6 To clean up when you're finished, enter: kustomize build . | kubectl delete -f -","title":"Orchestrate a Replicated PingDirectory Deployment"},{"location":"deployment/deployK8sReplicated/#orchestrate-a-replicated-pingdirectory-deployment","text":"You'll use kustomize for the replicated deployment of PingDirectory from your local pingidentity-devops-getting-started/20-kubernetes/03-replicated-pingdirectory directory (the location of the YAML files), and call into your local pingidentity-devops-getting-started/20-kubernetes/01-standalone/pingdirectory and pingidentity-devops-getting-started/20-kubernetes/01-standalone/pingdataconsole directories for the base product configurations. You'll use the PingDirectory server profile in our pingidentity-server-profiles/baseline repository. The env_vars.pingdirectory file contains: The environment variables to use for pingidentity-server-profiles/baseline/pingdirectory . The Kubernetes declarations for PingDirectory. An initial creation of 1,000 PingDirectory users. A command to tail the log files for display. kustomization.yaml does the following: References your local pingidentity-devops-getting-started/20-kubernetes/01-standalone/pingdirectory and pingidentity-devops-getting-started/20-kubernetes/01-standalone/pingdataconsole directories for the base product configurations. References a mounted Kubernetes storage class volume for disaster recovery ( storage.yaml ). Replaces the environment variables in the parent configMap with those in the specified env_vars.pingdirectory file. See also Orchestrate PingDirectory Deployments Across Kubernetes Clusters .","title":"Orchestrate a Replicated PingDirectory Deployment"},{"location":"deployment/deployK8sReplicated/#deploy-stack","text":"To orchestrate the replicated PingDirectory deployment, from your local pingidentity-devops-getting-started/20-kubernetes/03-replicated-pingdirectory directory, enter: kustomize build . | kubectl apply -f -","title":"Deploy Stack"},{"location":"deployment/deployK8sReplicated/#clean-up","text":"To clean up when you're finished, enter: kustomize build . | kubectl delete -f -","title":"Clean Up"},{"location":"deployment/deployK8sStandalone/","text":"Orchestrate Standalone Deployments \u00b6 You'll use the standalone configurations in your local pingidentity-devops-getting-started/20-kubernetes/01-standalone directory as the base product configurations with the server profiles in our pingidentity-server-profiles/getting-started repository. The commands in this topic are meant to be used with or without kustomize. When used without kustomize (as the steps in this topic do), the commands will return some benign errors regarding kustomization.yaml . An example of a benign kustomize error is: unable to recognize \"01-standalone/kustomization.yaml\" : no matches for kind \"Kustomization\" in version \"kustomize.config.k8s.io/v1beta1\" You can deploy a single (standalone) product container, or a set of standalone containers using Kubernetes. Deploy the Containers \u00b6 Go to any one of the product subdirectories in your local pingidentity-devops-getting-started/20-kubernetes/01-standalone directory. Orchestrate the deployment using the Kubernetes commands. For example: kubectl apply -f pingfederate/ To orchestrate a deployment of all of the products in your pingidentity-devops-getting-started/20-kubernetes/01-standalone directory. From your local pingidentity-devops-getting-started/20-kubernetes directory, enter: kubectl apply -R -f 01 -standalone/ Clean Up \u00b6 To clean up when you're finished, for a single product container, enter: kubectl delete -f <container>/ Where <container> is the name of a product container (such as, pingfederate ). For all products in the 01-standalone directory, enter: kubectl delete -R -f 01-standalone/","title":"Orchestrate Standalone Deployments"},{"location":"deployment/deployK8sStandalone/#orchestrate-standalone-deployments","text":"You'll use the standalone configurations in your local pingidentity-devops-getting-started/20-kubernetes/01-standalone directory as the base product configurations with the server profiles in our pingidentity-server-profiles/getting-started repository. The commands in this topic are meant to be used with or without kustomize. When used without kustomize (as the steps in this topic do), the commands will return some benign errors regarding kustomization.yaml . An example of a benign kustomize error is: unable to recognize \"01-standalone/kustomization.yaml\" : no matches for kind \"Kustomization\" in version \"kustomize.config.k8s.io/v1beta1\" You can deploy a single (standalone) product container, or a set of standalone containers using Kubernetes.","title":"Orchestrate Standalone Deployments"},{"location":"deployment/deployK8sStandalone/#deploy-the-containers","text":"Go to any one of the product subdirectories in your local pingidentity-devops-getting-started/20-kubernetes/01-standalone directory. Orchestrate the deployment using the Kubernetes commands. For example: kubectl apply -f pingfederate/ To orchestrate a deployment of all of the products in your pingidentity-devops-getting-started/20-kubernetes/01-standalone directory. From your local pingidentity-devops-getting-started/20-kubernetes directory, enter: kubectl apply -R -f 01 -standalone/","title":"Deploy the Containers"},{"location":"deployment/deployK8sStandalone/#clean-up","text":"To clean up when you're finished, for a single product container, enter: kubectl delete -f <container>/ Where <container> is the name of a product container (such as, pingfederate ). For all products in the 01-standalone directory, enter: kubectl delete -R -f 01-standalone/","title":"Clean Up"},{"location":"deployment/deployMonitoringStack/","text":"Deploy Monitoring Stack \u00b6 This example illustrates how to use Cloud Native Computing Foundation (CNCF) monitoring tools with a PingDirectory stack. There are tools in this stack to: Tool Purpose Monitor Ping Identity Software Collect Metrics Prometheus Alertsmanager cAdvisor prometheus/statsd_exporter InfluxDB Display Metrics Grafana Generate Load pingidentity/ldap-sdk-tools pingidentity/apache-jmeter Prometheus Much of the generic Prometheus work is taken from the vegasbrianc/prometheus repository. Prerequisites \u00b6 You've already been through Get Started to set up your DevOps environment and run a test deployment of the products. Pull our pingidentity-getting-started Git repo to ensure you have the latest sources. What You'll Do \u00b6 Deploy the stack Watch the load it generates Learn a bit about using the tools Display the metrics Clean up the stack PingDirectory produces a wide array of metrics. These metrics can be delivered in StatsD format to a location of your choosing using the StatsD monitoring endpoint for PingDirectory. See the PingDirectory documentation StatsD Monitoring Endpoint for more information. Deploy Stack \u00b6 From pingidentity-devops-getting-started/11-docker-compose/10-monitoring-stack run: docker-compose up -d Running this command will: Deploy the PingIdentity software. Pull metrics from the Ping Identity software into Prometheus-enabled endpoints (such as, StatsD metrics using statsd_exporter , which formats and hosts the metrics). Have Prometheus scrape the /metrics endpoint on statsd_exporter . Generate load to have metrics worth looking at, and push the metrics from the client application (JMeter) to InfluxDB. Deploy a dashboard in Grafana to visualize the metrics from Prometheus and other tools. Wait for PingDirectory to become healthy. For example: docker container ls \\ --filter name = pingdirectory_1 \\ --format 'table {{.Names}}\\t{{.Status}}' The results displayed will be similar to this: NAMES STATUS 10 -monitoring-stack_pingdirectory_1 Up 2 hours ( healthy ) About the Configuration \u00b6 There's a lot that can be discussed regarding the configuration. We'll focus on what is key to making this use case functional with minimal intervention, and describe what you may want to edit. View Configuration All relevant configurations are located in your local pingidentity-devops-getting-started/11-docker-compose/10-monitoring-stack/configs directory. The PingDirectory configuration looks like this: pingdirectory \u2514\u2500\u2500 pd.profile \u2514\u2500\u2500 dsconfig \u2514\u2500\u2500 15-prometheus-stats.dsconfig The baseline server profile. A single file with two dsconfig commands to create the StatsD monitoring endpoint and define where to push the metrics. Traditional profile layering is thought of as getting the profiles from multiple Git repos. However, sending a portion of a profile using the mounted /opt/in volume, and getting the rest of the profile information from a Git repo can still be considered layering. StatsD-Exporter The configuration file pingdirectory-statsd-mapping.yml defines which metrics to ingest and how to format them for Prometheus. This file is mounted to a location that is referenced from an argument passed to the startup command from the docker-compose.yaml file. Prometheus prometheus.yml defines when and where to look for metrics and any relevant alerting files. InfluxDB influxdb.conf prepares InfluxDB to receive metrics from JMeter. cAdvisor Specifically for Docker Compose, cAdvisor mounts to the actual Docker processes. alertmanager This can be used to set thresholds on metrics, and optionally send notifications. An example threshold is defined in configs/prometheus/alert.rules , and referenced in prometheus.yml . Sending notifications is defined in configs/alertmanager/config.yml . Grafana Grafana is a data visualizer. In the Grafana configurations, you'll find: The definition of datasources: datasources/datasource.yml . The definitions of dashboards. Runtime Data Grafana and Prometheus runtime data is stored in a Docker volume, so if you start and stop the containers, you'll not lose your work. However, it's still a good practice when building dashboards in Grafana to export the dashboard and add the JSON file to the dashboards folder. How Load is Generated \u00b6 Auto-Generated Load \u00b6 Traffic is generated in PingDirectory using our ldap-sdk-tools or apache-jmeter images. When PingDirectory is healthy, these tools will run as individual services based on the use case being implemented. You can view the logs of any of these services directly with docker-compose logs -f <service_name> . For example: docker-compose logs -f searchrate Generating Load \u00b6 Option 1 The most common way to generate load is by using the pingidentity/apache-jmeter image. To be effective with this tool, see JMeter usage . Option 2 To run another test using the ldap-sdk-tools utility, see ldap-sdk-tools . Option 3 Use tools available on the PingDirectory server: Shell into the PingDirectory server: docker container exec -it 10 -monitoring-stack_pingdirectory_1 sh Run the modrate tool. Enter: modrate \\ --hostname localhost --port 636 --bindDN cn = administrator --bindPassword 2FederateM0re \\ --entryDN \"uid=user.[0-4],ou=people,dc=example,dc=com\" \\ --useSSL --trustAll \\ --attribute description --valueLength 12 --numThreads 10 --ratePerSecond 20 modrate runs in the foreground in the container, so be ready to open another terminal if necessary to avoid stopping modrate . You'll see output similar to: PingDirectory:ca3f124e78aa:/opt > modrate \\ > --hostname localhost --port 636 --bindDN cn = administrator --bindPassword 2FederateM0re \\ > --entryDN \"uid=user.[0-4],ou=people,dc=example,dc=com\" \\ > --useSSL --trustAll \\ > --attribute description --valueLength 12 --numThreads 10 --ratePerSecond 20 Recent Recent Recent Overall Overall Mods/Sec Avg Dur ms Errors/Sec Mods/Sec Avg Dur ms ------------ ------------ ------------ ------------ ------------ 19 .998 5 .880 0 .000 19 .998 5 .880 19 .998 4 .214 0 .000 19 .998 5 .047 19 .999 3 .793 0 .000 19 .998 4 .629 20 .001 3 .608 0 .000 19 .999 4 .374 You also can return to the terminal running modrate after you change the modrate parameter settings to see the effect in Grafana. Display Metrics \u00b6 Metrics are displayed at these URLs: Tool Description Connection Details Grafana Data displayed in dashboards URL: http://localhost:3000 Username: admin Password: 2FederateM0re PingDirectory Raw StatsD data URL: http://localhost:9102/metrics Username: administrator Password: 2FederateM0re cAdvisor Container resource metrics URL: http://localhost:8080 node-exporter Raw node metrics URL: http://localhost:9100/metrics alertmanager Alerts displayed URL: http://localhost:9093/#/alerts Prometheus Query collected data URL: https://localhost:9090 The Grafana dashboards correspond to the dashboard definitions in configs/grafana/provisioning/dashboards . In Grafana, go to Dashboards -> Manage. The pre-populated dashboards with your live load results are displayed. Finishing Up \u00b6 To bring down the stack and remove the data stored in the Docker volumes, enter: docker-compose down docker volume rm 10 -monitoring-stack_grafana_data docker volume rm 10 -monitoring-stack_prometheus_data","title":"Deploy Monitoring Stack"},{"location":"deployment/deployMonitoringStack/#deploy-monitoring-stack","text":"This example illustrates how to use Cloud Native Computing Foundation (CNCF) monitoring tools with a PingDirectory stack. There are tools in this stack to: Tool Purpose Monitor Ping Identity Software Collect Metrics Prometheus Alertsmanager cAdvisor prometheus/statsd_exporter InfluxDB Display Metrics Grafana Generate Load pingidentity/ldap-sdk-tools pingidentity/apache-jmeter Prometheus Much of the generic Prometheus work is taken from the vegasbrianc/prometheus repository.","title":"Deploy Monitoring Stack"},{"location":"deployment/deployMonitoringStack/#prerequisites","text":"You've already been through Get Started to set up your DevOps environment and run a test deployment of the products. Pull our pingidentity-getting-started Git repo to ensure you have the latest sources.","title":"Prerequisites"},{"location":"deployment/deployMonitoringStack/#what-youll-do","text":"Deploy the stack Watch the load it generates Learn a bit about using the tools Display the metrics Clean up the stack PingDirectory produces a wide array of metrics. These metrics can be delivered in StatsD format to a location of your choosing using the StatsD monitoring endpoint for PingDirectory. See the PingDirectory documentation StatsD Monitoring Endpoint for more information.","title":"What You'll Do"},{"location":"deployment/deployMonitoringStack/#deploy-stack","text":"From pingidentity-devops-getting-started/11-docker-compose/10-monitoring-stack run: docker-compose up -d Running this command will: Deploy the PingIdentity software. Pull metrics from the Ping Identity software into Prometheus-enabled endpoints (such as, StatsD metrics using statsd_exporter , which formats and hosts the metrics). Have Prometheus scrape the /metrics endpoint on statsd_exporter . Generate load to have metrics worth looking at, and push the metrics from the client application (JMeter) to InfluxDB. Deploy a dashboard in Grafana to visualize the metrics from Prometheus and other tools. Wait for PingDirectory to become healthy. For example: docker container ls \\ --filter name = pingdirectory_1 \\ --format 'table {{.Names}}\\t{{.Status}}' The results displayed will be similar to this: NAMES STATUS 10 -monitoring-stack_pingdirectory_1 Up 2 hours ( healthy )","title":"Deploy Stack"},{"location":"deployment/deployMonitoringStack/#about-the-configuration","text":"There's a lot that can be discussed regarding the configuration. We'll focus on what is key to making this use case functional with minimal intervention, and describe what you may want to edit. View Configuration All relevant configurations are located in your local pingidentity-devops-getting-started/11-docker-compose/10-monitoring-stack/configs directory. The PingDirectory configuration looks like this: pingdirectory \u2514\u2500\u2500 pd.profile \u2514\u2500\u2500 dsconfig \u2514\u2500\u2500 15-prometheus-stats.dsconfig The baseline server profile. A single file with two dsconfig commands to create the StatsD monitoring endpoint and define where to push the metrics. Traditional profile layering is thought of as getting the profiles from multiple Git repos. However, sending a portion of a profile using the mounted /opt/in volume, and getting the rest of the profile information from a Git repo can still be considered layering. StatsD-Exporter The configuration file pingdirectory-statsd-mapping.yml defines which metrics to ingest and how to format them for Prometheus. This file is mounted to a location that is referenced from an argument passed to the startup command from the docker-compose.yaml file. Prometheus prometheus.yml defines when and where to look for metrics and any relevant alerting files. InfluxDB influxdb.conf prepares InfluxDB to receive metrics from JMeter. cAdvisor Specifically for Docker Compose, cAdvisor mounts to the actual Docker processes. alertmanager This can be used to set thresholds on metrics, and optionally send notifications. An example threshold is defined in configs/prometheus/alert.rules , and referenced in prometheus.yml . Sending notifications is defined in configs/alertmanager/config.yml . Grafana Grafana is a data visualizer. In the Grafana configurations, you'll find: The definition of datasources: datasources/datasource.yml . The definitions of dashboards. Runtime Data Grafana and Prometheus runtime data is stored in a Docker volume, so if you start and stop the containers, you'll not lose your work. However, it's still a good practice when building dashboards in Grafana to export the dashboard and add the JSON file to the dashboards folder.","title":"About the Configuration"},{"location":"deployment/deployMonitoringStack/#how-load-is-generated","text":"","title":"How Load is Generated"},{"location":"deployment/deployMonitoringStack/#auto-generated-load","text":"Traffic is generated in PingDirectory using our ldap-sdk-tools or apache-jmeter images. When PingDirectory is healthy, these tools will run as individual services based on the use case being implemented. You can view the logs of any of these services directly with docker-compose logs -f <service_name> . For example: docker-compose logs -f searchrate","title":"Auto-Generated Load"},{"location":"deployment/deployMonitoringStack/#generating-load","text":"Option 1 The most common way to generate load is by using the pingidentity/apache-jmeter image. To be effective with this tool, see JMeter usage . Option 2 To run another test using the ldap-sdk-tools utility, see ldap-sdk-tools . Option 3 Use tools available on the PingDirectory server: Shell into the PingDirectory server: docker container exec -it 10 -monitoring-stack_pingdirectory_1 sh Run the modrate tool. Enter: modrate \\ --hostname localhost --port 636 --bindDN cn = administrator --bindPassword 2FederateM0re \\ --entryDN \"uid=user.[0-4],ou=people,dc=example,dc=com\" \\ --useSSL --trustAll \\ --attribute description --valueLength 12 --numThreads 10 --ratePerSecond 20 modrate runs in the foreground in the container, so be ready to open another terminal if necessary to avoid stopping modrate . You'll see output similar to: PingDirectory:ca3f124e78aa:/opt > modrate \\ > --hostname localhost --port 636 --bindDN cn = administrator --bindPassword 2FederateM0re \\ > --entryDN \"uid=user.[0-4],ou=people,dc=example,dc=com\" \\ > --useSSL --trustAll \\ > --attribute description --valueLength 12 --numThreads 10 --ratePerSecond 20 Recent Recent Recent Overall Overall Mods/Sec Avg Dur ms Errors/Sec Mods/Sec Avg Dur ms ------------ ------------ ------------ ------------ ------------ 19 .998 5 .880 0 .000 19 .998 5 .880 19 .998 4 .214 0 .000 19 .998 5 .047 19 .999 3 .793 0 .000 19 .998 4 .629 20 .001 3 .608 0 .000 19 .999 4 .374 You also can return to the terminal running modrate after you change the modrate parameter settings to see the effect in Grafana.","title":"Generating Load"},{"location":"deployment/deployMonitoringStack/#display-metrics","text":"Metrics are displayed at these URLs: Tool Description Connection Details Grafana Data displayed in dashboards URL: http://localhost:3000 Username: admin Password: 2FederateM0re PingDirectory Raw StatsD data URL: http://localhost:9102/metrics Username: administrator Password: 2FederateM0re cAdvisor Container resource metrics URL: http://localhost:8080 node-exporter Raw node metrics URL: http://localhost:9100/metrics alertmanager Alerts displayed URL: http://localhost:9093/#/alerts Prometheus Query collected data URL: https://localhost:9090 The Grafana dashboards correspond to the dashboard definitions in configs/grafana/provisioning/dashboards . In Grafana, go to Dashboards -> Manage. The pre-populated dashboards with your live load results are displayed.","title":"Display Metrics"},{"location":"deployment/deployMonitoringStack/#finishing-up","text":"To bring down the stack and remove the data stored in the Docker volumes, enter: docker-compose down docker volume rm 10 -monitoring-stack_grafana_data docker volume rm 10 -monitoring-stack_prometheus_data","title":"Finishing Up"},{"location":"deployment/deployPFMultiRegionAWS/","text":"Kubernetes Multi Region Clustering using Native S3 Ping \u00b6 This document is specific to dynamic discovery with NATIVE_S3_PING and is an extension of PingFederate Cluster Across Multiple Kubernetes Clusters . This is the validated approach for PingFederate < 10.2 AWS S3 Prerequisites \u00b6 Create an S3 bucket with all appropriate security permissions Non-public Well scoped security policy, giving permissions to the service accounts running the EKS PingFederate clusters Encrypted Create an S3 bucket \u00b6 In the AWS console, select the S3 service. Select Buckets , and click Create Bucket . Enter a name for the bucket, select a region, and click Next . Enable the encrypt objects option, and any other options you need. Click Next . Select Block All Public Access , and click Next . Click Create Bucket . Select the bucket you just created from the displayed list. A window will open. Click Copy Bucker ARN , and retain this information for your security policy. Click on your bucket to open it, and click Permissions --> Bucket Policy . Use either the policy generator, or manually assign a security policy for the bucket that assigns the cluster user accounts these permissions: GetBucketLocation ListBucket DeleteObject /* GetObject /* PutObject /* The resource for GetBucketLocation and ListBucket is slightly different than the object permissions. The resource for GetBucketLocation and ListBucket is just the bucket ARN, but for the 3 object permissions, you must add \u201c/*\u201d on the end. What You'll Do \u00b6 You will deploy a multi-region adaptive Pingfederate cluster across multiple AWS EKS regional clusters. The kustomization.yaml in the 'engines' and 'admin-console' directories build on top of the standard DevOps PingFederate deployments. From each of these directories, running kustomize build . will generate Kubernetes yaml files that include: Two deployments: pingfederate-admin represents the admin console. pingfederate represents the engine(s) Two Configmaps. One for each deployment. These configmaps are nearly identical, but define the operational mode separately. The configmaps include a profile layer that turns on PingFederate Clustering. This layer simply includes: tcp.xml.subst run.properties.subst cluster-adaptive.conf.subst Two Services: One for each of the two deployments (9999 and 9031). PingFederate Engine Lifecycle \u00b6 Some features are added to the PingFederate Engine Deployment to support zero-downtime configuration deployments. explanations for these features are stored as comments in pingfederate-engine.yaml . Running \u00b6 Clone this repository to get the Kubernetes yaml and configuration files for the exercise, then: Bring up the admin console in the first Kubernetes cluster: cd admin-console Modify the 'env_vars.pingfederate-admin' file to include the name of the AWS S3 bucket, and the region of the S3 bucket to be used for the cluster list, as well as the appropriate region for adaptive clustering (PF_NODE_GROUP_ID) kustomize build . | kubectl apply -f - Wait for the pingfederate-admin pod to be running, then validate you can log into the console. You can port-forward the admin service and look at clustering via the admin console. kubectl port-forward svc/pingfederate 9999 :9999 Bring up one engine in the first Kubernetes cluster: cd ../engines Modify the 'env_vars.pingfederate-engine' file to include the name of the AWS S3 bucket, and the region of the S3 bucket to be used for the cluster list, as well as the appropriate region for adaptive clustering (PF_NODE_GROUP_ID) kustomize build . | kubectl apply -f - You can watch the admin console to make sure the engine appears in the cluster list. It would also be wise at this point to check the contents of the S3 bucket and make sure that both the IPs for the admin console and the engine node have been successfully written in. Scale up more engines in the first Kubernetes cluster: kubectl scale deployment pingfederate --replicas = 2 Again, validate that any new engines have successfully joined the cluster and written their IP to the S3 bucket Scale up engines in the 2nd Kubernetes cluster: Use kubectx to switch context to the 2nd Kubernetes cluster Modify the env_vars.pingfederate-engine file to include the second region for adaptive clustering (PF_NODE_GROUP_ID) kustomize build . | kubectl apply -f - kubectl scale deployment pingfederate --replicas = 2 Again, validate that any new engines have successfully joined the cluster and written their IP to the S3 bucket Cleanup Second Cluster (Engines Only) \u00b6 kubectl scale deployment/pingfederate --replicas = 0 cd engines kustomize build . | kubectl delete -f - Cleanup First Cluster (Engines & Admin) \u00b6 kubectx <first cluster> kubectl scale deployment/pingfederate --replicas = 0 kubectl scale deployment/pingfederate-admin --replicas = 0 kustomize build . | kubectl delete -f - cd ../admin-console kustomize build . | kubectl delete -f -","title":"PingFederate Across Kubernetes with S3"},{"location":"deployment/deployPFMultiRegionAWS/#kubernetes-multi-region-clustering-using-native-s3-ping","text":"This document is specific to dynamic discovery with NATIVE_S3_PING and is an extension of PingFederate Cluster Across Multiple Kubernetes Clusters . This is the validated approach for PingFederate < 10.2","title":"Kubernetes Multi Region Clustering using Native S3 Ping"},{"location":"deployment/deployPFMultiRegionAWS/#aws-s3-prerequisites","text":"Create an S3 bucket with all appropriate security permissions Non-public Well scoped security policy, giving permissions to the service accounts running the EKS PingFederate clusters Encrypted","title":"AWS S3 Prerequisites"},{"location":"deployment/deployPFMultiRegionAWS/#create-an-s3-bucket","text":"In the AWS console, select the S3 service. Select Buckets , and click Create Bucket . Enter a name for the bucket, select a region, and click Next . Enable the encrypt objects option, and any other options you need. Click Next . Select Block All Public Access , and click Next . Click Create Bucket . Select the bucket you just created from the displayed list. A window will open. Click Copy Bucker ARN , and retain this information for your security policy. Click on your bucket to open it, and click Permissions --> Bucket Policy . Use either the policy generator, or manually assign a security policy for the bucket that assigns the cluster user accounts these permissions: GetBucketLocation ListBucket DeleteObject /* GetObject /* PutObject /* The resource for GetBucketLocation and ListBucket is slightly different than the object permissions. The resource for GetBucketLocation and ListBucket is just the bucket ARN, but for the 3 object permissions, you must add \u201c/*\u201d on the end.","title":"Create an S3 bucket"},{"location":"deployment/deployPFMultiRegionAWS/#what-youll-do","text":"You will deploy a multi-region adaptive Pingfederate cluster across multiple AWS EKS regional clusters. The kustomization.yaml in the 'engines' and 'admin-console' directories build on top of the standard DevOps PingFederate deployments. From each of these directories, running kustomize build . will generate Kubernetes yaml files that include: Two deployments: pingfederate-admin represents the admin console. pingfederate represents the engine(s) Two Configmaps. One for each deployment. These configmaps are nearly identical, but define the operational mode separately. The configmaps include a profile layer that turns on PingFederate Clustering. This layer simply includes: tcp.xml.subst run.properties.subst cluster-adaptive.conf.subst Two Services: One for each of the two deployments (9999 and 9031).","title":"What You'll Do"},{"location":"deployment/deployPFMultiRegionAWS/#pingfederate-engine-lifecycle","text":"Some features are added to the PingFederate Engine Deployment to support zero-downtime configuration deployments. explanations for these features are stored as comments in pingfederate-engine.yaml .","title":"PingFederate Engine Lifecycle"},{"location":"deployment/deployPFMultiRegionAWS/#running","text":"Clone this repository to get the Kubernetes yaml and configuration files for the exercise, then: Bring up the admin console in the first Kubernetes cluster: cd admin-console Modify the 'env_vars.pingfederate-admin' file to include the name of the AWS S3 bucket, and the region of the S3 bucket to be used for the cluster list, as well as the appropriate region for adaptive clustering (PF_NODE_GROUP_ID) kustomize build . | kubectl apply -f - Wait for the pingfederate-admin pod to be running, then validate you can log into the console. You can port-forward the admin service and look at clustering via the admin console. kubectl port-forward svc/pingfederate 9999 :9999 Bring up one engine in the first Kubernetes cluster: cd ../engines Modify the 'env_vars.pingfederate-engine' file to include the name of the AWS S3 bucket, and the region of the S3 bucket to be used for the cluster list, as well as the appropriate region for adaptive clustering (PF_NODE_GROUP_ID) kustomize build . | kubectl apply -f - You can watch the admin console to make sure the engine appears in the cluster list. It would also be wise at this point to check the contents of the S3 bucket and make sure that both the IPs for the admin console and the engine node have been successfully written in. Scale up more engines in the first Kubernetes cluster: kubectl scale deployment pingfederate --replicas = 2 Again, validate that any new engines have successfully joined the cluster and written their IP to the S3 bucket Scale up engines in the 2nd Kubernetes cluster: Use kubectx to switch context to the 2nd Kubernetes cluster Modify the env_vars.pingfederate-engine file to include the second region for adaptive clustering (PF_NODE_GROUP_ID) kustomize build . | kubectl apply -f - kubectl scale deployment pingfederate --replicas = 2 Again, validate that any new engines have successfully joined the cluster and written their IP to the S3 bucket","title":"Running"},{"location":"deployment/deployPFMultiRegionAWS/#cleanup-second-cluster-engines-only","text":"kubectl scale deployment/pingfederate --replicas = 0 cd engines kustomize build . | kubectl delete -f -","title":"Cleanup Second Cluster (Engines Only)"},{"location":"deployment/deployPFMultiRegionAWS/#cleanup-first-cluster-engines-admin","text":"kubectx <first cluster> kubectl scale deployment/pingfederate --replicas = 0 kubectl scale deployment/pingfederate-admin --replicas = 0 kustomize build . | kubectl delete -f - cd ../admin-console kustomize build . | kubectl delete -f -","title":"Cleanup First Cluster (Engines &amp; Admin)"},{"location":"deployment/deployPFMultiRegionDNS/","text":"Kubernetes Multi Region Clustering using DNS \u00b6 This document is specific to dynamic discovery with DNS_PING and is an extension of PingFederate Cluster Across Multiple Kubernetes Clusters . This is the recommended approach for GA PingFederate 10.2+ Overview \u00b6 The PingIdentity PingFederate Docker image default instance/server/default/conf/tcp.xml file points to DNS_PING. Once you have two peered Kubernetes clusters, to span a PingFederate cluster across becomes easy. A single pingfederate cluster using DNS_PING queries a local headless service. In this example we use externalDNS to give an externalName to the headless service. externalDNS makes a corresponding record on AWS Route53 and constantly updates it with container ips of the backend PF engines. External DNS if unable to use externalDNS , another way to expose the headless service across clusters is needed. HAProxy may be viable. What You'll Do \u00b6 Edit the externalName of the pingfederate-cluster service and the DNS_QUERY_LOCATION variable as needed search on the files for # CHANGEME Deploy the clusters Cleanup Running \u00b6 Prerequisites \u00b6 PingFederate Cluster Across Multiple Kubernetes Clusters externalDNS Clone the getting-started Repository to get the Kubernetes yaml and configuration files for the exercise (20-kubernetes/14-dns-pingfederate-multiregion), then: Look through the files, there are embedded comments to explain the purpose of its structure. Fix the first un-commented line under any # CHANGEME in the files. This will be the Kubernetes namespace and the will be the externalName of the pingfederate-cluster service. Deploy the first cluster kubectl apply -f 01 -east.yaml Wait for the pingfederate-admin pod to be running, then validate you can log into the console. You can port-forward the admin service and look at clustering via the admin console. kubectl port-forward svc/pingfederate 9999 :9999 If you have console access to AWS Route53, you should be able to find the externalName specified and see the IPs of your containers. Switch Kubernetes context to second cluster Deploy the second cluster kubectl apply -f 02 -west.yaml Cleanup Clusters \u00b6 kubectl delete -f 02 -west.yaml Switch Kubernetes context to second cluster kubectl delete -f 01 -east.yaml","title":"PingFederate Across Kubernetes with DNS"},{"location":"deployment/deployPFMultiRegionDNS/#kubernetes-multi-region-clustering-using-dns","text":"This document is specific to dynamic discovery with DNS_PING and is an extension of PingFederate Cluster Across Multiple Kubernetes Clusters . This is the recommended approach for GA PingFederate 10.2+","title":"Kubernetes Multi Region Clustering using DNS"},{"location":"deployment/deployPFMultiRegionDNS/#overview","text":"The PingIdentity PingFederate Docker image default instance/server/default/conf/tcp.xml file points to DNS_PING. Once you have two peered Kubernetes clusters, to span a PingFederate cluster across becomes easy. A single pingfederate cluster using DNS_PING queries a local headless service. In this example we use externalDNS to give an externalName to the headless service. externalDNS makes a corresponding record on AWS Route53 and constantly updates it with container ips of the backend PF engines. External DNS if unable to use externalDNS , another way to expose the headless service across clusters is needed. HAProxy may be viable.","title":"Overview"},{"location":"deployment/deployPFMultiRegionDNS/#what-youll-do","text":"Edit the externalName of the pingfederate-cluster service and the DNS_QUERY_LOCATION variable as needed search on the files for # CHANGEME Deploy the clusters Cleanup","title":"What You'll Do"},{"location":"deployment/deployPFMultiRegionDNS/#running","text":"","title":"Running"},{"location":"deployment/deployPFMultiRegionDNS/#prerequisites","text":"PingFederate Cluster Across Multiple Kubernetes Clusters externalDNS Clone the getting-started Repository to get the Kubernetes yaml and configuration files for the exercise (20-kubernetes/14-dns-pingfederate-multiregion), then: Look through the files, there are embedded comments to explain the purpose of its structure. Fix the first un-commented line under any # CHANGEME in the files. This will be the Kubernetes namespace and the will be the externalName of the pingfederate-cluster service. Deploy the first cluster kubectl apply -f 01 -east.yaml Wait for the pingfederate-admin pod to be running, then validate you can log into the console. You can port-forward the admin service and look at clustering via the admin console. kubectl port-forward svc/pingfederate 9999 :9999 If you have console access to AWS Route53, you should be able to find the externalName specified and see the IPs of your containers. Switch Kubernetes context to second cluster Deploy the second cluster kubectl apply -f 02 -west.yaml","title":"Prerequisites"},{"location":"deployment/deployPFMultiRegionDNS/#cleanup-clusters","text":"kubectl delete -f 02 -west.yaml Switch Kubernetes context to second cluster kubectl delete -f 01 -east.yaml","title":"Cleanup Clusters"},{"location":"deployment/deployPaCluster/","text":"Deploy PingAccess Cluster \u00b6 This use case employs the pingidentity-server-profiles/pa-clustering server profile. This server profile contains an H2 database engine located in pingidentity-server-profiles/pa-clustering/pingaccess/instance/data/PingAccess.mv.db . H2 is configured to reference the PingAccess Admin engine at pingaccess:9090 . Remember to include this if you create your own server profile. This setting is not contained in an exported PingAccess configuration archive. Prerequisites \u00b6 You've already been through Get Started to set up your DevOps environment and run a test deployment of the products. What You'll Do \u00b6 Deploy the PingAccess cluster. Replicate the cluster configuration. Scale the PingAccess engines. Deploy Cluster \u00b6 You'll use the docker-compose.yaml file in your local pingidentity-devops-getting-started/11-docker-compose/06-pingaccess-cluster directory to deploy the cluster. From the pingidentity-devops-getting-started/11-docker-compose/06-pingaccess-cluster directory, start the stack. Enter: docker-compose up -d Check that the containers are healthy and running: docker-compose ps You can also display the startup logs: docker-compose logs -f To see the logs for a particular product container at any point, enter: docker-compose logs <product-container-name> Log in to the PingAccess Administrator Console: Product Connection Details PingAccess URL: https://localhost:9000 Username: administrator Password: 2FederateM0re Verify Cluster Status \u00b6 Check the status of the cluster using either the admin console or the Admin REST API: To use the Administrator Console: Log in to the Administrator Console: https://localhost:9000 Go to Settings. You should see your engine(s) here. To use the PingAccess Admin REST API, enter: curl -k -u administrator:2FederateM0re \\ -H 'X-XSRF-Header: PingAccess' https://localhost:9000/pa-admin-api/v3/engines The resulting response will be similar to this: { \"items\" :[ { \"id\" : 1 , \"name\" : \"1e0e17125564\" , \"description\" : null , \"configReplicationEnabled\" : true , \"keys\" :[ { \"jwk\" :{ \"kty\" : \"EC\" , \"kid\" : \"41097511-9945-49df-8a43-f463fb9fe353\" , \"x\" : \"-tZ6kNF1o2QCAK6bIG2DeGqpOnp6V6HJZcPhUJ3JbZ8\" , \"y\" : \"lO_BkXLnGLSiC4O7TPmWBDk2YOHuqno61QInkgL7-5M\" , \"crv\" : \"P-256\" }, \"created\" : 1582783126865 } ], \"httpProxyId\" : 0 , \"httpsProxyId\" : 0 , \"selectedCertificateId\" : 5 , \"certificateHash\" :{ \"algorithm\" : \"SHA1\" , \"hexValue\" : \"e8a4cc6163fce9b7216b284ef635306f07be381b\" } } ] } Scale Engines \u00b6 To scale up to 2 engine nodes: docker-compose up -d --scale pingaccess-engine = 2 Clean Up \u00b6 When you no longer want to run this stack, bring the stack down. To remove all of the containers and associated Docker networks, enter: docker-compose down To stop the running stack without removing any of the containers or associated Docker networks, enter: docker-compose stop To remove attached Docker Volumes docker volume prune","title":"Deploy PingAccess Cluster"},{"location":"deployment/deployPaCluster/#deploy-pingaccess-cluster","text":"This use case employs the pingidentity-server-profiles/pa-clustering server profile. This server profile contains an H2 database engine located in pingidentity-server-profiles/pa-clustering/pingaccess/instance/data/PingAccess.mv.db . H2 is configured to reference the PingAccess Admin engine at pingaccess:9090 . Remember to include this if you create your own server profile. This setting is not contained in an exported PingAccess configuration archive.","title":"Deploy PingAccess Cluster"},{"location":"deployment/deployPaCluster/#prerequisites","text":"You've already been through Get Started to set up your DevOps environment and run a test deployment of the products.","title":"Prerequisites"},{"location":"deployment/deployPaCluster/#what-youll-do","text":"Deploy the PingAccess cluster. Replicate the cluster configuration. Scale the PingAccess engines.","title":"What You'll Do"},{"location":"deployment/deployPaCluster/#deploy-cluster","text":"You'll use the docker-compose.yaml file in your local pingidentity-devops-getting-started/11-docker-compose/06-pingaccess-cluster directory to deploy the cluster. From the pingidentity-devops-getting-started/11-docker-compose/06-pingaccess-cluster directory, start the stack. Enter: docker-compose up -d Check that the containers are healthy and running: docker-compose ps You can also display the startup logs: docker-compose logs -f To see the logs for a particular product container at any point, enter: docker-compose logs <product-container-name> Log in to the PingAccess Administrator Console: Product Connection Details PingAccess URL: https://localhost:9000 Username: administrator Password: 2FederateM0re","title":"Deploy Cluster"},{"location":"deployment/deployPaCluster/#verify-cluster-status","text":"Check the status of the cluster using either the admin console or the Admin REST API: To use the Administrator Console: Log in to the Administrator Console: https://localhost:9000 Go to Settings. You should see your engine(s) here. To use the PingAccess Admin REST API, enter: curl -k -u administrator:2FederateM0re \\ -H 'X-XSRF-Header: PingAccess' https://localhost:9000/pa-admin-api/v3/engines The resulting response will be similar to this: { \"items\" :[ { \"id\" : 1 , \"name\" : \"1e0e17125564\" , \"description\" : null , \"configReplicationEnabled\" : true , \"keys\" :[ { \"jwk\" :{ \"kty\" : \"EC\" , \"kid\" : \"41097511-9945-49df-8a43-f463fb9fe353\" , \"x\" : \"-tZ6kNF1o2QCAK6bIG2DeGqpOnp6V6HJZcPhUJ3JbZ8\" , \"y\" : \"lO_BkXLnGLSiC4O7TPmWBDk2YOHuqno61QInkgL7-5M\" , \"crv\" : \"P-256\" }, \"created\" : 1582783126865 } ], \"httpProxyId\" : 0 , \"httpsProxyId\" : 0 , \"selectedCertificateId\" : 5 , \"certificateHash\" :{ \"algorithm\" : \"SHA1\" , \"hexValue\" : \"e8a4cc6163fce9b7216b284ef635306f07be381b\" } } ] }","title":"Verify Cluster Status"},{"location":"deployment/deployPaCluster/#scale-engines","text":"To scale up to 2 engine nodes: docker-compose up -d --scale pingaccess-engine = 2","title":"Scale Engines"},{"location":"deployment/deployPaCluster/#clean-up","text":"When you no longer want to run this stack, bring the stack down. To remove all of the containers and associated Docker networks, enter: docker-compose down To stop the running stack without removing any of the containers or associated Docker networks, enter: docker-compose stop To remove attached Docker Volumes docker volume prune","title":"Clean Up"},{"location":"deployment/deployPdgPap/","text":"Deploy PingDataGovernance with an External Policy Administration Point \u00b6 This example describes how to build PingDataGovernance policies, and employs server profile layering. The base profile, pingidentity-server-profiles/baseline/pingdatagovernance , configures PingDirectory and PingDataGovernance to proxy the PingDirectory Rest API and uses an embedded PingDataGovernance policy as the Policy Decision Service. A second layer pingidentity-server-profiles/pdg-pap-integration switches the Policy Decision Service to use an external PingDataGovernance Policy Administration Point (PDG-PAP). Prerequisites \u00b6 You've already been through Get started to set up your DevOps environment and run a test deployment of the products. What You'll Do \u00b6 Deploy the stack . Log in to the management consoles . Test the default use case . Build and test your own policy . Clean up . Deploy The Stack \u00b6 Go to your local 11-docker-compose/07-pingdatagovernance directory, and enter: docker-compose up -d When all of the containers are healthy, you can start testing. Login To The Management Consoles \u00b6 Product Connection Details PingDirectory URL: https://localhost:9443/console Server: pingdirectory:636 Username: administrator Password: 2FederateM0re PingDataGovernance URL: https://localhost:9443/console Server: pingdatagovernance:636 Username: administrator Password: 2FederateM0re PingDataGovernance PAP URL: https://localhost:8443 Username: admin Password: password123 Test Default Use Case \u00b6 The default use case does the following: Proxies the PingDirectory Rest API using a mock access token validator. If the passed bearer token is valid, PDG-PAP allows it to be forwarded to PingDirectory. PingDirectory uses the sub field in the token along with the the URL path to look up and return a users data. On the returned data, PDG-PAP accepts the response and allows it to be returned to the requestor. To test this use case: Access the PingDataGovernance server: curl -k 'https://localhost:7443/pd-rest-api/uid=user.1,ou=people,dc=example,dc=com' \\ --header 'Authorization: Bearer { \"active\":true,\"sub\" : \"user.1\", \"clientId\":\"client1\",\"scope\":\"ds\" }' Monitor the logs. To watch a request flow through all of the tools, you can tail -f each of these logs: PingDataGovernance: docker container exec -it \\ 07 -pingdatagovernance_pingdatagovernance_1 \\ tail -f /opt/out/instance/logs/policy-decision ( Ctrl+c to exit) PAP (standard container logs): docker container logs -f 07 -pingdatagovernance_pingdatagovernancepap_1 PingDirectory (standard container logs). Because the baseline profile has debug mode on, when you make a successful request through PingDataGovernance to PingDirectory, you'll see successful BIND and SEARCH logs containing the user you searched for: docker container logs -f 07 -pingdatagovernance_pingdirectory_1 Display the configurations in Data Console: Gateway API endpoints Select the API endpoints that are being proxied to display the related information. Policy Decision Service Select which policy will be used to govern data and access, and display the related information. PDP mode Select embedded for the default policy, or import a deployment package. Select external to use PAP. External Servers Display the PAP configuration and define the policy that is being used on PAP. Build and Test Your Own Policy \u00b6 Open PAP. Define a policy. Select external for the Policy Decision Service in Data Console. In Data Console, go to External Servers -> pingdatagovernancepap , and enter your policy name in the branch field. Save your changes. Make a request to the PingDataGovernance server again (as you did when testing the default use case): curl -k 'https://localhost:7443/pd-rest-api/uid=user.1,ou=people,dc=example,dc=com' \\ --header 'Authorization: Bearer { \"active\":true,\"sub\" : \"user.1\", \"clientId\":\"client1\",\"scope\":\"ds\" }' Watch the same logs, and you'll see your policy being used. In PDG-PAP, this will look similar to: 172 .20.0.3 - - [ 20 /May/2020:15:27:06 +0000 ] \"POST /api/governance-engine?decision-node=e51688ff-1dc9-4b6c-bb36-8af64d02e9d1&branch=<YOUR POLICY BRANCH NAME HERE> HTTP/1.1\" 400 118 \"-\" \"Jersey/2.17 (Apache HttpClient 4.5)\" 6 If you want further confirmation, in the Data Console, go to External Servers -> pingdatagovernancepap and put some \"junk\" in the branch box. You'll see that PingDataGovernance is unable to find the policy branch. Clean Up \u00b6 When you no longer want to run this stack, bring the stack down. To remove all of the containers and associated Docker networks, enter: docker-compose down To stop the running stack without removing any of the containers or associated Docker networks, enter: docker-compose stop To remove attached Docker Volumes docker volume prune","title":"Deploy PingDataGovernance with PAP"},{"location":"deployment/deployPdgPap/#deploy-pingdatagovernance-with-an-external-policy-administration-point","text":"This example describes how to build PingDataGovernance policies, and employs server profile layering. The base profile, pingidentity-server-profiles/baseline/pingdatagovernance , configures PingDirectory and PingDataGovernance to proxy the PingDirectory Rest API and uses an embedded PingDataGovernance policy as the Policy Decision Service. A second layer pingidentity-server-profiles/pdg-pap-integration switches the Policy Decision Service to use an external PingDataGovernance Policy Administration Point (PDG-PAP).","title":"Deploy PingDataGovernance with an External Policy Administration Point"},{"location":"deployment/deployPdgPap/#prerequisites","text":"You've already been through Get started to set up your DevOps environment and run a test deployment of the products.","title":"Prerequisites"},{"location":"deployment/deployPdgPap/#what-youll-do","text":"Deploy the stack . Log in to the management consoles . Test the default use case . Build and test your own policy . Clean up .","title":"What You'll Do"},{"location":"deployment/deployPdgPap/#deploy-the-stack","text":"Go to your local 11-docker-compose/07-pingdatagovernance directory, and enter: docker-compose up -d When all of the containers are healthy, you can start testing.","title":"Deploy The Stack"},{"location":"deployment/deployPdgPap/#login-to-the-management-consoles","text":"Product Connection Details PingDirectory URL: https://localhost:9443/console Server: pingdirectory:636 Username: administrator Password: 2FederateM0re PingDataGovernance URL: https://localhost:9443/console Server: pingdatagovernance:636 Username: administrator Password: 2FederateM0re PingDataGovernance PAP URL: https://localhost:8443 Username: admin Password: password123","title":"Login To The Management Consoles"},{"location":"deployment/deployPdgPap/#test-default-use-case","text":"The default use case does the following: Proxies the PingDirectory Rest API using a mock access token validator. If the passed bearer token is valid, PDG-PAP allows it to be forwarded to PingDirectory. PingDirectory uses the sub field in the token along with the the URL path to look up and return a users data. On the returned data, PDG-PAP accepts the response and allows it to be returned to the requestor. To test this use case: Access the PingDataGovernance server: curl -k 'https://localhost:7443/pd-rest-api/uid=user.1,ou=people,dc=example,dc=com' \\ --header 'Authorization: Bearer { \"active\":true,\"sub\" : \"user.1\", \"clientId\":\"client1\",\"scope\":\"ds\" }' Monitor the logs. To watch a request flow through all of the tools, you can tail -f each of these logs: PingDataGovernance: docker container exec -it \\ 07 -pingdatagovernance_pingdatagovernance_1 \\ tail -f /opt/out/instance/logs/policy-decision ( Ctrl+c to exit) PAP (standard container logs): docker container logs -f 07 -pingdatagovernance_pingdatagovernancepap_1 PingDirectory (standard container logs). Because the baseline profile has debug mode on, when you make a successful request through PingDataGovernance to PingDirectory, you'll see successful BIND and SEARCH logs containing the user you searched for: docker container logs -f 07 -pingdatagovernance_pingdirectory_1 Display the configurations in Data Console: Gateway API endpoints Select the API endpoints that are being proxied to display the related information. Policy Decision Service Select which policy will be used to govern data and access, and display the related information. PDP mode Select embedded for the default policy, or import a deployment package. Select external to use PAP. External Servers Display the PAP configuration and define the policy that is being used on PAP.","title":"Test Default Use Case"},{"location":"deployment/deployPdgPap/#build-and-test-your-own-policy","text":"Open PAP. Define a policy. Select external for the Policy Decision Service in Data Console. In Data Console, go to External Servers -> pingdatagovernancepap , and enter your policy name in the branch field. Save your changes. Make a request to the PingDataGovernance server again (as you did when testing the default use case): curl -k 'https://localhost:7443/pd-rest-api/uid=user.1,ou=people,dc=example,dc=com' \\ --header 'Authorization: Bearer { \"active\":true,\"sub\" : \"user.1\", \"clientId\":\"client1\",\"scope\":\"ds\" }' Watch the same logs, and you'll see your policy being used. In PDG-PAP, this will look similar to: 172 .20.0.3 - - [ 20 /May/2020:15:27:06 +0000 ] \"POST /api/governance-engine?decision-node=e51688ff-1dc9-4b6c-bb36-8af64d02e9d1&branch=<YOUR POLICY BRANCH NAME HERE> HTTP/1.1\" 400 118 \"-\" \"Jersey/2.17 (Apache HttpClient 4.5)\" 6 If you want further confirmation, in the Data Console, go to External Servers -> pingdatagovernancepap and put some \"junk\" in the branch box. You'll see that PingDataGovernance is unable to find the policy branch.","title":"Build and Test Your Own Policy"},{"location":"deployment/deployPdgPap/#clean-up","text":"When you no longer want to run this stack, bring the stack down. To remove all of the containers and associated Docker networks, enter: docker-compose down To stop the running stack without removing any of the containers or associated Docker networks, enter: docker-compose stop To remove attached Docker Volumes docker volume prune","title":"Clean Up"},{"location":"deployment/deployPfCluster/","text":"Deploy PingFederate Cluster \u00b6 This use case employs server profile layering, using the PingFederate server profile in pingidentity-server-profiles/pf-dns-ping-clustering/pingfederate directory as the base layer profile. This server profile contains two files critical to PingFederate clustering: tcp.xml.subst Specifies usage of DNS_PING for clustering and expects the environment variable, DNS_QUERY_LOCATION to be passed. run.properties.subst Indicates to the PingFederate container which OPERATIONAL_MODE the container is to be used. The environment variables CLUSTERED_CONSOLE or CLUSTERED_ENGINE need to be passed. The file structure for these files in pingidentity-server-profiles/pf-dns-ping-clustering/pingfederate looks like this: . \u2514\u2500\u2500 pingfederate \u2514\u2500\u2500 instance \u251c\u2500\u2500 bin \u2502 \u2514\u2500\u2500 run.properties.subst \u2514\u2500\u2500 server \u2514\u2500\u2500 default \u2514\u2500\u2500 conf \u2514\u2500\u2500 tcp.xml.subst The top profile layer uses the server profile in pingidentity-server-profiles/getting-started/pingfederate . See Layering Server Profiles for more information about using server profiles. Prerequisites \u00b6 You've already been through Get Started to set up your DevOps environment and run a test deployment of the products. PingFederate build image for version 10 or greater. (The DNS Discovery feature first available in version 10 is needed.) What You'll Do \u00b6 Deploy the PingFederate cluster. Verify the cluster status. Replicate the cluster configuration. Scale the PingFederate engines. Deploy Cluster \u00b6 You'll use the docker-compose.yaml file in your local pingidentity-devops-getting-started/11-docker-compose/05-pingfederate-cluster directory to deploy the cluster. From the pingidentity-devops-getting-started/11-docker-compose/05-pingfederate-cluster directory, start the stack. Enter: docker-compose up -d Check that the containers are healthy and running: docker-compose ps You can also display the startup logs: docker-compose logs -f To see the logs for a particular product container at any point, enter: docker-compose logs <product-container-name> Log in to the PingFederate Administrator Console: Product Connection Details PingFederate URL: https://localhost:9999/pingfederate/app Username: administrator Password: 2FederateM0re Verify Cluster Status \u00b6 Check the status of the cluster using either or the PingFederate Admin REST API: To use the Administrator Console: Log in to the Administrator Console: https://localhost:9999/pingfederate/app . Go to System --> Cluster Management and click Cluster Status . To use the PingFederate Admin REST API, enter: curl -u administrator:2FederateM0re \\ -k 'https://localhost:9999/pf-admin-api/v1/cluster/status' \\ --header 'x-xsrf-header: PingFederate' The resulting response will be similar to this: { \"nodes\" :[ { \"address\" : \"169.254.1.2:7600\" , \"mode\" : \"CLUSTERED_CONSOLE\" , \"index\" : 804046313 , \"nodeGroup\" : \"\" , \"version\" : \"10.0.0.15\" }, { \"address\" : \"169.254.1.3:7600\" , \"mode\" : \"CLUSTERED_ENGINE\" , \"index\" : 2142569058 , \"nodeGroup\" : \"\" , \"version\" : \"10.0.0.15\" , \"nodeTags\" : \"\" } ], \"lastConfigUpdateTime\" : \"2020-12-31T19:36:54.000Z\" , \"replicationRequired\" : true , \"mixedMode\" : false } Replicate Configuration \u00b6 Replicate configuration across the cluster using the either the PingFederate Administrator Console or the PingFederate Admin REST API: To use the Administrator Console: Log in to the Administrator Console: https://localhost:9999/pingfederate/app . Go to System --> Cluster Management and click Replicate Configuration . To use the PingFederate Admin REST API, enter: curl -X POST \\ -u administrator:2FederateM0re \\ -k 'https://localhost:9999/pf-admin-api/v1/cluster/replicate' \\ --header 'x-xsrf-header: PingFederate' The resulting response will be similar to this: { \"resultId\" : \"success\" , \"message\" : \"Operation succeeded.\" } Scale Engines \u00b6 To scale up to 2 engine nodes: docker-compose up -d --scale pingfederate = 2 Clean Up \u00b6 When you no longer want to run this stack, bring the stack down. To remove all of the containers and associated Docker networks, enter: docker-compose down To stop the running stack without removing any of the containers or associated Docker networks, enter: docker-compose stop To remove attached Docker Volumes docker volume prune","title":"Deploy PingFederate Cluster"},{"location":"deployment/deployPfCluster/#deploy-pingfederate-cluster","text":"This use case employs server profile layering, using the PingFederate server profile in pingidentity-server-profiles/pf-dns-ping-clustering/pingfederate directory as the base layer profile. This server profile contains two files critical to PingFederate clustering: tcp.xml.subst Specifies usage of DNS_PING for clustering and expects the environment variable, DNS_QUERY_LOCATION to be passed. run.properties.subst Indicates to the PingFederate container which OPERATIONAL_MODE the container is to be used. The environment variables CLUSTERED_CONSOLE or CLUSTERED_ENGINE need to be passed. The file structure for these files in pingidentity-server-profiles/pf-dns-ping-clustering/pingfederate looks like this: . \u2514\u2500\u2500 pingfederate \u2514\u2500\u2500 instance \u251c\u2500\u2500 bin \u2502 \u2514\u2500\u2500 run.properties.subst \u2514\u2500\u2500 server \u2514\u2500\u2500 default \u2514\u2500\u2500 conf \u2514\u2500\u2500 tcp.xml.subst The top profile layer uses the server profile in pingidentity-server-profiles/getting-started/pingfederate . See Layering Server Profiles for more information about using server profiles.","title":"Deploy PingFederate Cluster"},{"location":"deployment/deployPfCluster/#prerequisites","text":"You've already been through Get Started to set up your DevOps environment and run a test deployment of the products. PingFederate build image for version 10 or greater. (The DNS Discovery feature first available in version 10 is needed.)","title":"Prerequisites"},{"location":"deployment/deployPfCluster/#what-youll-do","text":"Deploy the PingFederate cluster. Verify the cluster status. Replicate the cluster configuration. Scale the PingFederate engines.","title":"What You'll Do"},{"location":"deployment/deployPfCluster/#deploy-cluster","text":"You'll use the docker-compose.yaml file in your local pingidentity-devops-getting-started/11-docker-compose/05-pingfederate-cluster directory to deploy the cluster. From the pingidentity-devops-getting-started/11-docker-compose/05-pingfederate-cluster directory, start the stack. Enter: docker-compose up -d Check that the containers are healthy and running: docker-compose ps You can also display the startup logs: docker-compose logs -f To see the logs for a particular product container at any point, enter: docker-compose logs <product-container-name> Log in to the PingFederate Administrator Console: Product Connection Details PingFederate URL: https://localhost:9999/pingfederate/app Username: administrator Password: 2FederateM0re","title":"Deploy Cluster"},{"location":"deployment/deployPfCluster/#verify-cluster-status","text":"Check the status of the cluster using either or the PingFederate Admin REST API: To use the Administrator Console: Log in to the Administrator Console: https://localhost:9999/pingfederate/app . Go to System --> Cluster Management and click Cluster Status . To use the PingFederate Admin REST API, enter: curl -u administrator:2FederateM0re \\ -k 'https://localhost:9999/pf-admin-api/v1/cluster/status' \\ --header 'x-xsrf-header: PingFederate' The resulting response will be similar to this: { \"nodes\" :[ { \"address\" : \"169.254.1.2:7600\" , \"mode\" : \"CLUSTERED_CONSOLE\" , \"index\" : 804046313 , \"nodeGroup\" : \"\" , \"version\" : \"10.0.0.15\" }, { \"address\" : \"169.254.1.3:7600\" , \"mode\" : \"CLUSTERED_ENGINE\" , \"index\" : 2142569058 , \"nodeGroup\" : \"\" , \"version\" : \"10.0.0.15\" , \"nodeTags\" : \"\" } ], \"lastConfigUpdateTime\" : \"2020-12-31T19:36:54.000Z\" , \"replicationRequired\" : true , \"mixedMode\" : false }","title":"Verify Cluster Status"},{"location":"deployment/deployPfCluster/#replicate-configuration","text":"Replicate configuration across the cluster using the either the PingFederate Administrator Console or the PingFederate Admin REST API: To use the Administrator Console: Log in to the Administrator Console: https://localhost:9999/pingfederate/app . Go to System --> Cluster Management and click Replicate Configuration . To use the PingFederate Admin REST API, enter: curl -X POST \\ -u administrator:2FederateM0re \\ -k 'https://localhost:9999/pf-admin-api/v1/cluster/replicate' \\ --header 'x-xsrf-header: PingFederate' The resulting response will be similar to this: { \"resultId\" : \"success\" , \"message\" : \"Operation succeeded.\" }","title":"Replicate Configuration"},{"location":"deployment/deployPfCluster/#scale-engines","text":"To scale up to 2 engine nodes: docker-compose up -d --scale pingfederate = 2","title":"Scale Engines"},{"location":"deployment/deployPfCluster/#clean-up","text":"When you no longer want to run this stack, bring the stack down. To remove all of the containers and associated Docker networks, enter: docker-compose down To stop the running stack without removing any of the containers or associated Docker networks, enter: docker-compose stop To remove attached Docker Volumes docker volume prune","title":"Clean Up"},{"location":"deployment/deployPingCentral/","text":"Deploy PingCentral \u00b6 This use case employs the pingidentity-server-profiles/baseline/pingcentral server profile. This server profile contains a MySQL database engine located in pingidentity-server-profiles/baseline/pingcentral/external-mysql-db . Prerequisites \u00b6 You've already been through Get Started to set up your DevOps environment and run a test deployment of the products. Either: Clone the pingidentity-server-profiles repository to your local ${HOME}/projects/devops directory. Fork the pingidentity-server-profiles repository to your Github repository, then clone this repository to a local directory. What You'll Do \u00b6 Deploy the stack. Log in to the management consoles. Bring down or stop the stack. Preserve the database. Configure trust for PingCentral. Configure SSO for PingCentral Deploy Stack \u00b6 You'll use the docker-compose.yaml file in your local pingidentity-devops-getting-started/11-docker-compose/30-pingcentral directory to deploy the cluster. Go to your local pingidentity-devops-getting-started/11-docker-compose/30-pingcentral directory. Enter: docker-compose up -d Check that the containers are healthy and running: docker-compose ps You can also display the startup logs: docker-compose logs -f To see the logs for a particular product container at any point, enter: docker-compose logs <product-container-name> Log in to the management consoles: Product Connection Details PingCentral URL: https://localhost:9022 Username: administrator Password: 2Federate Copy the MySQL database hostkey created on initial startup in ./conf/pingcentral.jwk to your local /tmp directory. You'll need the hostkey in a subsequent step. When you no longer want to run this stack, you can either stop the running stack, or bring the stack down. To stop the running stack without removing any of the containers or associated Docker networks, enter: docker-compose stop To remove all of the containers and associated Docker networks, enter: docker-compose down Preserve Database \u00b6 To preserve any updates to the MySQL database, you need to mount the ./conf/mysql/data directory to the /var/lib/mysql volume. You also need to mount ./conf/pingcentral.jwk to /opt/server/conf/pingcentral.jwk to save the hostkey file created on initial startup of the PingCentral container. You'll need the saved hostkey to access the database. If the stack is running, bring it down: docker-compose down Open the pingidentity-devops-getting-started/11-docker-compose/30-pingcentral/docker-compose.yml file and mount ./conf/mysql/data to the /var/lib/mysql volume under pingcentral-db . For example: pingcentral-db : image : mysql command : --default-authentication-plugin=mysql_native_password environment : MYSQL_ROOT_PASSWORD : 2Federate volumes : - ./conf/mysql/data:/var/lib/mysql ports : - \"3306:3306\" networks : - pingnet Keep the docker-compose.yml file open. In the pingidentity-devops-getting-started/11-docker-compose/30-pingcentral/docker-compose.yml file, also mount ./conf/pingcentral.jwk to the /opt/server/conf/pingcentral.jwk volume under the pingcentral service. For example: pingcentral : image : pingidentity/pingcentral:${PING_IDENTITY_DEVOPS_TAG} command : wait-for pingcentral-db:3306 -t 7200 -- entrypoint.sh start-server environment : - SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_PATH=baseline/pingcentral/external-mysql-db - PING_IDENTITY_ACCEPT_EULA=YES - PING_CENTRAL_BLIND_TRUST=true - PING_CENTRAL_VERIFY_HOSTNAME=false - MYSQL_USER=root - MYSQL_PASSWORD=2Federate env_file : - ~/.pingidentity/devops volumes : - ./conf/pingcentral.jwk:/opt/server/conf/pingcentral.jwk ports : - \"9022:9022\" depends_on : - \"pingcentral-db\" networks : - pingnet Save docker-compose.yml and start the stack: docker-compose up -d Copy the hostkey pingcentral.jwk file you saved to your local /tmp in a prior step to the /opt/out/instance/conf volume. Enter: docker cp /tmp/pingcentral.jwk:/opt/out/instance/conf The hostkey will now be persisted and available at each startup. Configure Trust \u00b6 By default, for the purposes of quick setup, the PingCentral container is insecure. This is due to the environment variable PING_CENTRAL_BLIND_TRUST=true setting in the docker-compose.yml file. By default, all certificates are trusted. Caution : Remember to change this setting for production environments. Setting PING_CENTRAL_BLIND_TRUST=false allows public certificates to be used only by your Ping Identity environments (such as PingFederate), unless you set up the trust store and configure PingCentral to use this trust store. To set up the trust in the container, first create your trust store according to the PingCentral documentation . Inject the trust store into the PingCentral container: For stacks, inject the trust store using the volumes definition in the docker-compose.yml file to mount ./conf/keystore.jks to /opt/in/instance/conf/keystore.jks . For example: services : pingcentral : volumes : - ./conf/keystore.jks:/opt/in/instance/conf/keystore.jks For standalone PingCentral containers, use: docker run --volume \\ ./conf/keystore.jks:/opt/in/instance/conf/keystore.jks Configure PingCentral to use the created trust either by using environment variables or the properties file: Using environment variables For stacks, specify these environment variables in the environment definition of the docker-compose.yml file: services : pingcentral : environment : - server.ssl.trust-any=false - server.ssl.https.verify-hostname=false - server.ssl.delegate-to-system=false - server.ssl.trust-store=/opt/in/instance/conf/keystore.jks - server.ssl.trust-store-password=InsertTruststorePasswordHere For standalone PingCentral containers: docker run --env server.ssl.trust-any = false \\ --env server.ssl.https.verify-hostname = false \\ --env server.ssl.delegate-to-system = false \\ --env server.ssl.trust-store = /opt/in/instance/conf/keystore.jks \\ --env server.ssl.trust-store-password = <InsertTruststorePasswordHere> Using properties files Update the following properties in your pingidentity-server-profiles/baseline/pingcentral/external-mysql-db/instance/conf/application.properties.subst file: server.ssl.trust-any server.ssl.https.verify-hostname server.ssl.delegate-to-system server.ssl.trust-store server.ssl.trust-store-password Configure SSO \u00b6 You can enable SSO by either: Editing the properties file pingidentity-server-profiles/baseline/pingcentral/external-mysql-db/instance/conf/application.properties.subst . Using environment variables. You may also need to edit the hosts file used by the container. For stacks, you can update the container's /etc/hosts file by adding extra_hosts definitions to the docker-compose.yml file. For example: services : pingcentral : extra_hosts : - \"pingfedenvironment.ping-eng.com:12.105.33.333\" - \"pingcentral-sso-domain.com:127.0.0.1\" Using the properties file Update the pingidentity-server-profiles/baseline/pingcentral/external-mysql-db/instance/conf/application.properties.subst file according to the PingCentral documentation . Inject the application.properties.subst file into the container using the volumes definition in the docker-compose.yml file to mount ./conf/application.properties to the /opt/in/instance/conf/application.properties volume under the pingcentral service of the docker-compose.yml file : pingcentral : volumes : - ./conf/application.properties:/opt/in/instance/conf/application.properties Using environment variables To enable SSO using environment variables, add environment definitions for these environment variables. For stacks, add the definitions to the docker-compose.yml file. For example: services : pingcentral : environment : - pingcentral.sso.oidc.enabled=true - pingcentral.sso.oidc.issuer-uri=https://pingfedenvironment.ping-eng.com:9031 - pingcentral.sso.oidc.client-id=ac_oic_client_id - pingcentral.sso.oidc.client-secret=ClientSecretHere - pingcentral.sso.oidc.oauth-jwk-set-uri=https://pingfedenvironment.ping-eng.com:9031/ext/oauth/pingcentral/jwks For standalone PingCentral containers: docker run --env pingcentral.sso.oidc.enabled = true \\ --env pingcentral.sso.oidc.issuer-uri = https://pingfedenvironment.ping-eng.com:9031 \\ --env pingcentral.sso.oidc.client-id = ac_oic_client_id \\ --env pingcentral.sso.oidc.client-secret = ClientSecretHere \\ --env pingcentral.sso.oidc.oauth-jwk-set-uri = https://pingfedenvironment.ping-eng.com:9031/ext/oauth/pingcentral/jwks","title":"Deploy PingCentral"},{"location":"deployment/deployPingCentral/#deploy-pingcentral","text":"This use case employs the pingidentity-server-profiles/baseline/pingcentral server profile. This server profile contains a MySQL database engine located in pingidentity-server-profiles/baseline/pingcentral/external-mysql-db .","title":"Deploy PingCentral"},{"location":"deployment/deployPingCentral/#prerequisites","text":"You've already been through Get Started to set up your DevOps environment and run a test deployment of the products. Either: Clone the pingidentity-server-profiles repository to your local ${HOME}/projects/devops directory. Fork the pingidentity-server-profiles repository to your Github repository, then clone this repository to a local directory.","title":"Prerequisites"},{"location":"deployment/deployPingCentral/#what-youll-do","text":"Deploy the stack. Log in to the management consoles. Bring down or stop the stack. Preserve the database. Configure trust for PingCentral. Configure SSO for PingCentral","title":"What You'll Do"},{"location":"deployment/deployPingCentral/#deploy-stack","text":"You'll use the docker-compose.yaml file in your local pingidentity-devops-getting-started/11-docker-compose/30-pingcentral directory to deploy the cluster. Go to your local pingidentity-devops-getting-started/11-docker-compose/30-pingcentral directory. Enter: docker-compose up -d Check that the containers are healthy and running: docker-compose ps You can also display the startup logs: docker-compose logs -f To see the logs for a particular product container at any point, enter: docker-compose logs <product-container-name> Log in to the management consoles: Product Connection Details PingCentral URL: https://localhost:9022 Username: administrator Password: 2Federate Copy the MySQL database hostkey created on initial startup in ./conf/pingcentral.jwk to your local /tmp directory. You'll need the hostkey in a subsequent step. When you no longer want to run this stack, you can either stop the running stack, or bring the stack down. To stop the running stack without removing any of the containers or associated Docker networks, enter: docker-compose stop To remove all of the containers and associated Docker networks, enter: docker-compose down","title":"Deploy Stack"},{"location":"deployment/deployPingCentral/#preserve-database","text":"To preserve any updates to the MySQL database, you need to mount the ./conf/mysql/data directory to the /var/lib/mysql volume. You also need to mount ./conf/pingcentral.jwk to /opt/server/conf/pingcentral.jwk to save the hostkey file created on initial startup of the PingCentral container. You'll need the saved hostkey to access the database. If the stack is running, bring it down: docker-compose down Open the pingidentity-devops-getting-started/11-docker-compose/30-pingcentral/docker-compose.yml file and mount ./conf/mysql/data to the /var/lib/mysql volume under pingcentral-db . For example: pingcentral-db : image : mysql command : --default-authentication-plugin=mysql_native_password environment : MYSQL_ROOT_PASSWORD : 2Federate volumes : - ./conf/mysql/data:/var/lib/mysql ports : - \"3306:3306\" networks : - pingnet Keep the docker-compose.yml file open. In the pingidentity-devops-getting-started/11-docker-compose/30-pingcentral/docker-compose.yml file, also mount ./conf/pingcentral.jwk to the /opt/server/conf/pingcentral.jwk volume under the pingcentral service. For example: pingcentral : image : pingidentity/pingcentral:${PING_IDENTITY_DEVOPS_TAG} command : wait-for pingcentral-db:3306 -t 7200 -- entrypoint.sh start-server environment : - SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_PATH=baseline/pingcentral/external-mysql-db - PING_IDENTITY_ACCEPT_EULA=YES - PING_CENTRAL_BLIND_TRUST=true - PING_CENTRAL_VERIFY_HOSTNAME=false - MYSQL_USER=root - MYSQL_PASSWORD=2Federate env_file : - ~/.pingidentity/devops volumes : - ./conf/pingcentral.jwk:/opt/server/conf/pingcentral.jwk ports : - \"9022:9022\" depends_on : - \"pingcentral-db\" networks : - pingnet Save docker-compose.yml and start the stack: docker-compose up -d Copy the hostkey pingcentral.jwk file you saved to your local /tmp in a prior step to the /opt/out/instance/conf volume. Enter: docker cp /tmp/pingcentral.jwk:/opt/out/instance/conf The hostkey will now be persisted and available at each startup.","title":"Preserve Database"},{"location":"deployment/deployPingCentral/#configure-trust","text":"By default, for the purposes of quick setup, the PingCentral container is insecure. This is due to the environment variable PING_CENTRAL_BLIND_TRUST=true setting in the docker-compose.yml file. By default, all certificates are trusted. Caution : Remember to change this setting for production environments. Setting PING_CENTRAL_BLIND_TRUST=false allows public certificates to be used only by your Ping Identity environments (such as PingFederate), unless you set up the trust store and configure PingCentral to use this trust store. To set up the trust in the container, first create your trust store according to the PingCentral documentation . Inject the trust store into the PingCentral container: For stacks, inject the trust store using the volumes definition in the docker-compose.yml file to mount ./conf/keystore.jks to /opt/in/instance/conf/keystore.jks . For example: services : pingcentral : volumes : - ./conf/keystore.jks:/opt/in/instance/conf/keystore.jks For standalone PingCentral containers, use: docker run --volume \\ ./conf/keystore.jks:/opt/in/instance/conf/keystore.jks Configure PingCentral to use the created trust either by using environment variables or the properties file: Using environment variables For stacks, specify these environment variables in the environment definition of the docker-compose.yml file: services : pingcentral : environment : - server.ssl.trust-any=false - server.ssl.https.verify-hostname=false - server.ssl.delegate-to-system=false - server.ssl.trust-store=/opt/in/instance/conf/keystore.jks - server.ssl.trust-store-password=InsertTruststorePasswordHere For standalone PingCentral containers: docker run --env server.ssl.trust-any = false \\ --env server.ssl.https.verify-hostname = false \\ --env server.ssl.delegate-to-system = false \\ --env server.ssl.trust-store = /opt/in/instance/conf/keystore.jks \\ --env server.ssl.trust-store-password = <InsertTruststorePasswordHere> Using properties files Update the following properties in your pingidentity-server-profiles/baseline/pingcentral/external-mysql-db/instance/conf/application.properties.subst file: server.ssl.trust-any server.ssl.https.verify-hostname server.ssl.delegate-to-system server.ssl.trust-store server.ssl.trust-store-password","title":"Configure Trust"},{"location":"deployment/deployPingCentral/#configure-sso","text":"You can enable SSO by either: Editing the properties file pingidentity-server-profiles/baseline/pingcentral/external-mysql-db/instance/conf/application.properties.subst . Using environment variables. You may also need to edit the hosts file used by the container. For stacks, you can update the container's /etc/hosts file by adding extra_hosts definitions to the docker-compose.yml file. For example: services : pingcentral : extra_hosts : - \"pingfedenvironment.ping-eng.com:12.105.33.333\" - \"pingcentral-sso-domain.com:127.0.0.1\" Using the properties file Update the pingidentity-server-profiles/baseline/pingcentral/external-mysql-db/instance/conf/application.properties.subst file according to the PingCentral documentation . Inject the application.properties.subst file into the container using the volumes definition in the docker-compose.yml file to mount ./conf/application.properties to the /opt/in/instance/conf/application.properties volume under the pingcentral service of the docker-compose.yml file : pingcentral : volumes : - ./conf/application.properties:/opt/in/instance/conf/application.properties Using environment variables To enable SSO using environment variables, add environment definitions for these environment variables. For stacks, add the definitions to the docker-compose.yml file. For example: services : pingcentral : environment : - pingcentral.sso.oidc.enabled=true - pingcentral.sso.oidc.issuer-uri=https://pingfedenvironment.ping-eng.com:9031 - pingcentral.sso.oidc.client-id=ac_oic_client_id - pingcentral.sso.oidc.client-secret=ClientSecretHere - pingcentral.sso.oidc.oauth-jwk-set-uri=https://pingfedenvironment.ping-eng.com:9031/ext/oauth/pingcentral/jwks For standalone PingCentral containers: docker run --env pingcentral.sso.oidc.enabled = true \\ --env pingcentral.sso.oidc.issuer-uri = https://pingfedenvironment.ping-eng.com:9031 \\ --env pingcentral.sso.oidc.client-id = ac_oic_client_id \\ --env pingcentral.sso.oidc.client-secret = ClientSecretHere \\ --env pingcentral.sso.oidc.oauth-jwk-set-uri = https://pingfedenvironment.ping-eng.com:9031/ext/oauth/pingcentral/jwks","title":"Configure SSO"},{"location":"deployment/deployPingDataConsoleSSO/","text":"Deploy PingDataConsole with PingOne SSO enabled \u00b6 You'll use Docker Compose to deploy a PingDirectory and PingDataConsole stack. PingDataConsole will have SSO enabled with PingOne. Note: Configuring SSO with PingOne requires PingDirectory and PingDataConsole versions of at least 8.2.0.0. What You'll Do \u00b6 Deploy the PingDirectory and PingDataConsole stack. Test the deployment. Bring down or stop the stack. Prerequisites \u00b6 You've already been through Get Started to set up your DevOps environment and run a test deployment of the products. You've set up an application in PingOne representing your PingDataConsole instance, with a redirect URL of https://localhost:8443/console/oidc/cb. See the PingDirectory documentation (\"Configuring PingOne to use SSO for the PingData Administrative Console\") for details. You will need the Issuer, Client ID, and Client Secret values from PingOne. You've created a user in PingOne corresponding to a root user DN in PingDirectory. This example expects a user named Jane Smith, with username jsmith. This user will need to be given a password. You've set the variable values from PingOne in your local devops/pingidentity-devops-getting-started/11-docker-compose/13-pingdataconsole-pingone-sso/docker-compose.yml . Deploy the PingDirectory and PingDataConsole stack \u00b6 Go to your local devops/pingidentity-devops-getting-started/11-docker-compose/13-pingdataconsole-pingone-sso directory. Enter: docker-compose up -d Check that PingDirectory and PingDataConsole are healthy and running: docker-compose ps You can also display the startup logs: docker-compose logs -f To see the logs for a particular product container at any point, enter: docker-compose logs <product-container-name> Test Deployment \u00b6 In a browser, go to https://localhost:8443/console/login You should be redirected to a PingOne login page. Login with a PingOne user that corresponds to a configured root user DN (jsmith). You can generate an initial password for the user in PingOne. You should be successfully logged in to the console, where you can manage your PingDirectory instance. Clean Up \u00b6 When you no longer want to run this stack, bring the stack down. To remove all of the containers and associated Docker networks, enter: docker-compose down To stop the running stack without removing any of the containers or associated Docker networks, enter: docker-compose stop To remove attached Docker Volumes docker volume prune","title":"Deploy PingDataConsole with PingOne SSO"},{"location":"deployment/deployPingDataConsoleSSO/#deploy-pingdataconsole-with-pingone-sso-enabled","text":"You'll use Docker Compose to deploy a PingDirectory and PingDataConsole stack. PingDataConsole will have SSO enabled with PingOne. Note: Configuring SSO with PingOne requires PingDirectory and PingDataConsole versions of at least 8.2.0.0.","title":"Deploy PingDataConsole with PingOne SSO enabled"},{"location":"deployment/deployPingDataConsoleSSO/#what-youll-do","text":"Deploy the PingDirectory and PingDataConsole stack. Test the deployment. Bring down or stop the stack.","title":"What You'll Do"},{"location":"deployment/deployPingDataConsoleSSO/#prerequisites","text":"You've already been through Get Started to set up your DevOps environment and run a test deployment of the products. You've set up an application in PingOne representing your PingDataConsole instance, with a redirect URL of https://localhost:8443/console/oidc/cb. See the PingDirectory documentation (\"Configuring PingOne to use SSO for the PingData Administrative Console\") for details. You will need the Issuer, Client ID, and Client Secret values from PingOne. You've created a user in PingOne corresponding to a root user DN in PingDirectory. This example expects a user named Jane Smith, with username jsmith. This user will need to be given a password. You've set the variable values from PingOne in your local devops/pingidentity-devops-getting-started/11-docker-compose/13-pingdataconsole-pingone-sso/docker-compose.yml .","title":"Prerequisites"},{"location":"deployment/deployPingDataConsoleSSO/#deploy-the-pingdirectory-and-pingdataconsole-stack","text":"Go to your local devops/pingidentity-devops-getting-started/11-docker-compose/13-pingdataconsole-pingone-sso directory. Enter: docker-compose up -d Check that PingDirectory and PingDataConsole are healthy and running: docker-compose ps You can also display the startup logs: docker-compose logs -f To see the logs for a particular product container at any point, enter: docker-compose logs <product-container-name>","title":"Deploy the PingDirectory and PingDataConsole stack"},{"location":"deployment/deployPingDataConsoleSSO/#test-deployment","text":"In a browser, go to https://localhost:8443/console/login You should be redirected to a PingOne login page. Login with a PingOne user that corresponds to a configured root user DN (jsmith). You can generate an initial password for the user in PingOne. You should be successfully logged in to the console, where you can manage your PingDirectory instance.","title":"Test Deployment"},{"location":"deployment/deployPingDataConsoleSSO/#clean-up","text":"When you no longer want to run this stack, bring the stack down. To remove all of the containers and associated Docker networks, enter: docker-compose down To stop the running stack without removing any of the containers or associated Docker networks, enter: docker-compose stop To remove attached Docker Volumes docker volume prune","title":"Clean Up"},{"location":"deployment/deployReplication/","text":"Deploy a Replicated PingDirectory Pair \u00b6 You'll use Docker Compose to deploy a replicated pair of PingDirectory containers. What You'll Do \u00b6 Deploy the replicated pair. Test the deployment. Bring down or stop the stack. Prerequisites \u00b6 You've already been through Get Started to set up your DevOps environment and run a test deployment of the products. Deploy Stack \u00b6 Go to your local devops/pingidentity-devops-getting-started/11-docker-compose/02-replicated-pair directory. Enter: docker-compose up -d This kicks off a PingDirectory instance that will stand up, become healthy, and then go into a loop looking for other directories. Scale up instances docker-compose up -d --scale pingdirectory = 2 At intervals, check to see when the containers are healthy and running: docker-compose ps You can also display the startup logs: docker-compose logs -f To see the logs for a particular product container at any point, enter: docker-compose logs <product-container-name> DevOps Aliases Enter dhelp for a listing of the DevOps command aliases. See the Docker Compose Command Line Reference for the Docker Compose commands. To view the running instance, log in to PingDirectory using the Ping Data Console: Product Connection Details PingDirectory URL: https://localhost:8443/console Server: pingdirectory:636 Username: administrator Password: 2FederateM0re Test Deployment \u00b6 Verify that data is replicating between the pair by adding a description entry for the first container. First, exec into the container. docker container exec -it \\ 02 -replicated-pair_pingdirectory_1 \\ /opt/out/instance/bin/ldapmodify ... # Successfully connected to localhost:636 Then copy and paste this entire block: dn: uid = user.0,ou = people,dc = example,dc = com changetype: modify replace: description description: Made this change on the first container. <Ctrl-D> The blank line followed by the <Ctrl-D> is important. It's how entries are separated in the LDAP Data Interchange Format (LDIF). Check that the second container in the pair now has a matching entry for the description. Enter: docker container exec -it \\ 02-replicated-pair_pingdirectory_2 \\ /opt/out/instance/bin/ldapsearch \\ -b uid=user.0,ou=people,dc=example,dc=com \\ -s base '(&)' description The result should show the description that you specified for the first container, similar to this: # dn: uid=user.0,ou=people,dc=example,dc=com # description: Made this change on the first container. Clean Up \u00b6 When you no longer want to run this stack, bring the stack down. To remove all of the containers and associated Docker networks, enter: docker-compose down To stop the running stack without removing any of the containers or associated Docker networks, enter: docker-compose stop To remove attached Docker Volumes docker volume prune","title":"Deploy Replicated PingDirectory Pair"},{"location":"deployment/deployReplication/#deploy-a-replicated-pingdirectory-pair","text":"You'll use Docker Compose to deploy a replicated pair of PingDirectory containers.","title":"Deploy a Replicated PingDirectory Pair"},{"location":"deployment/deployReplication/#what-youll-do","text":"Deploy the replicated pair. Test the deployment. Bring down or stop the stack.","title":"What You'll Do"},{"location":"deployment/deployReplication/#prerequisites","text":"You've already been through Get Started to set up your DevOps environment and run a test deployment of the products.","title":"Prerequisites"},{"location":"deployment/deployReplication/#deploy-stack","text":"Go to your local devops/pingidentity-devops-getting-started/11-docker-compose/02-replicated-pair directory. Enter: docker-compose up -d This kicks off a PingDirectory instance that will stand up, become healthy, and then go into a loop looking for other directories. Scale up instances docker-compose up -d --scale pingdirectory = 2 At intervals, check to see when the containers are healthy and running: docker-compose ps You can also display the startup logs: docker-compose logs -f To see the logs for a particular product container at any point, enter: docker-compose logs <product-container-name> DevOps Aliases Enter dhelp for a listing of the DevOps command aliases. See the Docker Compose Command Line Reference for the Docker Compose commands. To view the running instance, log in to PingDirectory using the Ping Data Console: Product Connection Details PingDirectory URL: https://localhost:8443/console Server: pingdirectory:636 Username: administrator Password: 2FederateM0re","title":"Deploy Stack"},{"location":"deployment/deployReplication/#test-deployment","text":"Verify that data is replicating between the pair by adding a description entry for the first container. First, exec into the container. docker container exec -it \\ 02 -replicated-pair_pingdirectory_1 \\ /opt/out/instance/bin/ldapmodify ... # Successfully connected to localhost:636 Then copy and paste this entire block: dn: uid = user.0,ou = people,dc = example,dc = com changetype: modify replace: description description: Made this change on the first container. <Ctrl-D> The blank line followed by the <Ctrl-D> is important. It's how entries are separated in the LDAP Data Interchange Format (LDIF). Check that the second container in the pair now has a matching entry for the description. Enter: docker container exec -it \\ 02-replicated-pair_pingdirectory_2 \\ /opt/out/instance/bin/ldapsearch \\ -b uid=user.0,ou=people,dc=example,dc=com \\ -s base '(&)' description The result should show the description that you specified for the first container, similar to this: # dn: uid=user.0,ou=people,dc=example,dc=com # description: Made this change on the first container.","title":"Test Deployment"},{"location":"deployment/deployReplication/#clean-up","text":"When you no longer want to run this stack, bring the stack down. To remove all of the containers and associated Docker networks, enter: docker-compose down To stop the running stack without removing any of the containers or associated Docker networks, enter: docker-compose stop To remove attached Docker Volumes docker volume prune","title":"Clean Up"},{"location":"deployment/deploySiemStack/","text":"Deploy an Elasticsearch SIEM Stack \u00b6 This example deploys a PingFederate, PingAccess, and PingDirectory stack with Elasticsearch infrastructure built in for visualizing traffic and other security or log data. The architecture looks like this: Threat intel and TOR Endpoints are provided by AlienVault and the TOR Network Endpoint List. Threat feeds are updated on an interval via setting an environment variable in docker-compose.yaml . Warning : This stack is not intended for production environments. Prerequisites \u00b6 You've already been through Get Started to set up your DevOps environment and run a test deployment of the products. For most Linux distributions (local or on a platform), you'll need to increase the vm.max_map_count setting to support the necessary heap size. Enter: sudo sysctl -w vm.max_map_count = 262144 Your Linux machine needs at least 12 Gb of RAM for Docker to run this stack. For Apple macos or Microsoft Windows machines, ensure the Docker Resources is set to a minimum 10 Gb of RAM, or the containers will crash. For Amazon Web Services (AWS) use a M5.XL or M5a.XL VPC. 16 Gb RAM is required, and at least 50 Gb of storage is recommended. Optional \u00b6 If you're using Slack, you can generate a Slack Webhook URL from the Slack Admin for alerting: https://api.slack.com/messaging/webhooks . Instal Setup \u00b6 From the pingidentity-devops-getting-started directory, pull the repo to ensure that you have current files: git pull Go to the pingidentity-devops-getting-started/11-docker-compose/11-siem-stack/ directory. Create a siem.env file in the 11-siem-stack directory, and copy these entries into the siem.env file: COMPOSE_PROJECT_NAME = es ELASTIC_VERSION = 7 .6.1 ELASTIC_PASSWORD = 2FederateM0re ES_ADMIN_PD_USER_PASS = FederateTheB3st! PING_IDENTITY_DEVOPS_USER = <your-username> PING_IDENTITY_DEVOPS_KEY = <your-key> Deploy Stack \u00b6 From the pingidentity-devops-getting-started/11-docker-compose/11-siem-stack/ directory, start the stack: docker-compose up -d Monitor the container startup using one of these commands: docker-compose ps docker-compose logs -f (Optional) If you're using Slack, and you've already created your Webhook URL (see the optional prerequisite above), you can run the Slack configuration script to configure slack alerts: ./config_slack_alerts The script prompts for your Webhook URL and Elasticsearch password. The Webhook URL updates the destination for your alerts within Slack. The password is used to push watchers into Elasticsearch. You don't need to provide your Webhook URL in the future. If you don't provide it, it simply will not update it. You can re-run this script any time. This will update and push new watchers you create from the ./elasticsearch-siem/watchers folder. Post-Deployment \u00b6 When PingDirectory is up and healthy: Kibana console: URL: https://localhost:5601/ . User name: es_admin or elastic (local user). Password: FederateTheB3st! (the ES_ADMIN_PD_USER_PASS value in the siem.env file you created). Kibana saved objects You can load the saved objects by going to \"Saved Objects\" under the Kibana settings and exporting all. The exported file is saved in ./elasticsearch-siem/kibana_config/kib_base.ndjson . Elasticsearch templates for indexes Index mappings and config are stored in the ./elasticsearch-siem/index_templates directory. The scripts will load the template or templates when the cluster state is green. Logstash pipeline TOR Enrichment Threat Intel (Alien Vault Provided) GEO IP Lookup GEO Distance Query (template driven) Data Parsing The Logstash pipeline is stored in the directory structure. It includes parsers for all Ping Identity log sources. Cleaning Up \u00b6 There are persistent volumes used for Elasticsearch data and certificates, so you'll also need to clear the volumes when you bring the stack down. Enter: docker-compose down docker volume prune Dashboard Examples \u00b6 PingFederate Threat Intel Dashboard \u00b6 Ping Identity SIEM Dashboard \u00b6 Ping Federate Dashboard \u00b6 Audit and System logs are delivered (set to Debug by default). For Log4J, PingFederate sends logs on 2 different Syslog ports using a custom mapping. PingAccess Dashboard \u00b6 Audit and System logs are delivered (set to Debug by default). For Log4J, PingAccess sends logs on 2 different Syslog ports using a custom mapping. PingDirectory Dashboard \u00b6 Audit logs are being delivered. There are 2 containers that produce load. These are disabled by default. You can uncomment these entries in the docker-compose.yaml file to use them: authrate_ok authrate_ko For Log4J, PingDirectory sends logs on 1 Syslog port using a custom mapping. Included Slack Alerts \u00b6 These can be customized through Watchers: User authenticates over 1200km away within a 6 hour period. User authenticates successfully from TOR through Ping Federate (potential credential theft). User authenticates successfully from Known Malicious IP through Ping Federate (potential credential theft). Account Lockout detected through Ping Federate (potential brute force). Likely SAML signature modifications (forced tampering with authentication protocols). Slack Alert Examples (not all are shown) \u00b6 These are Low / Medium / High alert examples:","title":"Deploy an Elasticsearch SIEM Stack"},{"location":"deployment/deploySiemStack/#deploy-an-elasticsearch-siem-stack","text":"This example deploys a PingFederate, PingAccess, and PingDirectory stack with Elasticsearch infrastructure built in for visualizing traffic and other security or log data. The architecture looks like this: Threat intel and TOR Endpoints are provided by AlienVault and the TOR Network Endpoint List. Threat feeds are updated on an interval via setting an environment variable in docker-compose.yaml . Warning : This stack is not intended for production environments.","title":"Deploy an Elasticsearch SIEM Stack"},{"location":"deployment/deploySiemStack/#prerequisites","text":"You've already been through Get Started to set up your DevOps environment and run a test deployment of the products. For most Linux distributions (local or on a platform), you'll need to increase the vm.max_map_count setting to support the necessary heap size. Enter: sudo sysctl -w vm.max_map_count = 262144 Your Linux machine needs at least 12 Gb of RAM for Docker to run this stack. For Apple macos or Microsoft Windows machines, ensure the Docker Resources is set to a minimum 10 Gb of RAM, or the containers will crash. For Amazon Web Services (AWS) use a M5.XL or M5a.XL VPC. 16 Gb RAM is required, and at least 50 Gb of storage is recommended.","title":"Prerequisites"},{"location":"deployment/deploySiemStack/#optional","text":"If you're using Slack, you can generate a Slack Webhook URL from the Slack Admin for alerting: https://api.slack.com/messaging/webhooks .","title":"Optional"},{"location":"deployment/deploySiemStack/#instal-setup","text":"From the pingidentity-devops-getting-started directory, pull the repo to ensure that you have current files: git pull Go to the pingidentity-devops-getting-started/11-docker-compose/11-siem-stack/ directory. Create a siem.env file in the 11-siem-stack directory, and copy these entries into the siem.env file: COMPOSE_PROJECT_NAME = es ELASTIC_VERSION = 7 .6.1 ELASTIC_PASSWORD = 2FederateM0re ES_ADMIN_PD_USER_PASS = FederateTheB3st! PING_IDENTITY_DEVOPS_USER = <your-username> PING_IDENTITY_DEVOPS_KEY = <your-key>","title":"Instal Setup"},{"location":"deployment/deploySiemStack/#deploy-stack","text":"From the pingidentity-devops-getting-started/11-docker-compose/11-siem-stack/ directory, start the stack: docker-compose up -d Monitor the container startup using one of these commands: docker-compose ps docker-compose logs -f (Optional) If you're using Slack, and you've already created your Webhook URL (see the optional prerequisite above), you can run the Slack configuration script to configure slack alerts: ./config_slack_alerts The script prompts for your Webhook URL and Elasticsearch password. The Webhook URL updates the destination for your alerts within Slack. The password is used to push watchers into Elasticsearch. You don't need to provide your Webhook URL in the future. If you don't provide it, it simply will not update it. You can re-run this script any time. This will update and push new watchers you create from the ./elasticsearch-siem/watchers folder.","title":"Deploy Stack"},{"location":"deployment/deploySiemStack/#post-deployment","text":"When PingDirectory is up and healthy: Kibana console: URL: https://localhost:5601/ . User name: es_admin or elastic (local user). Password: FederateTheB3st! (the ES_ADMIN_PD_USER_PASS value in the siem.env file you created). Kibana saved objects You can load the saved objects by going to \"Saved Objects\" under the Kibana settings and exporting all. The exported file is saved in ./elasticsearch-siem/kibana_config/kib_base.ndjson . Elasticsearch templates for indexes Index mappings and config are stored in the ./elasticsearch-siem/index_templates directory. The scripts will load the template or templates when the cluster state is green. Logstash pipeline TOR Enrichment Threat Intel (Alien Vault Provided) GEO IP Lookup GEO Distance Query (template driven) Data Parsing The Logstash pipeline is stored in the directory structure. It includes parsers for all Ping Identity log sources.","title":"Post-Deployment"},{"location":"deployment/deploySiemStack/#cleaning-up","text":"There are persistent volumes used for Elasticsearch data and certificates, so you'll also need to clear the volumes when you bring the stack down. Enter: docker-compose down docker volume prune","title":"Cleaning Up"},{"location":"deployment/deploySiemStack/#dashboard-examples","text":"","title":"Dashboard Examples"},{"location":"deployment/deploySiemStack/#pingfederate-threat-intel-dashboard","text":"","title":"PingFederate Threat Intel Dashboard"},{"location":"deployment/deploySiemStack/#ping-identity-siem-dashboard","text":"","title":"Ping Identity SIEM Dashboard"},{"location":"deployment/deploySiemStack/#ping-federate-dashboard","text":"Audit and System logs are delivered (set to Debug by default). For Log4J, PingFederate sends logs on 2 different Syslog ports using a custom mapping.","title":"Ping Federate Dashboard"},{"location":"deployment/deploySiemStack/#pingaccess-dashboard","text":"Audit and System logs are delivered (set to Debug by default). For Log4J, PingAccess sends logs on 2 different Syslog ports using a custom mapping.","title":"PingAccess Dashboard"},{"location":"deployment/deploySiemStack/#pingdirectory-dashboard","text":"Audit logs are being delivered. There are 2 containers that produce load. These are disabled by default. You can uncomment these entries in the docker-compose.yaml file to use them: authrate_ok authrate_ko For Log4J, PingDirectory sends logs on 1 Syslog port using a custom mapping.","title":"PingDirectory Dashboard"},{"location":"deployment/deploySiemStack/#included-slack-alerts","text":"These can be customized through Watchers: User authenticates over 1200km away within a 6 hour period. User authenticates successfully from TOR through Ping Federate (potential credential theft). User authenticates successfully from Known Malicious IP through Ping Federate (potential credential theft). Account Lockout detected through Ping Federate (potential brute force). Likely SAML signature modifications (forced tampering with authentication protocols).","title":"Included Slack Alerts"},{"location":"deployment/deploySiemStack/#slack-alert-examples-not-all-are-shown","text":"These are Low / Medium / High alert examples:","title":"Slack Alert Examples (not all are shown)"},{"location":"deployment/deploySimpleStack/","text":"Deploy a PingFederate and PingDirectory Stack \u00b6 You'll use Docker Compose to deploy a PingFederate and PingDirectory stack. What You'll Do \u00b6 Deploy the stack. Log in to the management consoles. Bring down or stop the stack. Prerequisites \u00b6 You've already been through Get Started to set up your DevOps environment and run a test deployment of the products. Deploy Stack \u00b6 Go to your local pingidentity-devops-getting-started/11-docker-compose/01-simple-stack directory. Enter: docker-compose up -d Check that the containers are healthy and running: docker-compose ps To display the startup logs: docker-compose logs -f To see the logs for a particular product container at any point, enter: docker-compose logs <product-container-name> Log in to the management consoles: Product Connection Details PingFederate URL: https://localhost:9999/pingfederate/app Username: administrator Password: 2FederateM0re PingDirectory URL: https://localhost:8443/console Server: pingdirectory Username: administrator Password: 2FederateM0re Clean Up \u00b6 When you no longer want to run this stack, you can either stop the running stack, or bring the stack down. To stop the running stack without removing any of the containers or associated Docker networks, enter: docker-compose stop To remove all of the containers and associated Docker networks, enter: docker-compose down To remove attached Docker Volumes docker volume prune","title":"Deploy PingFederate and PingDirectory Stack"},{"location":"deployment/deploySimpleStack/#deploy-a-pingfederate-and-pingdirectory-stack","text":"You'll use Docker Compose to deploy a PingFederate and PingDirectory stack.","title":"Deploy a PingFederate and PingDirectory Stack"},{"location":"deployment/deploySimpleStack/#what-youll-do","text":"Deploy the stack. Log in to the management consoles. Bring down or stop the stack.","title":"What You'll Do"},{"location":"deployment/deploySimpleStack/#prerequisites","text":"You've already been through Get Started to set up your DevOps environment and run a test deployment of the products.","title":"Prerequisites"},{"location":"deployment/deploySimpleStack/#deploy-stack","text":"Go to your local pingidentity-devops-getting-started/11-docker-compose/01-simple-stack directory. Enter: docker-compose up -d Check that the containers are healthy and running: docker-compose ps To display the startup logs: docker-compose logs -f To see the logs for a particular product container at any point, enter: docker-compose logs <product-container-name> Log in to the management consoles: Product Connection Details PingFederate URL: https://localhost:9999/pingfederate/app Username: administrator Password: 2FederateM0re PingDirectory URL: https://localhost:8443/console Server: pingdirectory Username: administrator Password: 2FederateM0re","title":"Deploy Stack"},{"location":"deployment/deploySimpleStack/#clean-up","text":"When you no longer want to run this stack, you can either stop the running stack, or bring the stack down. To stop the running stack without removing any of the containers or associated Docker networks, enter: docker-compose stop To remove all of the containers and associated Docker networks, enter: docker-compose down To remove attached Docker Volumes docker volume prune","title":"Clean Up"},{"location":"deployment/deploySync/","text":"Deploy PingDirectory and PingDataSync \u00b6 You'll use Docker Compose to deploy a PingDirectory and PingDataSync stack. PingDataSync will synchronize data from a source tree on a PingDirectory instance to a destination tree on the same PingDirectory instance. The entries from ou=source,o=sync to ou=destination,o=sync will be synchronized every second. What You'll Do \u00b6 Deploy the PingDirectory and PingDataSync stack. Test the deployment. Bring down or stop the stack. Prerequisites \u00b6 You've already been through Get started to set up your DevOps environment and run a test deployment of the products. Deploy Stack \u00b6 Go to your local devops/pingidentity-devops-getting-started/11-docker-compose/04-simple-sync directory. Enter: docker-compose up -d Check that PingDirectory and PingDataSync are healthy and running: docker-compose ps You can also display the startup logs: docker-compose logs -f To see the logs for a particular product container at any point, enter: docker-compose logs <product-container-name> Test the Deployment \u00b6 The stack will sync entries from ou=source,o=sync to ou=destination,o=sync every second. In one terminal window, tail the logs from the PingDataSync server: docker logs 04 -simple-sync_pingdatasync_1 -f In a second window, make a change to the ou=source,o=sync tree: docker container exec -it 04-simple-sync_pingdirectory_1 \\ /opt/out/instance/bin/ldapmodify dn: uid=user.0,ou=people,ou=source,o=sync changetype: modify replace: description description: Change to source user.0 <Ctrl-D> You'll see messages in the PingDataSync log showing ADD/MODIFY of the user sync'd to the ou=destination,o=sync tree. To verify this, enter: docker container exec -it \\ 04-simple-sync_pingdirectory_1 \\ /opt/out/instance/bin/ldapsearch \\ -b uid=user.0,ou=people,ou=destination,o=sync \\ -s base '(&)' description Entries similar to this will be returned: # dn: uid=user.0,ou=People,ou=destination,o=sync # description: Change to source user.0 Clean Up \u00b6 When you no longer want to run this stack, bring the stack down. To remove all of the containers and associated Docker networks, enter: docker-compose down To stop the running stack without removing any of the containers or associated Docker networks, enter: docker-compose stop To remove attached Docker Volumes docker volume prune","title":"Deploy PingDirectory and PingDataSync"},{"location":"deployment/deploySync/#deploy-pingdirectory-and-pingdatasync","text":"You'll use Docker Compose to deploy a PingDirectory and PingDataSync stack. PingDataSync will synchronize data from a source tree on a PingDirectory instance to a destination tree on the same PingDirectory instance. The entries from ou=source,o=sync to ou=destination,o=sync will be synchronized every second.","title":"Deploy PingDirectory and PingDataSync"},{"location":"deployment/deploySync/#what-youll-do","text":"Deploy the PingDirectory and PingDataSync stack. Test the deployment. Bring down or stop the stack.","title":"What You'll Do"},{"location":"deployment/deploySync/#prerequisites","text":"You've already been through Get started to set up your DevOps environment and run a test deployment of the products.","title":"Prerequisites"},{"location":"deployment/deploySync/#deploy-stack","text":"Go to your local devops/pingidentity-devops-getting-started/11-docker-compose/04-simple-sync directory. Enter: docker-compose up -d Check that PingDirectory and PingDataSync are healthy and running: docker-compose ps You can also display the startup logs: docker-compose logs -f To see the logs for a particular product container at any point, enter: docker-compose logs <product-container-name>","title":"Deploy Stack"},{"location":"deployment/deploySync/#test-the-deployment","text":"The stack will sync entries from ou=source,o=sync to ou=destination,o=sync every second. In one terminal window, tail the logs from the PingDataSync server: docker logs 04 -simple-sync_pingdatasync_1 -f In a second window, make a change to the ou=source,o=sync tree: docker container exec -it 04-simple-sync_pingdirectory_1 \\ /opt/out/instance/bin/ldapmodify dn: uid=user.0,ou=people,ou=source,o=sync changetype: modify replace: description description: Change to source user.0 <Ctrl-D> You'll see messages in the PingDataSync log showing ADD/MODIFY of the user sync'd to the ou=destination,o=sync tree. To verify this, enter: docker container exec -it \\ 04-simple-sync_pingdirectory_1 \\ /opt/out/instance/bin/ldapsearch \\ -b uid=user.0,ou=people,ou=destination,o=sync \\ -s base '(&)' description Entries similar to this will be returned: # dn: uid=user.0,ou=People,ou=destination,o=sync # description: Change to source user.0","title":"Test the Deployment"},{"location":"deployment/deploySync/#clean-up","text":"When you no longer want to run this stack, bring the stack down. To remove all of the containers and associated Docker networks, enter: docker-compose down To stop the running stack without removing any of the containers or associated Docker networks, enter: docker-compose stop To remove attached Docker Volumes docker volume prune","title":"Clean Up"},{"location":"deployment/deploySyncFailover/","text":"Deploy a PingDataSync Failover Server \u00b6 You'll use Docker Compose to deploy a PingDirectory and PingDataSync stack. PingDataSync will synchronize data from a source tree on a PingDirectory instance to a destination tree on the same PingDirectory instance. The entries from ou=source,o=sync to ou=destination,o=sync will be synchronized every second. Then you will scale up the PingDataSync service to enable failover, so that if an active PingDataSync server goes down, a second server will automatically become active and pick up where the first left off. Note: Configuring failover requires a PingDataSync version of at least 8.2.0.0. What You'll Do \u00b6 Deploy the PingDirectory and PingDataSync stack. Scale up the PingDataSync service Test the deployment. Bring down or stop the stack. Prerequisites \u00b6 You've already been through Get Started to set up your DevOps environment and run a test deployment of the products. Deploy the PingDirectory and PingDataSync Stack \u00b6 Go to your local devops/pingidentity-devops-getting-started/11-docker-compose/12-sync-failover-pair directory. Enter: docker-compose up -d Check that PingDirectory and PingDataSync are healthy and running: docker-compose ps You can also display the startup logs: docker-compose logs -f To see the logs for a particular product container at any point, enter: docker-compose logs <product-container-name> Scale PingDataSync instances docker-compose up -d --scale pingdatasync = 2 Test Deployment \u00b6 The stack will sync entries from ou=source,o=sync to ou=destination,o=sync every second. One of the two sync servers will be considered active, while the other remains on standby. In one terminal window, tail the logs from the PingDataSync servers: docker-compose logs -f pingdatasync In a second window, make a change to the ou=source,o=sync tree: docker container exec -it 12-sync-failover-pair_pingdirectory_1 /opt/out/instance/bin/ldapmodify dn: uid=user.0,ou=people,ou=source,o=sync changetype: modify replace: description description: Change to source user.0 <Ctrl-D> You'll see messages in the PingDataSync log showing ADD/MODIFY of the user synced to the ou=destination,o=sync tree. To verify this, enter: docker container exec -it 12-sync-failover-pair-sync_pingdirectory_1 /opt/out/instance/bin/ldapsearch -b uid=user.0,ou=people,ou=destination,o=sync -s base '(&)' description Entries similar to this will be returned: # dn: uid=user.0,ou=People,ou=destination,o=sync # description: Change to source user.0 In the log messages displayed in step 3, you'll see that one of the two PingDataSync servers handled the change. You can stop the container that handled the change to see future operations handled by the remaining PingDataSync server: docker stop 12 -sync-failover-pair_pingdatasync_1 You can now repeat steps 2 and 3 to verify that the remaining PingDataSync server is now active. It may take a moment to become active and handle the change after the first server is stopped. Clean Up \u00b6 When you no longer want to run this stack, you can either stop the running stack, or bring the stack down. To stop the running stack without removing any of the containers or associated Docker networks, enter: docker-compose stop To remove all of the containers and associated Docker networks, enter: docker-compose down To remove attached Docker Volumes docker volume prune","title":"Deploy PingDataSync Failover Server"},{"location":"deployment/deploySyncFailover/#deploy-a-pingdatasync-failover-server","text":"You'll use Docker Compose to deploy a PingDirectory and PingDataSync stack. PingDataSync will synchronize data from a source tree on a PingDirectory instance to a destination tree on the same PingDirectory instance. The entries from ou=source,o=sync to ou=destination,o=sync will be synchronized every second. Then you will scale up the PingDataSync service to enable failover, so that if an active PingDataSync server goes down, a second server will automatically become active and pick up where the first left off. Note: Configuring failover requires a PingDataSync version of at least 8.2.0.0.","title":"Deploy a PingDataSync Failover Server"},{"location":"deployment/deploySyncFailover/#what-youll-do","text":"Deploy the PingDirectory and PingDataSync stack. Scale up the PingDataSync service Test the deployment. Bring down or stop the stack.","title":"What You'll Do"},{"location":"deployment/deploySyncFailover/#prerequisites","text":"You've already been through Get Started to set up your DevOps environment and run a test deployment of the products.","title":"Prerequisites"},{"location":"deployment/deploySyncFailover/#deploy-the-pingdirectory-and-pingdatasync-stack","text":"Go to your local devops/pingidentity-devops-getting-started/11-docker-compose/12-sync-failover-pair directory. Enter: docker-compose up -d Check that PingDirectory and PingDataSync are healthy and running: docker-compose ps You can also display the startup logs: docker-compose logs -f To see the logs for a particular product container at any point, enter: docker-compose logs <product-container-name> Scale PingDataSync instances docker-compose up -d --scale pingdatasync = 2","title":"Deploy the PingDirectory and PingDataSync Stack"},{"location":"deployment/deploySyncFailover/#test-deployment","text":"The stack will sync entries from ou=source,o=sync to ou=destination,o=sync every second. One of the two sync servers will be considered active, while the other remains on standby. In one terminal window, tail the logs from the PingDataSync servers: docker-compose logs -f pingdatasync In a second window, make a change to the ou=source,o=sync tree: docker container exec -it 12-sync-failover-pair_pingdirectory_1 /opt/out/instance/bin/ldapmodify dn: uid=user.0,ou=people,ou=source,o=sync changetype: modify replace: description description: Change to source user.0 <Ctrl-D> You'll see messages in the PingDataSync log showing ADD/MODIFY of the user synced to the ou=destination,o=sync tree. To verify this, enter: docker container exec -it 12-sync-failover-pair-sync_pingdirectory_1 /opt/out/instance/bin/ldapsearch -b uid=user.0,ou=people,ou=destination,o=sync -s base '(&)' description Entries similar to this will be returned: # dn: uid=user.0,ou=People,ou=destination,o=sync # description: Change to source user.0 In the log messages displayed in step 3, you'll see that one of the two PingDataSync servers handled the change. You can stop the container that handled the change to see future operations handled by the remaining PingDataSync server: docker stop 12 -sync-failover-pair_pingdatasync_1 You can now repeat steps 2 and 3 to verify that the remaining PingDataSync server is now active. It may take a moment to become active and handle the change after the first server is stopped.","title":"Test Deployment"},{"location":"deployment/deploySyncFailover/#clean-up","text":"When you no longer want to run this stack, you can either stop the running stack, or bring the stack down. To stop the running stack without removing any of the containers or associated Docker networks, enter: docker-compose stop To remove all of the containers and associated Docker networks, enter: docker-compose down To remove attached Docker Volumes docker volume prune","title":"Clean Up"},{"location":"deployment/deployVault/","text":"Deploy Hashicorp Vault \u00b6 This is an example of deploying Hashicorp Vault (Vault) with PingFederate and PingAccess to manage their corresponding master keys ( pf.pwk and pa.pwk ). Using Vault, you can also manage license files, DevOps keys, product secrets, and others. What you'll do \u00b6 You'll clone the Vault Helm chart to deploy a near-production environment to validate and manage the product master keys, product secrets, and authentication policies. Prerequisites \u00b6 You've already been through Get started to set up your DevOps environment and run a test deployment of the products. Vault . Helm . Vault uses Helm 3. Kubernetes 1.7 OpenSSL or your favorite PKI tool. Pull our pingidentity-getting-started repo to ensure you have the latest sources. Deployment architecture \u00b6 Illustrated below are the specific configuration items we are using for this deployment. Additionally, we'll deploy Vault into Amazon Elastic Kubernetes Service (EKS) and using some of AWS's specific services ( AWS KMS and AWS DynamoDB ) to help simplify the deployment architecture. Enable TLS \u00b6 Before you deploy Vault using Helm, you'll need to add the TLS key pair (public and private keys) and CA chain files as a Kubernetes secret. The public certificate and private key need to be separate files. You can either use OpenSSL to quickly create a self-signed certificate, or use one signed by your Certificate Authority. If you're using a self-signed certificate, the public certificate is also the CA certificate. Create the Kubernetes secret using Vault, the TLS key pair, and the certificate: kubectl create secret generic vault-certstore \\ --from-file = vault.key = <local_path_to_tls_key>/tls.key \\ --from-file = vault.crt = <local_path_to_tls_cert>/tls.crt \\ --from-file = vault.ca = <local_path_to_ca_cert>/vault.ca Ensure that these parameters in the values.yaml file located in your local pingidentity-devops-getting-started/20-kubernetes/08-vault/vault-helm directory are set as follows: global: Enable TLS globally: global : tlsDisable : false extraEnvironmentVars: Set the environment variable that will contain the path to the CA Certificate used for TLS. extraEnvironmentVars : VAULT_CACERT : /vault/userconfig/vault-certstore/vault.ca extraVolumes: Set the volume mount for the certificate store secret. This mount will contain the TLS public certificate, private key and CA certificate. extraVolumes : - type : secret name : vault-certstore ha: Set Vault to use high-availability (HA) mode. Vault uses Hashicorp Consul for its storage backend. The default configuration provided will work with the Consul (Helm) project by default. You can also manually configure Vault to use a different HA backend. ha : enabled : true replicas : 3 # Add the following parameters to the `listener \"tcp\"` element to enable TLS: config : | ui = true log_level = \"Debug\" listener \"tcp\" { tls_disable = 0 address = \"[::]:8200\" cluster_address = \"[::]:8201\" tls_cert_file = \"/vault/userconfig/vault-certstore/vault.crt\" tls_key_file = \"/vault/userconfig/vault-certstore/vault.key\" tls_client_ca_file = \"/vault/userconfig/vault-certstore/vault.ca\" } Storage Backend \u00b6 We can take advantage of some AWS services to simplify our deployment architecture. The Vault Helm chart has examples for using files and/or an existing Consul deployment. Here, we'll update Vault's HA deployment to use AWS DynamoDB. Create an AWS access key and secret with permissions to manage the dynamodb. You'll find the permissions that the Vault IAM user requires to manage dynamodb in the Vault documentation Required AWS Permissions . Vault will create the necessary table in dynamodb if it does not already exist. See the Vault documentation DynamoDB Storage Backend for additional parameters when using dynamodb as a storage mechanism. Add your AWS access and secret key as a Kubernetes secret: kubectl create secret generic dynamodb-access-secret-keys \\ --from-literal = AWS_ACCESS_KEY_ID = <your_aws_acccess_key> \\ --from-literal = AWS_SECRET_ACCESS_KEY = <your_aws_acccess_key_secret> Kubernetes can provide the secrets as environment variables that Vault can use, so you do not accidentally expose your the secret outside of Kubernetes. Update the values.yaml file to include your AWS key and secret within the extraSecretEnvironmentVars section: extraSecretEnvironmentVars : - envName : AWS_SECRET_ACCESS_KEY secretName : dynamodb-access-secret-key secretKey : AWS_SECRET_ACCESS_KEY - envName : AWS_ACCESS_KEY_ID secretName : dynamodb-access-secret-key secretKey : AWS_ACCESS_KEY_ID In the ha section, update the dynamodb storage element with your corresponding AWS region and dynamodb table name: storage \"dynamodb\" { ha_enabled = \"true\" region = \"<aws_region>\" table = \"<dynamodb_table_name>\" } Auto Unseal \u00b6 To keep things simple, we'll use Vault's Auto Unseal with the AWS Key Management Service (KMS). You'll need to set up your existing AWS access key and secret with the correct permissions. Because we're using dynamodb for backend storage, you'll also add the permissions to your AWS access key. See the Vault documentation AWS KMS authentication for more information. Vault can retrieve the AWS KMS key using an environment variable, so it is not accidentally exposed outside of the Kubernetes environment. Add the KMS key as a Kubernetes secret: kubectl create secret generic aws-kms-key-id \\ --from-literal = KMS_KEY_ID = <your_key_id> Update the values.yaml file to include your AWS key and secret in the extraSecretEnvironmentVars section: extraSecretEnvironmentVars : - envName : VAULT_AWSKMS_SEAL_KEY_ID secretName : aws-kms-key-id secretKey : KMS_KEY_ID In the ha section config map ( config ), add a seal element, and update the region parameter with your AWS region value: seal \"awskms\" { region = \"<aws_region>\" } Deploy Vault using Helm \u00b6 See the Vault documentation for Kubernetes for complete information. Go to your local pingidentity-devops-getting-started/20-kubernetes/08-vault/vault-helm directory and enter: helm install vault /. Information similar to the following will be displayed: NAME: vault LAST DEPLOYED: Thu Mar 12 17 :27:19 2020 NAMESPACE: ping-cloud-devops-eks STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Thank you for installing HashiCorp Vault! Your release is named vault. To learn more about the release, try: $ helm status vault $ helm get vault If this is the first time you've deployed Vault in this environment, you'll need to initialize it. Enter: kubectl exec vault-0 -- vault operator init If the initialization is successful, you'll receive the recovery keys and Initial Root token. Make sure you store the recovery keys and root token in a secure location. Information similar to the following is displayed: Recovery Key 1 : <key_1> Recovery Key 2 : <key_2> Recovery Key 3 : <key_3> Recovery Key 4 : <key_4> Recovery Key 5 : <key_5> Initial Root Token: <root_token> Success! Vault is initialized Recovery key initialized with 5 key shares and a key threshold of 3 . Please securely distribute the key shares printed above. Pod Authentication \u00b6 Our products and applications need to have a Vault client token to authenticate to Vault. Because we're using Kubernetes, we can use Vault's Kubernetes auth method. When the Kubernetes auth method is enabled, Vault can use a pod's Kubernetes service account token to authenticate and exchange for a Vault client token. The Vault client token is associated with a particular role with permissions to perform certain Vault operations. Attach to the namespace where the Vault will be deployed: kubens ping-cloud-devops-eks-vault Vault's cluster role binding creates a service account vault to perform delegated authentication and authorization checks. This service account is used by the Kubernetes authentication mechanism to allow authentication by other applications. Retrieve the service account secret name and set the environment variable SA_SECRET_NAME : export SA_SECRET_NAME = $( kubectl get serviceaccounts vault -o jsonpath = \"{.secrets[].name}\" ) Save the service account CA certificate, Kubernetes cluster API hostname, and the service account token to environment variables. These variable values will be used in the Kerberos auth method configuration. a. Save the service account CA certificate: export SA_CA_CRT = $( kubectl get secret $SA_SECRET_NAME -o jsonpath = \"{.data['ca\\.crt']}\" | base64 --decode ; echo ) b. Save the Kubernetes cluster API hostname: export K8S_API_HOST = $( kubectl config view --minify -o jsonpath = '{.clusters[0].cluster.server}' ) c. Save the service account token: export SA_TOKEN=$(kubectl get secret $SA_SECRET_NAME -o jsonpath=\"{.data['token']}\" | base64 --decode; echo) Add Vault policies \u00b6 You can choose the method to use to add the policies to your Vault: CLI API UI You'll need to add to Vault the policy files pingfederate.hcl and pingaccess.hcl to ensure the products have access only to their own secrets and keys. Be sure to update the <namespace> and <env> tags in your policy files with the appropriate values. The recommended value for <namespace> is your Kubernetes namespace. The typical values for <env> are dev, staging, and prod. For example: <namespace> : ping-cloud-eks-bob <env> : dev Adding the policies using the CLI \u00b6 Connect to a Vault pod and enter the following at the command line: Showing PingFederate entries here. You'll need to do the same for PingAccess. vault policy write <namespace>-<env>-pingfederate -<< # Enable transit secrets engine path \"sys/mounts/transit\" { capabilities = [ \"read\" , \"update\" , \"list\" ] } # To read enabled secrets engines path \"sys/mounts\" { capabilities = [ \"create\" , \"read\" , \"update\" , \"delete\" ] } # Manage the keys transit keys endpoint path \"transit/keys/<namespace>-<environment>-pingfederate\" { capabilities = [ \"create\" , \"read\" , \"update\" , \"list\" ] } # Manage the keys transit keys endpoint path \"transit/encrypt/<namespace>-<environment>-pingfederate\" { capabilities = [ \"create\" , \"read\" , \"update\" , \"list\" ] } # Manage the keys transit keys endpoint path \"transit/decrypt/<namespace>-<environment>-pingfederate\" { capabilities = [ \"create\" , \"read\" , \"update\" , \"list\" ] } #Manage the cubbyhole secrets engine path \"cubbyhole/<namespace>/<env>/pingfederate/masterkey\" { capabilities = [ \"create\" , \"read\" , \"update\" , \"list\" ] } EOF Using the API \u00b6 Your client needs to be able to access the Vault API endpoint. For example: curl \\ --header \"X-Vault-Token: ...\" \\ --request PUT \\ --data @pingfederate-policy.hcl \\ http://127.0.0.1:8200/v1/sys/policy/<namespace>-<env>-pingfederate Using the Vault UI \u00b6 Port-forward the Vault port for the UI, go to `https://localhost: , and add the entries (as shown for the CLI method): Showing PingFederate entries here. You'll need to do the same for PingAccess. Configure Kubernetes Auth \u00b6 You need to enable Kubernetes auth. It's important to note that Vault doesn't need to be deployed in a Kubernetes environment to support Kubernetes auth. You're also able to support multiple kubernetes clusters. The following commands can be performed by the Vault admin or a configuration management tool. Enable Kubernetes auth: kubectl exec vault-0 -- vault auth enable kubernetes Configure Kubernetes auth: kubectl exec vault-0 -- vault write auth/kubernetes/config \\ token_reviewer_jwt = $SA_TOKEN \\ kubernetes_host = $K8S_API_HOST \\ kubernetes_ca_cert = $SA_CA_CRT Register a role for each product. To give you more control over product permissions, we'll use this naming convention for roles: <k8s-namespace>-<environment>-<product_name> . For example k8s-namespace = ping-cloud-devops-eks-apps environment = dev product_name = pingfederate ping-cloud-devops-eks-apps-dev-pingfederate To register the roles, update the following command with your role name, the product namespace, and policy name before executing: kubectl exec vault-0 -- vault write auth/kubernetes/role/<namespace>-<environment>-<product_name> \\ bound_service_account_names=vault-auth \\ bound_service_account_namespaces=<application_namespace> \\ policies=<policy> Transit Secret Engine \u00b6 The Transit secret engine uses a Vault-managed key to support encryption and decryption of each product's master key. Each product implements a common interface (MasterKeyEncryptor) that will encrypt the master key while at rest. CubbyHole Secret Engine \u00b6 The CubbyHole secret engine is used to store the master key for each product. This is to assist backups and restoration. In addition, this can be used to assist with migrating configurations from one environment to another (for example, from dev to staging).","title":"Deploy Hashicorp Vault"},{"location":"deployment/deployVault/#deploy-hashicorp-vault","text":"This is an example of deploying Hashicorp Vault (Vault) with PingFederate and PingAccess to manage their corresponding master keys ( pf.pwk and pa.pwk ). Using Vault, you can also manage license files, DevOps keys, product secrets, and others.","title":"Deploy Hashicorp Vault"},{"location":"deployment/deployVault/#what-youll-do","text":"You'll clone the Vault Helm chart to deploy a near-production environment to validate and manage the product master keys, product secrets, and authentication policies.","title":"What you'll do"},{"location":"deployment/deployVault/#prerequisites","text":"You've already been through Get started to set up your DevOps environment and run a test deployment of the products. Vault . Helm . Vault uses Helm 3. Kubernetes 1.7 OpenSSL or your favorite PKI tool. Pull our pingidentity-getting-started repo to ensure you have the latest sources.","title":"Prerequisites"},{"location":"deployment/deployVault/#deployment-architecture","text":"Illustrated below are the specific configuration items we are using for this deployment. Additionally, we'll deploy Vault into Amazon Elastic Kubernetes Service (EKS) and using some of AWS's specific services ( AWS KMS and AWS DynamoDB ) to help simplify the deployment architecture.","title":"Deployment architecture"},{"location":"deployment/deployVault/#enable-tls","text":"Before you deploy Vault using Helm, you'll need to add the TLS key pair (public and private keys) and CA chain files as a Kubernetes secret. The public certificate and private key need to be separate files. You can either use OpenSSL to quickly create a self-signed certificate, or use one signed by your Certificate Authority. If you're using a self-signed certificate, the public certificate is also the CA certificate. Create the Kubernetes secret using Vault, the TLS key pair, and the certificate: kubectl create secret generic vault-certstore \\ --from-file = vault.key = <local_path_to_tls_key>/tls.key \\ --from-file = vault.crt = <local_path_to_tls_cert>/tls.crt \\ --from-file = vault.ca = <local_path_to_ca_cert>/vault.ca Ensure that these parameters in the values.yaml file located in your local pingidentity-devops-getting-started/20-kubernetes/08-vault/vault-helm directory are set as follows: global: Enable TLS globally: global : tlsDisable : false extraEnvironmentVars: Set the environment variable that will contain the path to the CA Certificate used for TLS. extraEnvironmentVars : VAULT_CACERT : /vault/userconfig/vault-certstore/vault.ca extraVolumes: Set the volume mount for the certificate store secret. This mount will contain the TLS public certificate, private key and CA certificate. extraVolumes : - type : secret name : vault-certstore ha: Set Vault to use high-availability (HA) mode. Vault uses Hashicorp Consul for its storage backend. The default configuration provided will work with the Consul (Helm) project by default. You can also manually configure Vault to use a different HA backend. ha : enabled : true replicas : 3 # Add the following parameters to the `listener \"tcp\"` element to enable TLS: config : | ui = true log_level = \"Debug\" listener \"tcp\" { tls_disable = 0 address = \"[::]:8200\" cluster_address = \"[::]:8201\" tls_cert_file = \"/vault/userconfig/vault-certstore/vault.crt\" tls_key_file = \"/vault/userconfig/vault-certstore/vault.key\" tls_client_ca_file = \"/vault/userconfig/vault-certstore/vault.ca\" }","title":"Enable TLS"},{"location":"deployment/deployVault/#storage-backend","text":"We can take advantage of some AWS services to simplify our deployment architecture. The Vault Helm chart has examples for using files and/or an existing Consul deployment. Here, we'll update Vault's HA deployment to use AWS DynamoDB. Create an AWS access key and secret with permissions to manage the dynamodb. You'll find the permissions that the Vault IAM user requires to manage dynamodb in the Vault documentation Required AWS Permissions . Vault will create the necessary table in dynamodb if it does not already exist. See the Vault documentation DynamoDB Storage Backend for additional parameters when using dynamodb as a storage mechanism. Add your AWS access and secret key as a Kubernetes secret: kubectl create secret generic dynamodb-access-secret-keys \\ --from-literal = AWS_ACCESS_KEY_ID = <your_aws_acccess_key> \\ --from-literal = AWS_SECRET_ACCESS_KEY = <your_aws_acccess_key_secret> Kubernetes can provide the secrets as environment variables that Vault can use, so you do not accidentally expose your the secret outside of Kubernetes. Update the values.yaml file to include your AWS key and secret within the extraSecretEnvironmentVars section: extraSecretEnvironmentVars : - envName : AWS_SECRET_ACCESS_KEY secretName : dynamodb-access-secret-key secretKey : AWS_SECRET_ACCESS_KEY - envName : AWS_ACCESS_KEY_ID secretName : dynamodb-access-secret-key secretKey : AWS_ACCESS_KEY_ID In the ha section, update the dynamodb storage element with your corresponding AWS region and dynamodb table name: storage \"dynamodb\" { ha_enabled = \"true\" region = \"<aws_region>\" table = \"<dynamodb_table_name>\" }","title":"Storage Backend"},{"location":"deployment/deployVault/#auto-unseal","text":"To keep things simple, we'll use Vault's Auto Unseal with the AWS Key Management Service (KMS). You'll need to set up your existing AWS access key and secret with the correct permissions. Because we're using dynamodb for backend storage, you'll also add the permissions to your AWS access key. See the Vault documentation AWS KMS authentication for more information. Vault can retrieve the AWS KMS key using an environment variable, so it is not accidentally exposed outside of the Kubernetes environment. Add the KMS key as a Kubernetes secret: kubectl create secret generic aws-kms-key-id \\ --from-literal = KMS_KEY_ID = <your_key_id> Update the values.yaml file to include your AWS key and secret in the extraSecretEnvironmentVars section: extraSecretEnvironmentVars : - envName : VAULT_AWSKMS_SEAL_KEY_ID secretName : aws-kms-key-id secretKey : KMS_KEY_ID In the ha section config map ( config ), add a seal element, and update the region parameter with your AWS region value: seal \"awskms\" { region = \"<aws_region>\" }","title":"Auto Unseal"},{"location":"deployment/deployVault/#deploy-vault-using-helm","text":"See the Vault documentation for Kubernetes for complete information. Go to your local pingidentity-devops-getting-started/20-kubernetes/08-vault/vault-helm directory and enter: helm install vault /. Information similar to the following will be displayed: NAME: vault LAST DEPLOYED: Thu Mar 12 17 :27:19 2020 NAMESPACE: ping-cloud-devops-eks STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Thank you for installing HashiCorp Vault! Your release is named vault. To learn more about the release, try: $ helm status vault $ helm get vault If this is the first time you've deployed Vault in this environment, you'll need to initialize it. Enter: kubectl exec vault-0 -- vault operator init If the initialization is successful, you'll receive the recovery keys and Initial Root token. Make sure you store the recovery keys and root token in a secure location. Information similar to the following is displayed: Recovery Key 1 : <key_1> Recovery Key 2 : <key_2> Recovery Key 3 : <key_3> Recovery Key 4 : <key_4> Recovery Key 5 : <key_5> Initial Root Token: <root_token> Success! Vault is initialized Recovery key initialized with 5 key shares and a key threshold of 3 . Please securely distribute the key shares printed above.","title":"Deploy Vault using Helm"},{"location":"deployment/deployVault/#pod-authentication","text":"Our products and applications need to have a Vault client token to authenticate to Vault. Because we're using Kubernetes, we can use Vault's Kubernetes auth method. When the Kubernetes auth method is enabled, Vault can use a pod's Kubernetes service account token to authenticate and exchange for a Vault client token. The Vault client token is associated with a particular role with permissions to perform certain Vault operations. Attach to the namespace where the Vault will be deployed: kubens ping-cloud-devops-eks-vault Vault's cluster role binding creates a service account vault to perform delegated authentication and authorization checks. This service account is used by the Kubernetes authentication mechanism to allow authentication by other applications. Retrieve the service account secret name and set the environment variable SA_SECRET_NAME : export SA_SECRET_NAME = $( kubectl get serviceaccounts vault -o jsonpath = \"{.secrets[].name}\" ) Save the service account CA certificate, Kubernetes cluster API hostname, and the service account token to environment variables. These variable values will be used in the Kerberos auth method configuration. a. Save the service account CA certificate: export SA_CA_CRT = $( kubectl get secret $SA_SECRET_NAME -o jsonpath = \"{.data['ca\\.crt']}\" | base64 --decode ; echo ) b. Save the Kubernetes cluster API hostname: export K8S_API_HOST = $( kubectl config view --minify -o jsonpath = '{.clusters[0].cluster.server}' ) c. Save the service account token: export SA_TOKEN=$(kubectl get secret $SA_SECRET_NAME -o jsonpath=\"{.data['token']}\" | base64 --decode; echo)","title":"Pod Authentication"},{"location":"deployment/deployVault/#add-vault-policies","text":"You can choose the method to use to add the policies to your Vault: CLI API UI You'll need to add to Vault the policy files pingfederate.hcl and pingaccess.hcl to ensure the products have access only to their own secrets and keys. Be sure to update the <namespace> and <env> tags in your policy files with the appropriate values. The recommended value for <namespace> is your Kubernetes namespace. The typical values for <env> are dev, staging, and prod. For example: <namespace> : ping-cloud-eks-bob <env> : dev","title":"Add Vault policies"},{"location":"deployment/deployVault/#adding-the-policies-using-the-cli","text":"Connect to a Vault pod and enter the following at the command line: Showing PingFederate entries here. You'll need to do the same for PingAccess. vault policy write <namespace>-<env>-pingfederate -<< # Enable transit secrets engine path \"sys/mounts/transit\" { capabilities = [ \"read\" , \"update\" , \"list\" ] } # To read enabled secrets engines path \"sys/mounts\" { capabilities = [ \"create\" , \"read\" , \"update\" , \"delete\" ] } # Manage the keys transit keys endpoint path \"transit/keys/<namespace>-<environment>-pingfederate\" { capabilities = [ \"create\" , \"read\" , \"update\" , \"list\" ] } # Manage the keys transit keys endpoint path \"transit/encrypt/<namespace>-<environment>-pingfederate\" { capabilities = [ \"create\" , \"read\" , \"update\" , \"list\" ] } # Manage the keys transit keys endpoint path \"transit/decrypt/<namespace>-<environment>-pingfederate\" { capabilities = [ \"create\" , \"read\" , \"update\" , \"list\" ] } #Manage the cubbyhole secrets engine path \"cubbyhole/<namespace>/<env>/pingfederate/masterkey\" { capabilities = [ \"create\" , \"read\" , \"update\" , \"list\" ] } EOF","title":"Adding the policies using the CLI"},{"location":"deployment/deployVault/#using-the-api","text":"Your client needs to be able to access the Vault API endpoint. For example: curl \\ --header \"X-Vault-Token: ...\" \\ --request PUT \\ --data @pingfederate-policy.hcl \\ http://127.0.0.1:8200/v1/sys/policy/<namespace>-<env>-pingfederate","title":"Using the API"},{"location":"deployment/deployVault/#using-the-vault-ui","text":"Port-forward the Vault port for the UI, go to `https://localhost: , and add the entries (as shown for the CLI method): Showing PingFederate entries here. You'll need to do the same for PingAccess.","title":"Using the Vault UI"},{"location":"deployment/deployVault/#configure-kubernetes-auth","text":"You need to enable Kubernetes auth. It's important to note that Vault doesn't need to be deployed in a Kubernetes environment to support Kubernetes auth. You're also able to support multiple kubernetes clusters. The following commands can be performed by the Vault admin or a configuration management tool. Enable Kubernetes auth: kubectl exec vault-0 -- vault auth enable kubernetes Configure Kubernetes auth: kubectl exec vault-0 -- vault write auth/kubernetes/config \\ token_reviewer_jwt = $SA_TOKEN \\ kubernetes_host = $K8S_API_HOST \\ kubernetes_ca_cert = $SA_CA_CRT Register a role for each product. To give you more control over product permissions, we'll use this naming convention for roles: <k8s-namespace>-<environment>-<product_name> . For example k8s-namespace = ping-cloud-devops-eks-apps environment = dev product_name = pingfederate ping-cloud-devops-eks-apps-dev-pingfederate To register the roles, update the following command with your role name, the product namespace, and policy name before executing: kubectl exec vault-0 -- vault write auth/kubernetes/role/<namespace>-<environment>-<product_name> \\ bound_service_account_names=vault-auth \\ bound_service_account_namespaces=<application_namespace> \\ policies=<policy>","title":"Configure Kubernetes Auth"},{"location":"deployment/deployVault/#transit-secret-engine","text":"The Transit secret engine uses a Vault-managed key to support encryption and decryption of each product's master key. Each product implements a common interface (MasterKeyEncryptor) that will encrypt the master key while at rest.","title":"Transit Secret Engine"},{"location":"deployment/deployVault/#cubbyhole-secret-engine","text":"The CubbyHole secret engine is used to store the master key for each product. This is to assist backups and restoration. In addition, this can be used to assist with migrating configurations from one environment to another (for example, from dev to staging).","title":"CubbyHole Secret Engine"},{"location":"deployment/k8sClusterSizing/","text":"Sizing Kubernetes clusters \u00b6 When creating your Kubernetes cluster, it's important that you size the nodes appropriately. Kubernetes cluster capacity \u00b6 When determining the capacity of your cluster, there are a number of ways to approach sizing the nodes. For example, if you calculated a cluster sizing of 16 cpu and 64GB RAM, you could break down the node sizing into these options: 2 nodes: 8 CPU / 32 Gb RAM 4 nodes: 4 CPU / 16 Gb RAM 8 nodes: 2 CPU / 8 Gb RAM 16 nodes: 1 CPU / 4 Gb RAM To understand which sizing option to select, let's examine the pros and cons. You can generally assume that instance cost per CPU/RAM is linear among the major cloud platforms. Option 1: fewer, larger nodes \u00b6 Pros \u00b6 If you have applications that are CPU or RAM intensive, having a larger number of nodes will ensure your application has sufficient resources. Cons \u00b6 High availability is difficult to achieve with a minimal set of nodes. If your application has 50 instances with 25 pods per node and a node goes down, you've lost 50% of your service. Scaling: When autoscaling your cluster, the increment size becomes larger, which may result in provisioning more hardware than what is required. Option 2: more, smaller nodes \u00b6 Pros \u00b6 High availability is easier to maintain. If you have 50 instances with 2 pods per node and one node goes down, you've only reduced your service by 4%. Cons \u00b6 More system overhead to manage all of the nodes. Possible under-utilization, as the nodes may be too small to add additional services. Guidance \u00b6 For production deployments where high availability is paramount, creating a cluster with more nodes and having less pods per node is preferable to ensure the health of your deployed service. For some applications, you may decide to size 1 pod per node. To determine the physical instance type, multiply the desired resources for each service by the number of pods per node, plus additional for system overhead. Follow product guidelines to determine system requirements. Example service using 3 pods per node \u00b6 Typically deployed with 2 CPU and 4GB RAM. Multiply by 3. Node requirement: 6 CPU 12 GB RAM. Add 10% for system overhead. For these requirements in Amazon Web Services (AWS), a c5.2xlarge type (8 CPU / 16 Gb RAM) may be the instance type selected. To determine the base number of nodes required, divide the number of pods by 3 to determine your minimum cluster size. Ensure that you add definitions for cluster horizontal auto-scaling to ensure your cluster scales in or out as needed.","title":"Sizing Kubernetes Clusters"},{"location":"deployment/k8sClusterSizing/#sizing-kubernetes-clusters","text":"When creating your Kubernetes cluster, it's important that you size the nodes appropriately.","title":"Sizing Kubernetes clusters"},{"location":"deployment/k8sClusterSizing/#kubernetes-cluster-capacity","text":"When determining the capacity of your cluster, there are a number of ways to approach sizing the nodes. For example, if you calculated a cluster sizing of 16 cpu and 64GB RAM, you could break down the node sizing into these options: 2 nodes: 8 CPU / 32 Gb RAM 4 nodes: 4 CPU / 16 Gb RAM 8 nodes: 2 CPU / 8 Gb RAM 16 nodes: 1 CPU / 4 Gb RAM To understand which sizing option to select, let's examine the pros and cons. You can generally assume that instance cost per CPU/RAM is linear among the major cloud platforms.","title":"Kubernetes cluster capacity"},{"location":"deployment/k8sClusterSizing/#option-1-fewer-larger-nodes","text":"","title":"Option 1: fewer, larger nodes"},{"location":"deployment/k8sClusterSizing/#pros","text":"If you have applications that are CPU or RAM intensive, having a larger number of nodes will ensure your application has sufficient resources.","title":"Pros"},{"location":"deployment/k8sClusterSizing/#cons","text":"High availability is difficult to achieve with a minimal set of nodes. If your application has 50 instances with 25 pods per node and a node goes down, you've lost 50% of your service. Scaling: When autoscaling your cluster, the increment size becomes larger, which may result in provisioning more hardware than what is required.","title":"Cons"},{"location":"deployment/k8sClusterSizing/#option-2-more-smaller-nodes","text":"","title":"Option 2: more, smaller nodes"},{"location":"deployment/k8sClusterSizing/#pros_1","text":"High availability is easier to maintain. If you have 50 instances with 2 pods per node and one node goes down, you've only reduced your service by 4%.","title":"Pros"},{"location":"deployment/k8sClusterSizing/#cons_1","text":"More system overhead to manage all of the nodes. Possible under-utilization, as the nodes may be too small to add additional services.","title":"Cons"},{"location":"deployment/k8sClusterSizing/#guidance","text":"For production deployments where high availability is paramount, creating a cluster with more nodes and having less pods per node is preferable to ensure the health of your deployed service. For some applications, you may decide to size 1 pod per node. To determine the physical instance type, multiply the desired resources for each service by the number of pods per node, plus additional for system overhead. Follow product guidelines to determine system requirements.","title":"Guidance"},{"location":"deployment/k8sClusterSizing/#example-service-using-3-pods-per-node","text":"Typically deployed with 2 CPU and 4GB RAM. Multiply by 3. Node requirement: 6 CPU 12 GB RAM. Add 10% for system overhead. For these requirements in Amazon Web Services (AWS), a c5.2xlarge type (8 CPU / 16 Gb RAM) may be the instance type selected. To determine the base number of nodes required, divide the number of pods by 3 to determine your minimum cluster size. Ensure that you add definitions for cluster horizontal auto-scaling to ensure your cluster scales in or out as needed.","title":"Example service using 3 pods per node"},{"location":"deployment/pfMultiRegionAWSPrereq/","text":"AWS Multi-Region PingFederate Cluster (Prerequisite) \u00b6 In this example, we'll deploy 2 PingFederate clusters, each in a different Amazon Web Services (AWS) region. An AWS virtual private cloud (VPC) is assigned and dedicated to each cluster. Throughout this document, \"VPC\" is synonymous with \"cluster\". Prerequisites \u00b6 AWS CLI . eksctl , the current version. AWS account permissions to create clusters. Configure the AWS CLI \u00b6 If you've not already done so, configure the AWS CLI to use your profile and credentials: Assign your profile and supply your aws_access_key_id and aws_secret_access_key . Enter: aws configure --profile = <aws-profile> Then enter your aws_access_key_id and aws_secret_access_key . Open your ~/.aws/credentials file in a text editor and add your AWS role_arn . For example: \u201crole_arn = arn:aws:iam::xxxxxxxx4146:role/GTE\u201d Create the Multi-Region Clusters \u00b6 Create the YAML files to configure the the clusters. You'll create the clusters in different AWS regions. We'll be using the ca-central-1 region and the us-west-2 region in this document. Configure the first cluster. For example, using the ca-central-1 region and the reserved CIDR 172.16.0.0: apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : pingfed-ca-central-1 region : ca-central-1 version : \"1.17\" vpc : cidr : 172.16.0.0/16 managedNodeGroups : - name : us-west-2a-worker-nodes instanceType : t3a.2xlarge labels : {} tags : {} minSize : 1 maxSize : 2 desiredCapacity : 1 volumeSize : 12 privateNetworking : true ssh : publicKeyPath : ~/.ssh/id_rsa.pub iam : withAddonPolicies : imageBuilder : true autoScaler : true externalDNS : true certManager : true appMesh : true ebs : true fsx : true efs : true albIngress : true xRay : true cloudWatch : true - name : us-west-2b-worker-nodes instanceType : t3a.2xlarge labels : {} tags : {} minSize : 1 maxSize : 2 desiredCapacity : 1 volumeSize : 12 privateNetworking : true ssh : publicKeyPath : ~/.ssh/id_rsa.pub iam : withAddonPolicies : imageBuilder : true autoScaler : true externalDNS : true certManager : true appMesh : true ebs : true fsx : true efs : true albIngress : true xRay : true cloudWatch : true Config Settings For production purposes, select a VPC with a private IP. The ssh entry is optional, allowing you to SSH in to your cluster. Configure the second cluster. For example, using the us-west-2 region and the reserved CIDR 10.0.0.0: apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : pingfed-us-west-2 region : us-west-2 version : \"1.17\" vpc : cidr : 10.0.0.0/16 managedNodeGroups : - name : us-west-2a-worker-nodes instanceType : t3a.2xlarge labels : {} tags : {} minSize : 1 maxSize : 2 desiredCapacity : 1 volumeSize : 12 privateNetworking : true ssh : publicKeyPath : ~/.ssh/id_rsa.pub iam : withAddonPolicies : imageBuilder : true autoScaler : true externalDNS : true certManager : true appMesh : true ebs : true fsx : true efs : true albIngress : true xRay : true cloudWatch : true - name : us-west-2b-worker-nodes instanceType : t3a.2xlarge labels : {} tags : {} minSize : 1 maxSize : 2 desiredCapacity : 1 volumeSize : 12 privateNetworking : true ssh : publicKeyPath : ~/.ssh/id_rsa.pub iam : withAddonPolicies : imageBuilder : true autoScaler : true externalDNS : true certManager : true appMesh : true ebs : true fsx : true efs : true albIngress : true xRay : true cloudWatch : true Config Settings For production purposes, select a VPC with a private IP. The ssh entry is optional, allowing you to SSH in to your cluster. Create the clusters using eksctl . Create the first cluster. For example: eksctl create cluster -f ca-central-1.yaml --profile <aws-profile> Create the second cluster. For example: eksctl create cluster -f us-west-2.yaml --profile <aws-profile> Log in to the AWS console, go to the VPC service, select Your VPCs (under Virtual Private Cloud), and note the VPC details for the clusters you've created. VPC IDs Retain the VpcId values for the ca-central-1 and us-west-2 VPCs. You'll use these in subsequent steps. Set up VPC peering between the two clusters. You'll create a peering connection from the cluster in the us-west-2 region to the cluster in the ca-central-1 region. You'll do this from the VPC Dashboard as in the prior step. In the top right of the page, select the Oregon (us-west-2) region. Select Peering Connections , and click Create Peering Connection . Assign a unique name for the peering connection (for example, us-west-2-to-ca-central-1). Under Select a local VPC to peer with , enter the VpcId value for the us-west-2 VPC. Under Select another VPC to peer with , select My account --> Another region --> Canada Central (ca-central-1). Under VPC (Accepter) , enter the VpcId value for the ca-central-1 region. Click Create Peering Connection . When successful, a confirmation is displayed. Click OK to continue. In the top right of the page, change the region to Canada Central . Select Peering Connections . Connection Status Notice that the peering connection status for us-west-2 shows as Pending Acceptance . Select the ca-central-1 connection, click the Actions dropdown list, and select Accept Request . You'll be prompted to confirm. Connection Status The VPC peering connection status should now show as Active . Get the subnets information for each cluster node. Each cluster node uses a different subnet, so there'll be three subnets assigned to each VPC. The information displayed will contain the subnet ID for each subnet. You'll use the subnet IDs in the subsequent step to get the associated routing tables. In the top right of the page, change the region to Oregon . Go to the EC2 service, and select Instances . Apply a filter, if needed, to find your nodes for the cluster. Select each node, and record the Subnet ID of each. You'll use the subnet IDs in a subsequent step. In the top right of the page, change the region to Canada Central , and repeat the 2 previous steps to find and record the subnet IDs for this VPC. Get the routing table associated with the subnets for each VPC. Go to the VPC service. (You're still using the Canada Central region.) In the VPC Dashboard, select Subnets . For each subnet displayed, record the Routing Table value. You may have a single routing table for all of your subnets. You'll use the routing table ID or IDs in a subsequent step. In the top right of the page, change the region to Oregon , and repeat the 2 previous steps to find and record the routing table ID or IDs for this VPC. Modify the routing table or tables for each VPC to add a route to the other VPC using the peering connection you created. In the VPC Dashboard, select Route Tables . (You're still using the Oregon region.) Select the route table you recorded for the us-west-2 (Oregon) VPC, and click the Routes button. You should see 2 routes displayed. Click Edit Routes --> Add Route , and for Destination , enter the CIDR block for the ca-central-1 cluster (172.16.0.0/16). For Target , select the VPC peering connection you created in a prior step. Click Save Routes . A route for the ca-central-1 cluster directed to the peering connection is displayed. If more than one routing table is used for the us-west-2 VPC, repeat the previous steps for each routing table. In the top right of the page, change the region to Canada Central . Select the route table you recorded for the ca-central-1 (Canada Central) VPC, and click the Routes button. You should see 2 routes displayed. Click Edit Routes --> Add Route , and for Destination , enter the CIDR block for the us-west-2 cluster (10.0.0.0/16). For Target , select the VPC peering connection you created in a prior step. Click Save Routes . A route for the us-west-2 cluster directed to the peering connection is displayed. If more than one routing table is used for the ca-central-1 VPC, repeat the previous steps for each routing table. Update the Security Groups for each VPC. You'll get the Security Group IDs for each VPC, then add inbound and outbound rules for both the us-west-2 VPC, and the ca-central-1 VPC. In the VPC Dashboard, select Security Groups . (You're still using the Canada Central region.) Apply a filter to find the security groups for the ca-central-1 cluster, and select the security group with \u201c-nodegroup\u201d in the name. This is the security group used for the firewall settings for all the worker nodes in the ca-central-1 cluster. Click Inbound Rules --> Add Rule . Select these values for the rule: Name Value Type Custom TCP Rule Protocol TCP Port Range 7600-7700 Source Custom. Enter the CIDR block for the us-west-2 (10.0.0.0/16) cluster. Click Save Rules to save the inbound security group rule for the ca-central-1 cluster. Click Outbound Rules --> Add Rule . Select these values for the rule: Name Value Type Custom TCP Rule Protocol TCP Port Range 7600-7700 Source Custom. Enter the CIDR block for the us-west-2 (10.0.0.0/16) cluster. Click Save Rules to save the outbound security group rule for the ca-central-1 cluster. In the top right of the page, change the region to Oregon . You'll now repeat the previous steps to add inbound and outbound rules for the us-west-2 cluster. Apply a filter to find the security groups for the us-west-2 cluster, and select the security group with \u201c-nodegroup\u201d in the name. This is the security group used for the firewall settings for all the worker nodes in the us-west-2 cluster. Click Inbound Rules --> Add Rule . Select these values for the rule: Name Value Type Custom TCP Rule Protocol TCP Port Range 7600-7700 Source Custom. Enter the CIDR block for the ca-central-1 (172.16.0.0/16) cluster. Click Save Rules to save the inbound security group rule for the us-west-2 cluster. Click Outbound Rules --> Add Rule . Select these values for the rule: Name Value Type Custom TCP Rule Protocol TCP Port Range 7600-7700 Source Custom. Enter the CIDR block for the ca-central-1 (172.16.0.0/16) cluster. Click Save Rules to save the outbound security group rule for the us-west-2 cluster.","title":"AWS Multi-Region Prereq"},{"location":"deployment/pfMultiRegionAWSPrereq/#aws-multi-region-pingfederate-cluster-prerequisite","text":"In this example, we'll deploy 2 PingFederate clusters, each in a different Amazon Web Services (AWS) region. An AWS virtual private cloud (VPC) is assigned and dedicated to each cluster. Throughout this document, \"VPC\" is synonymous with \"cluster\".","title":"AWS Multi-Region PingFederate Cluster (Prerequisite)"},{"location":"deployment/pfMultiRegionAWSPrereq/#prerequisites","text":"AWS CLI . eksctl , the current version. AWS account permissions to create clusters.","title":"Prerequisites"},{"location":"deployment/pfMultiRegionAWSPrereq/#configure-the-aws-cli","text":"If you've not already done so, configure the AWS CLI to use your profile and credentials: Assign your profile and supply your aws_access_key_id and aws_secret_access_key . Enter: aws configure --profile = <aws-profile> Then enter your aws_access_key_id and aws_secret_access_key . Open your ~/.aws/credentials file in a text editor and add your AWS role_arn . For example: \u201crole_arn = arn:aws:iam::xxxxxxxx4146:role/GTE\u201d","title":"Configure the AWS CLI"},{"location":"deployment/pfMultiRegionAWSPrereq/#create-the-multi-region-clusters","text":"Create the YAML files to configure the the clusters. You'll create the clusters in different AWS regions. We'll be using the ca-central-1 region and the us-west-2 region in this document. Configure the first cluster. For example, using the ca-central-1 region and the reserved CIDR 172.16.0.0: apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : pingfed-ca-central-1 region : ca-central-1 version : \"1.17\" vpc : cidr : 172.16.0.0/16 managedNodeGroups : - name : us-west-2a-worker-nodes instanceType : t3a.2xlarge labels : {} tags : {} minSize : 1 maxSize : 2 desiredCapacity : 1 volumeSize : 12 privateNetworking : true ssh : publicKeyPath : ~/.ssh/id_rsa.pub iam : withAddonPolicies : imageBuilder : true autoScaler : true externalDNS : true certManager : true appMesh : true ebs : true fsx : true efs : true albIngress : true xRay : true cloudWatch : true - name : us-west-2b-worker-nodes instanceType : t3a.2xlarge labels : {} tags : {} minSize : 1 maxSize : 2 desiredCapacity : 1 volumeSize : 12 privateNetworking : true ssh : publicKeyPath : ~/.ssh/id_rsa.pub iam : withAddonPolicies : imageBuilder : true autoScaler : true externalDNS : true certManager : true appMesh : true ebs : true fsx : true efs : true albIngress : true xRay : true cloudWatch : true Config Settings For production purposes, select a VPC with a private IP. The ssh entry is optional, allowing you to SSH in to your cluster. Configure the second cluster. For example, using the us-west-2 region and the reserved CIDR 10.0.0.0: apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : pingfed-us-west-2 region : us-west-2 version : \"1.17\" vpc : cidr : 10.0.0.0/16 managedNodeGroups : - name : us-west-2a-worker-nodes instanceType : t3a.2xlarge labels : {} tags : {} minSize : 1 maxSize : 2 desiredCapacity : 1 volumeSize : 12 privateNetworking : true ssh : publicKeyPath : ~/.ssh/id_rsa.pub iam : withAddonPolicies : imageBuilder : true autoScaler : true externalDNS : true certManager : true appMesh : true ebs : true fsx : true efs : true albIngress : true xRay : true cloudWatch : true - name : us-west-2b-worker-nodes instanceType : t3a.2xlarge labels : {} tags : {} minSize : 1 maxSize : 2 desiredCapacity : 1 volumeSize : 12 privateNetworking : true ssh : publicKeyPath : ~/.ssh/id_rsa.pub iam : withAddonPolicies : imageBuilder : true autoScaler : true externalDNS : true certManager : true appMesh : true ebs : true fsx : true efs : true albIngress : true xRay : true cloudWatch : true Config Settings For production purposes, select a VPC with a private IP. The ssh entry is optional, allowing you to SSH in to your cluster. Create the clusters using eksctl . Create the first cluster. For example: eksctl create cluster -f ca-central-1.yaml --profile <aws-profile> Create the second cluster. For example: eksctl create cluster -f us-west-2.yaml --profile <aws-profile> Log in to the AWS console, go to the VPC service, select Your VPCs (under Virtual Private Cloud), and note the VPC details for the clusters you've created. VPC IDs Retain the VpcId values for the ca-central-1 and us-west-2 VPCs. You'll use these in subsequent steps. Set up VPC peering between the two clusters. You'll create a peering connection from the cluster in the us-west-2 region to the cluster in the ca-central-1 region. You'll do this from the VPC Dashboard as in the prior step. In the top right of the page, select the Oregon (us-west-2) region. Select Peering Connections , and click Create Peering Connection . Assign a unique name for the peering connection (for example, us-west-2-to-ca-central-1). Under Select a local VPC to peer with , enter the VpcId value for the us-west-2 VPC. Under Select another VPC to peer with , select My account --> Another region --> Canada Central (ca-central-1). Under VPC (Accepter) , enter the VpcId value for the ca-central-1 region. Click Create Peering Connection . When successful, a confirmation is displayed. Click OK to continue. In the top right of the page, change the region to Canada Central . Select Peering Connections . Connection Status Notice that the peering connection status for us-west-2 shows as Pending Acceptance . Select the ca-central-1 connection, click the Actions dropdown list, and select Accept Request . You'll be prompted to confirm. Connection Status The VPC peering connection status should now show as Active . Get the subnets information for each cluster node. Each cluster node uses a different subnet, so there'll be three subnets assigned to each VPC. The information displayed will contain the subnet ID for each subnet. You'll use the subnet IDs in the subsequent step to get the associated routing tables. In the top right of the page, change the region to Oregon . Go to the EC2 service, and select Instances . Apply a filter, if needed, to find your nodes for the cluster. Select each node, and record the Subnet ID of each. You'll use the subnet IDs in a subsequent step. In the top right of the page, change the region to Canada Central , and repeat the 2 previous steps to find and record the subnet IDs for this VPC. Get the routing table associated with the subnets for each VPC. Go to the VPC service. (You're still using the Canada Central region.) In the VPC Dashboard, select Subnets . For each subnet displayed, record the Routing Table value. You may have a single routing table for all of your subnets. You'll use the routing table ID or IDs in a subsequent step. In the top right of the page, change the region to Oregon , and repeat the 2 previous steps to find and record the routing table ID or IDs for this VPC. Modify the routing table or tables for each VPC to add a route to the other VPC using the peering connection you created. In the VPC Dashboard, select Route Tables . (You're still using the Oregon region.) Select the route table you recorded for the us-west-2 (Oregon) VPC, and click the Routes button. You should see 2 routes displayed. Click Edit Routes --> Add Route , and for Destination , enter the CIDR block for the ca-central-1 cluster (172.16.0.0/16). For Target , select the VPC peering connection you created in a prior step. Click Save Routes . A route for the ca-central-1 cluster directed to the peering connection is displayed. If more than one routing table is used for the us-west-2 VPC, repeat the previous steps for each routing table. In the top right of the page, change the region to Canada Central . Select the route table you recorded for the ca-central-1 (Canada Central) VPC, and click the Routes button. You should see 2 routes displayed. Click Edit Routes --> Add Route , and for Destination , enter the CIDR block for the us-west-2 cluster (10.0.0.0/16). For Target , select the VPC peering connection you created in a prior step. Click Save Routes . A route for the us-west-2 cluster directed to the peering connection is displayed. If more than one routing table is used for the ca-central-1 VPC, repeat the previous steps for each routing table. Update the Security Groups for each VPC. You'll get the Security Group IDs for each VPC, then add inbound and outbound rules for both the us-west-2 VPC, and the ca-central-1 VPC. In the VPC Dashboard, select Security Groups . (You're still using the Canada Central region.) Apply a filter to find the security groups for the ca-central-1 cluster, and select the security group with \u201c-nodegroup\u201d in the name. This is the security group used for the firewall settings for all the worker nodes in the ca-central-1 cluster. Click Inbound Rules --> Add Rule . Select these values for the rule: Name Value Type Custom TCP Rule Protocol TCP Port Range 7600-7700 Source Custom. Enter the CIDR block for the us-west-2 (10.0.0.0/16) cluster. Click Save Rules to save the inbound security group rule for the ca-central-1 cluster. Click Outbound Rules --> Add Rule . Select these values for the rule: Name Value Type Custom TCP Rule Protocol TCP Port Range 7600-7700 Source Custom. Enter the CIDR block for the us-west-2 (10.0.0.0/16) cluster. Click Save Rules to save the outbound security group rule for the ca-central-1 cluster. In the top right of the page, change the region to Oregon . You'll now repeat the previous steps to add inbound and outbound rules for the us-west-2 cluster. Apply a filter to find the security groups for the us-west-2 cluster, and select the security group with \u201c-nodegroup\u201d in the name. This is the security group used for the firewall settings for all the worker nodes in the us-west-2 cluster. Click Inbound Rules --> Add Rule . Select these values for the rule: Name Value Type Custom TCP Rule Protocol TCP Port Range 7600-7700 Source Custom. Enter the CIDR block for the ca-central-1 (172.16.0.0/16) cluster. Click Save Rules to save the inbound security group rule for the us-west-2 cluster. Click Outbound Rules --> Add Rule . Select these values for the rule: Name Value Type Custom TCP Rule Protocol TCP Port Range 7600-7700 Source Custom. Enter the CIDR block for the ca-central-1 (172.16.0.0/16) cluster. Click Save Rules to save the outbound security group rule for the us-west-2 cluster.","title":"Create the Multi-Region Clusters"},{"location":"docker-builds/","text":"Build Your Own Docker Images \u00b6 This document explains the Ping Identity Docker images and how they are built. There are several image builds provided, including Dockerfile , README.md and scripts (i.e. entrypoint.sh ) for each docker build. Some images are used as foundations to build other images. Docker Image Description pingbase Base OS, default enviroment variables, volumes, healthcheck and entrypoint command defintions. This image provides a base to all Ping Identity docker images pingcommon Files and scripts used with all Ping Identity docker images pingdatacommon Files and scripts used with all Ping Identity Data docker images (i.e. PingDirectory, PingDataSync) pingfederate Product details for PingFederate pingaccess Product details for PingAccess pingdirectory Product details for PingDirectory pingdatasync Product details for PingDataSync pingdownloader Product bits download service ldap-sdk-tools LDAP SDK tools availble of use with Ping Directory Hook Scripts Used by Docker Builds \u00b6 There are several hook scripts used by the docker builds. Full details on these hooks are included in the Docker Builds Hooks Document","title":"Build Your Own Docker Images"},{"location":"docker-builds/#build-your-own-docker-images","text":"This document explains the Ping Identity Docker images and how they are built. There are several image builds provided, including Dockerfile , README.md and scripts (i.e. entrypoint.sh ) for each docker build. Some images are used as foundations to build other images. Docker Image Description pingbase Base OS, default enviroment variables, volumes, healthcheck and entrypoint command defintions. This image provides a base to all Ping Identity docker images pingcommon Files and scripts used with all Ping Identity docker images pingdatacommon Files and scripts used with all Ping Identity Data docker images (i.e. PingDirectory, PingDataSync) pingfederate Product details for PingFederate pingaccess Product details for PingAccess pingdirectory Product details for PingDirectory pingdatasync Product details for PingDataSync pingdownloader Product bits download service ldap-sdk-tools LDAP SDK tools availble of use with Ping Directory","title":"Build Your Own Docker Images"},{"location":"docker-builds/#hook-scripts-used-by-docker-builds","text":"There are several hook scripts used by the docker builds. Full details on these hooks are included in the Docker Builds Hooks Document","title":"Hook Scripts Used by Docker Builds"},{"location":"docker-builds/DOCKER_BUILDS_HOOKS/","text":"Docker Builds - Hooks \u00b6 Audience - Operators of DevOps Cloud environments. Not intended for Developers and admins of the Ping Identity products. Description - This document describes the many number of scripts that are called in during the lifecycle of a Ping Identity docker image from the initial entrypoint.sh script. Included with the base docker images, there is an example/stub provided for all possible hooks. It is very important that these names be used if a developer wishes to make subtle changes to their server-profile. The full ordered list of scripts that are called depending on what type of image (i.e. pingdirectory or pingdatasync) are: Hooks Details \u00b6 Details on hooks can be found within the code of each hook in the Docker-Builds Repo as well in pingidentity-devops-getting-started/docs/docker-images/<image_name>/hooks for each of the products images.","title":"Docker Builds - Hooks"},{"location":"docker-builds/DOCKER_BUILDS_HOOKS/#docker-builds-hooks","text":"Audience - Operators of DevOps Cloud environments. Not intended for Developers and admins of the Ping Identity products. Description - This document describes the many number of scripts that are called in during the lifecycle of a Ping Identity docker image from the initial entrypoint.sh script. Included with the base docker images, there is an example/stub provided for all possible hooks. It is very important that these names be used if a developer wishes to make subtle changes to their server-profile. The full ordered list of scripts that are called depending on what type of image (i.e. pingdirectory or pingdatasync) are:","title":"Docker Builds - Hooks"},{"location":"docker-builds/DOCKER_BUILDS_HOOKS/#hooks-details","text":"Details on hooks can be found within the code of each hook in the Docker-Builds Repo as well in pingidentity-devops-getting-started/docs/docker-images/<image_name>/hooks for each of the products images.","title":"Hooks Details"},{"location":"docker-images/","text":"See Using release tags for more information.","title":"Index"},{"location":"docker-images/apache-jmeter/","text":"Environment Variables \u00b6 The following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/run.sh ## Docker Container Hook Scripts Please go here for details on all apache-jmeter hook scripts This document is auto-generated from apache-jmeter/Dockerfile Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Apache JMeter"},{"location":"docker-images/apache-jmeter/#environment-variables","text":"The following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/run.sh ## Docker Container Hook Scripts Please go here for details on all apache-jmeter hook scripts This document is auto-generated from apache-jmeter/Dockerfile Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Environment Variables"},{"location":"docker-images/apache-jmeter/hooks/","text":"Ping Identity DevOps apache-jmeter Hooks \u00b6 List of available hooks: * 04-check-variables.sh * 17-check-license.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from apache-jmeter/opt/staging/hooks Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `apache-jmeter` Hooks"},{"location":"docker-images/apache-jmeter/hooks/#ping-identity-devops-apache-jmeter-hooks","text":"List of available hooks: * 04-check-variables.sh * 17-check-license.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from apache-jmeter/opt/staging/hooks Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps apache-jmeter Hooks"},{"location":"docker-images/apache-jmeter/hooks/04-check-variables.sh/","text":"Ping Identity DevOps apache-jmeter Hook - 04-check-variables.sh \u00b6 This document is auto-generated from apache-jmeter/opt/staging/hooks/04-check-variables.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `apache-jmeter` Hook - `04-check-variables.sh`"},{"location":"docker-images/apache-jmeter/hooks/04-check-variables.sh/#ping-identity-devops-apache-jmeter-hook-04-check-variablessh","text":"This document is auto-generated from apache-jmeter/opt/staging/hooks/04-check-variables.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps apache-jmeter Hook - 04-check-variables.sh"},{"location":"docker-images/apache-jmeter/hooks/17-check-license.sh/","text":"Ping Identity DevOps apache-jmeter Hook - 17-check-license.sh \u00b6 This document is auto-generated from apache-jmeter/opt/staging/hooks/17-check-license.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `apache-jmeter` Hook - `17-check-license.sh`"},{"location":"docker-images/apache-jmeter/hooks/17-check-license.sh/#ping-identity-devops-apache-jmeter-hook-17-check-licensesh","text":"This document is auto-generated from apache-jmeter/opt/staging/hooks/17-check-license.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps apache-jmeter Hook - 17-check-license.sh"},{"location":"docker-images/ldap-sdk-tools/","text":"Ping Identity Docker Image - ldap-sdk-tools \u00b6 This docker image provides an apine image with the LDAP Client SDK tools to be used against other PingDirectory instances. Related Docker Images \u00b6 pingidentity/pingdownloader - Image used to download ping product openjdk:8-jre8-alpine - Alpine server to run LDAP SDK Tools from Environment Variables \u00b6 The following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} PATH /opt/tools:${PATH} ## List all available tools docker run -it --rm pingidentity/ldap-sdk-tools:edge ls Use LDAPSearch \u00b6 Get some help \u00b6 docker run -it --rm pingidentity/ldap-sdk-tools:edge ldapsearch --help Simple search \u00b6 docker run -it --rm pingidentity/ldap-sdk-tools:edge \\ ldapsearch \\ -b dc = example,dc = com \\ -p 1389 \"(objectClass=*)\" Save output to host file \u00b6 docker run -it --rm \\ -v /tmp:/opt/out \\ pingidentity/ldap-sdk-tools:edge \\ ldapsearch \\ --baseDN dc = example,dc = com \\ --port 1389 \\ --outputFormat json \"(objectClass=*)\" >/tmp/search-result.json Use manage-certificates \u00b6 trusting certificates \u00b6 PWD = 2FederateM0re mkdir -p /tmp/hibp docker run -it --rm \\ -v /tmp/hibp:/opt/out \\ pingidentity/ldap-sdk-tools:edge \\ manage-certificates trust-server-certificate \\ --hostname haveibeenpwned.com \\ --port 443 \\ --keystore /opt/out/hibp-2019.jks \\ --keystore-password ${ PWD } ls -all /tmp/hibp keytool -list \\ -keystore /tmp/hibp/hibp-2019.jks \\ -storepass ${ PWD } Docker Container Hook Scripts \u00b6 Please go here for details on all ldap-sdk-tools hook scripts This document is auto-generated from ldap-sdk-tools/Dockerfile Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"ldap-sdk-tools"},{"location":"docker-images/ldap-sdk-tools/#ping-identity-docker-image-ldap-sdk-tools","text":"This docker image provides an apine image with the LDAP Client SDK tools to be used against other PingDirectory instances.","title":"Ping Identity Docker Image - ldap-sdk-tools"},{"location":"docker-images/ldap-sdk-tools/#related-docker-images","text":"pingidentity/pingdownloader - Image used to download ping product openjdk:8-jre8-alpine - Alpine server to run LDAP SDK Tools from","title":"Related Docker Images"},{"location":"docker-images/ldap-sdk-tools/#environment-variables","text":"The following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} PATH /opt/tools:${PATH} ## List all available tools docker run -it --rm pingidentity/ldap-sdk-tools:edge ls","title":"Environment Variables"},{"location":"docker-images/ldap-sdk-tools/#use-ldapsearch","text":"","title":"Use LDAPSearch"},{"location":"docker-images/ldap-sdk-tools/#get-some-help","text":"docker run -it --rm pingidentity/ldap-sdk-tools:edge ldapsearch --help","title":"Get some help"},{"location":"docker-images/ldap-sdk-tools/#simple-search","text":"docker run -it --rm pingidentity/ldap-sdk-tools:edge \\ ldapsearch \\ -b dc = example,dc = com \\ -p 1389 \"(objectClass=*)\"","title":"Simple search"},{"location":"docker-images/ldap-sdk-tools/#save-output-to-host-file","text":"docker run -it --rm \\ -v /tmp:/opt/out \\ pingidentity/ldap-sdk-tools:edge \\ ldapsearch \\ --baseDN dc = example,dc = com \\ --port 1389 \\ --outputFormat json \"(objectClass=*)\" >/tmp/search-result.json","title":"Save output to host file"},{"location":"docker-images/ldap-sdk-tools/#use-manage-certificates","text":"","title":"Use manage-certificates"},{"location":"docker-images/ldap-sdk-tools/#trusting-certificates","text":"PWD = 2FederateM0re mkdir -p /tmp/hibp docker run -it --rm \\ -v /tmp/hibp:/opt/out \\ pingidentity/ldap-sdk-tools:edge \\ manage-certificates trust-server-certificate \\ --hostname haveibeenpwned.com \\ --port 443 \\ --keystore /opt/out/hibp-2019.jks \\ --keystore-password ${ PWD } ls -all /tmp/hibp keytool -list \\ -keystore /tmp/hibp/hibp-2019.jks \\ -storepass ${ PWD }","title":"trusting certificates"},{"location":"docker-images/ldap-sdk-tools/#docker-container-hook-scripts","text":"Please go here for details on all ldap-sdk-tools hook scripts This document is auto-generated from ldap-sdk-tools/Dockerfile Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Docker Container Hook Scripts"},{"location":"docker-images/ldap-sdk-tools/hooks/","text":"Ping Identity DevOps ldap-sdk-tools Hooks \u00b6 There are no default hooks defined for the ldap-sdk-tools image. Hooks defined by parent images (i.e. pingcommon/pingdatacommon) will be inherited by this image. Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `ldap-sdk-tools` Hooks"},{"location":"docker-images/ldap-sdk-tools/hooks/#ping-identity-devops-ldap-sdk-tools-hooks","text":"There are no default hooks defined for the ldap-sdk-tools image. Hooks defined by parent images (i.e. pingcommon/pingdatacommon) will be inherited by this image. Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps ldap-sdk-tools Hooks"},{"location":"docker-images/pingaccess/","text":"Ping Identity DevOps Docker Image - pingaccess \u00b6 This docker image includes the Ping Identity PingAccess product binaries and associated hook scripts to create and run both PingAccess Admin and Engine nodes. Related Docker Images \u00b6 pingidentity/pingbase - Parent Image > This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingcommon - Common Ping files (i.e. hook scripts) pingidentity/pingdownloader - Used to download product bits Environment Variables \u00b6 In addition to environment variables inherited from pingidentity/pingbase , the following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} PING_PRODUCT PingAccess LICENSE_DIR ${SERVER_ROOT_DIR}/conf LICENSE_FILE_NAME pingaccess.lic LICENSE_SHORT_NAME PA LICENSE_VERSION ${LICENSE_VERSION} PA_ADMIN_PASSWORD ${INITIAL_ADMIN_PASSWORD} PING_IDENTITY_PASSWORD ${PA_ADMIN_PASSWORD} Specify a password for administrator user for interaction with admin API OPERATIONAL_MODE STANDALONE PA_ADMIN_PASSWORD_INITIAL 2Access Change non-default password at startup by including this and PING_IDENTITY_PASSWORD STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/run.sh TAIL_LOG_FILES ${SERVER_ROOT_DIR}/log/pingaccess.log PA_ADMIN_PORT 9000 PA_ENGINE_PORT 3000 ## Ports Exposed The following ports are exposed from the container. If a variable is used, then it may come from a parent container - ${PA_ADMIN_PORT} - ${PA_ENGINE_PORT} - ${HTTPS_PORT} Running a PingAccess container \u00b6 To run a PingAccess container: docker run \\ --name pingaccess \\ --publish 9000 :9000 \\ --publish 443 :443 \\ --detach \\ --env SERVER_PROFILE_URL = https://github.com/pingidentity/pingidentity-server-profiles.git \\ --env SERVER_PROFILE_PATH = getting-started/pingaccess \\ --env PING_IDENTITY_ACCEPT_EULA = YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingaccess:edge Follow Docker logs with: docker logs -f pingaccess If using the command above with the embedded server profile , log in with: * https://localhost:9000 * Username: Administrator * Password: 2FederateM0re Docker Container Hook Scripts \u00b6 Please go here for details on all pingaccess hook scripts This document is auto-generated from pingaccess/Dockerfile Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"PingAccess"},{"location":"docker-images/pingaccess/#ping-identity-devops-docker-image-pingaccess","text":"This docker image includes the Ping Identity PingAccess product binaries and associated hook scripts to create and run both PingAccess Admin and Engine nodes.","title":"Ping Identity DevOps Docker Image - pingaccess"},{"location":"docker-images/pingaccess/#related-docker-images","text":"pingidentity/pingbase - Parent Image > This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingcommon - Common Ping files (i.e. hook scripts) pingidentity/pingdownloader - Used to download product bits","title":"Related Docker Images"},{"location":"docker-images/pingaccess/#environment-variables","text":"In addition to environment variables inherited from pingidentity/pingbase , the following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} PING_PRODUCT PingAccess LICENSE_DIR ${SERVER_ROOT_DIR}/conf LICENSE_FILE_NAME pingaccess.lic LICENSE_SHORT_NAME PA LICENSE_VERSION ${LICENSE_VERSION} PA_ADMIN_PASSWORD ${INITIAL_ADMIN_PASSWORD} PING_IDENTITY_PASSWORD ${PA_ADMIN_PASSWORD} Specify a password for administrator user for interaction with admin API OPERATIONAL_MODE STANDALONE PA_ADMIN_PASSWORD_INITIAL 2Access Change non-default password at startup by including this and PING_IDENTITY_PASSWORD STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/run.sh TAIL_LOG_FILES ${SERVER_ROOT_DIR}/log/pingaccess.log PA_ADMIN_PORT 9000 PA_ENGINE_PORT 3000 ## Ports Exposed The following ports are exposed from the container. If a variable is used, then it may come from a parent container - ${PA_ADMIN_PORT} - ${PA_ENGINE_PORT} - ${HTTPS_PORT}","title":"Environment Variables"},{"location":"docker-images/pingaccess/#running-a-pingaccess-container","text":"To run a PingAccess container: docker run \\ --name pingaccess \\ --publish 9000 :9000 \\ --publish 443 :443 \\ --detach \\ --env SERVER_PROFILE_URL = https://github.com/pingidentity/pingidentity-server-profiles.git \\ --env SERVER_PROFILE_PATH = getting-started/pingaccess \\ --env PING_IDENTITY_ACCEPT_EULA = YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingaccess:edge Follow Docker logs with: docker logs -f pingaccess If using the command above with the embedded server profile , log in with: * https://localhost:9000 * Username: Administrator * Password: 2FederateM0re","title":"Running a PingAccess container"},{"location":"docker-images/pingaccess/#docker-container-hook-scripts","text":"Please go here for details on all pingaccess hook scripts This document is auto-generated from pingaccess/Dockerfile Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Docker Container Hook Scripts"},{"location":"docker-images/pingaccess/hooks/","text":"Ping Identity DevOps pingaccess Hooks \u00b6 List of available hooks: * 50-before-post-start.sh * 51-add-engine.sh * 80-post-start.sh * 81-after-start-process.sh * 83-change-password.sh * 85-import-configuration.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from pingaccess/opt/staging/hooks Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingaccess` Hooks"},{"location":"docker-images/pingaccess/hooks/#ping-identity-devops-pingaccess-hooks","text":"List of available hooks: * 50-before-post-start.sh * 51-add-engine.sh * 80-post-start.sh * 81-after-start-process.sh * 83-change-password.sh * 85-import-configuration.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from pingaccess/opt/staging/hooks Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingaccess Hooks"},{"location":"docker-images/pingaccess/hooks/50-before-post-start.sh/","text":"Ping Identity DevOps pingaccess Hook - 50-before-post-start.sh \u00b6 This is called after the start or restart sequence has finished and before the server within the container starts This document is auto-generated from pingaccess/opt/staging/hooks/50-before-post-start.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingaccess` Hook - `50-before-post-start.sh`"},{"location":"docker-images/pingaccess/hooks/50-before-post-start.sh/#ping-identity-devops-pingaccess-hook-50-before-post-startsh","text":"This is called after the start or restart sequence has finished and before the server within the container starts This document is auto-generated from pingaccess/opt/staging/hooks/50-before-post-start.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingaccess Hook - 50-before-post-start.sh"},{"location":"docker-images/pingaccess/hooks/51-add-engine.sh/","text":"Ping Identity DevOps pingaccess Hook - 51-add-engine.sh \u00b6 This script is started in the background immediately before the server within the container is started This is useful to implement any logic that needs to occur after the server is up and running For example, enabling replication in PingDirectory, initializing Sync Pipes in PingDataSync or issuing admin API calls to PingFederate or PingAccess This document is auto-generated from pingaccess/opt/staging/hooks/51-add-engine.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingaccess` Hook - `51-add-engine.sh`"},{"location":"docker-images/pingaccess/hooks/51-add-engine.sh/#ping-identity-devops-pingaccess-hook-51-add-enginesh","text":"This script is started in the background immediately before the server within the container is started This is useful to implement any logic that needs to occur after the server is up and running For example, enabling replication in PingDirectory, initializing Sync Pipes in PingDataSync or issuing admin API calls to PingFederate or PingAccess This document is auto-generated from pingaccess/opt/staging/hooks/51-add-engine.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingaccess Hook - 51-add-engine.sh"},{"location":"docker-images/pingaccess/hooks/80-post-start.sh/","text":"Ping Identity DevOps pingaccess Hook - 80-post-start.sh \u00b6 This script is used to import any configurations that are needed after PingAccess starts This document is auto-generated from pingaccess/opt/staging/hooks/80-post-start.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingaccess` Hook - `80-post-start.sh`"},{"location":"docker-images/pingaccess/hooks/80-post-start.sh/#ping-identity-devops-pingaccess-hook-80-post-startsh","text":"This script is used to import any configurations that are needed after PingAccess starts This document is auto-generated from pingaccess/opt/staging/hooks/80-post-start.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingaccess Hook - 80-post-start.sh"},{"location":"docker-images/pingaccess/hooks/81-after-start-process.sh/","text":"Ping Identity DevOps pingaccess Hook - 81-after-start-process.sh \u00b6 This document is auto-generated from pingaccess/opt/staging/hooks/81-after-start-process.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingaccess` Hook - `81-after-start-process.sh`"},{"location":"docker-images/pingaccess/hooks/81-after-start-process.sh/#ping-identity-devops-pingaccess-hook-81-after-start-processsh","text":"This document is auto-generated from pingaccess/opt/staging/hooks/81-after-start-process.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingaccess Hook - 81-after-start-process.sh"},{"location":"docker-images/pingaccess/hooks/81-import-initial-configuration.sh/","text":"Ping Identity DevOps pingaccess Hook - 81-import-initial-configuration.sh \u00b6 This script is started in the background immediately before the server within the container is started This is useful to implement any logic that needs to occur after the server is up and running For example, enabling replication in PingDirectory, initializing Sync Pipes in PingDataSync or issuing admin API calls to PingFederate or PingAccess This document auto-generated from pingaccess/hooks/81-import-initial-configuration.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingaccess` Hook - `81-import-initial-configuration.sh`"},{"location":"docker-images/pingaccess/hooks/81-import-initial-configuration.sh/#ping-identity-devops-pingaccess-hook-81-import-initial-configurationsh","text":"This script is started in the background immediately before the server within the container is started This is useful to implement any logic that needs to occur after the server is up and running For example, enabling replication in PingDirectory, initializing Sync Pipes in PingDataSync or issuing admin API calls to PingFederate or PingAccess This document auto-generated from pingaccess/hooks/81-import-initial-configuration.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingaccess Hook - 81-import-initial-configuration.sh"},{"location":"docker-images/pingaccess/hooks/83-change-password.sh/","text":"Ping Identity DevOps pingaccess Hook - 83-change-password.sh \u00b6 This document is auto-generated from pingaccess/opt/staging/hooks/83-change-password.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingaccess` Hook - `83-change-password.sh`"},{"location":"docker-images/pingaccess/hooks/83-change-password.sh/#ping-identity-devops-pingaccess-hook-83-change-passwordsh","text":"This document is auto-generated from pingaccess/opt/staging/hooks/83-change-password.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingaccess Hook - 83-change-password.sh"},{"location":"docker-images/pingaccess/hooks/83-create-initial-password.sh/","text":"Ping Identity DevOps pingaccess Hook - 83-create-initial-password.sh \u00b6 This document auto-generated from pingaccess/hooks/83-create-initial-password.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingaccess` Hook - `83-create-initial-password.sh`"},{"location":"docker-images/pingaccess/hooks/83-create-initial-password.sh/#ping-identity-devops-pingaccess-hook-83-create-initial-passwordsh","text":"This document auto-generated from pingaccess/hooks/83-create-initial-password.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingaccess Hook - 83-create-initial-password.sh"},{"location":"docker-images/pingaccess/hooks/85-import-configuration.sh/","text":"Ping Identity DevOps pingaccess Hook - 85-import-configuration.sh \u00b6 This script is started in the background immediately before the server within the container is started This is useful to implement any logic that needs to occur after the server is up and running For example, enabling replication in PingDirectory, initializing Sync Pipes in PingDataSync or issuing admin API calls to PingFederate or PingAccess This document is auto-generated from pingaccess/opt/staging/hooks/85-import-configuration.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingaccess` Hook - `85-import-configuration.sh`"},{"location":"docker-images/pingaccess/hooks/85-import-configuration.sh/#ping-identity-devops-pingaccess-hook-85-import-configurationsh","text":"This script is started in the background immediately before the server within the container is started This is useful to implement any logic that needs to occur after the server is up and running For example, enabling replication in PingDirectory, initializing Sync Pipes in PingDataSync or issuing admin API calls to PingFederate or PingAccess This document is auto-generated from pingaccess/opt/staging/hooks/85-import-configuration.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingaccess Hook - 85-import-configuration.sh"},{"location":"docker-images/pingaccess/hooks/85-import-initial-configuration.sh/","text":"Ping Identity DevOps pingaccess Hook - 85-import-initial-configuration.sh \u00b6 This script is started in the background immediately before the server within the container is started This is useful to implement any logic that needs to occur after the server is up and running For example, enabling replication in PingDirectory, initializing Sync Pipes in PingDataSync or issuing admin API calls to PingFederate or PingAccess This document auto-generated from pingaccess/hooks/85-import-initial-configuration.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingaccess` Hook - `85-import-initial-configuration.sh`"},{"location":"docker-images/pingaccess/hooks/85-import-initial-configuration.sh/#ping-identity-devops-pingaccess-hook-85-import-initial-configurationsh","text":"This script is started in the background immediately before the server within the container is started This is useful to implement any logic that needs to occur after the server is up and running For example, enabling replication in PingDirectory, initializing Sync Pipes in PingDataSync or issuing admin API calls to PingFederate or PingAccess This document auto-generated from pingaccess/hooks/85-import-initial-configuration.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingaccess Hook - 85-import-initial-configuration.sh"},{"location":"docker-images/pingbase/","text":"Ping Identity Docker Image - pingbase \u00b6 This docker image provides a base image for all Ping Identity DevOps product images. Environment Variables \u00b6 The following environment ENV variables can be used with this image. ENV Variable Default Description IMAGE_VERSION ${IMAGE_VERSION} Image version, set by build process of the docker build IMAGE_GIT_REV ${IMAGE_GIT_REV} Image git revision, set by build process of the docker build PING_IDENTITY_ACCEPT_EULA NO Must be set to 'YES' for the container to start BASE ${BASE:-/opt} Location of the top level directory where everything is located in image/container IN_DIR ${BASE}/in Location of a local server-profile volume OUT_DIR ${BASE}/out Path to the runtime volume SERVER_BITS_DIR ${BASE}/server Path to the server bits BAK_DIR ${BASE}/backup Path to a volume generically used to export or backup data LOGS_DIR ${BASE}/logs Path to a volume generically used for logging PING_IDENTITY_DEVOPS_FILE devops-secret File name for devops-creds passed as a Docker secret STAGING_DIR ${BASE}/staging Path to the staging area where the remote and local server profiles can be merged STAGING_MANIFEST ${BASE}/staging-manifest.txt Path to a manifest of files expected in the staging dir on first image startup CLEAN_STAGING_DIR true Whether to clean the staging dir when the image starts SECRETS_DIR /run/secrets Default path to the secrets TOPOLOGY_FILE ${STAGING_DIR}/topology.json Path to the topology file HOOKS_DIR ${STAGING_DIR}/hooks Path where all the hooks scripts are stored CONTAINER_ENV ${STAGING_DIR}/.env Environment Property file use to share variables between scripts in container SERVER_PROFILE_DIR /tmp/server-profile Path where the remote server profile is checked out or cloned before being staged prior to being applied on the runtime SERVER_PROFILE_URL A valid git HTTPS URL (not ssh) SERVER_PROFILE_URL_REDACT true SERVER_PROFILE_BRANCH A valid git branch (optional) SERVER_PROFILE_PATH The subdirectory in the git repo SERVER_PROFILE_UPDATE false Whether to update the server profile upon container restart SERVER_ROOT_DIR ${OUT_DIR}/instance Path from which the runtime executes SECURITY_CHECKS_STRICT false Requires strict checks on security SECURITY_CHECKS_FILENAME .jwk .pin Perform a check for filenames that may violate security (i.e. secret material) UNSAFE_CONTINUE_ON_ERROR If this is set to true, then the container will provide a hard warning and continue. LICENSE_DIR ${SERVER_ROOT_DIR} License directory and filename STARTUP_COMMAND The command that the entrypoint will execute in the foreground to instantiate the container STARTUP_FOREGROUND_OPTS The command-line options to provide to the the startup command when the container starts with the server in the foreground. This is the normal start flow for the container STARTUP_BACKGROUND_OPTS The command-line options to provide to the the startup command when the container starts with the server in the background. This is the debug start flow for the container PING_IDENTITY_DEVOPS_KEY_REDACT true TAIL_LOG_FILES A whitespace separated list of log files to tail to the container standard output - DO NOT USE WILDCARDS like /path/to/logs/*.log TAIL_LOG_PARALLEL Set to true to use parallel for the invocation of the tail utility COLORIZE_LOGS true If 'true', the output logs will be colorized with GREENs and REDs, otherwise, no colorization will be done. This is good for tools that monitor logs and colorization gets in the way. LOCATION Docker Location default value LOCATION_VALIDATION true Any string denoting a logical/physical location MAX_HEAP_SIZE 384m Heap size (for java products) JVM_TUNING AGGRESSIVE VERBOSE false Triggers verbose messages in scripts using the set -x option. PING_DEBUG false Set the server in debug mode, with increased output PING_PRODUCT The name of Ping product. Should be overridden by child images. PING_PRODUCT_VALIDATION true i.e. PingFederate,PingDirectory ADDITIONAL_SETUP_ARGS List of setup arguments passed to Ping Data setup-arguments.txt file LDAP_PORT 389 Port over which to communicate for LDAP LDAPS_PORT 636 Port over which to communicate for LDAPS HTTPS_PORT 443 Port over which to communicate for HTTPS JMX_PORT 689 Port for monitoring over JMX protocol ORCHESTRATION_TYPE The type of orchestration tool used to run the container, normally set in the deployment (.yaml) file. Expected values include: - compose - swarm - kubernetes Defaults to blank (i.e. No type is set) USER_BASE_DN dc=example,dc=com DOLLAR '$' PD_ENGINE_PUBLIC_HOSTNAME localhost PD (PingDirectory) public hostname that may be used in redirects PD_ENGINE_PRIVATE_HOSTNAME pingdirectory PD (PingDirectory) private hostname PDP_ENGINE_PUBLIC_HOSTNAME localhost PDP (PingDirectoryProxy) public hostname that may be used in redirects PDP_ENGINE_PRIVATE_HOSTNAME pingdirectoryproxy PDP (PingDirectoryProxy) private hostname PDS_ENGINE_PUBLIC_HOSTNAME localhost PDS (PingDataSync) public hostname that may be used in redirects PDS_ENGINE_PRIVATE_HOSTNAME pingdatasync PDS (PingDataSync) private hostname PDG_ENGINE_PUBLIC_HOSTNAME localhost PDG (PingDataGovernance) public hostname that may be used in redirects PDG_ENGINE_PRIVATE_HOSTNAME pingdatagovernance PDG (PingDataGovernance) private hostname PDGP_ENGINE_PUBLIC_HOSTNAME localhost PDGP (PingDataGovernance-PAP) public hostname that may be used in redirects PDGP_ENGINE_PRIVATE_HOSTNAME pingdatagovernancepap PDGP (PingDataGovernance-PAP) private hostname PF_ENGINE_PUBLIC_HOSTNAME localhost PF (PingFederate) engine public hostname that may be used in redirects PF_ENGINE_PRIVATE_HOSTNAME pingfederate PF (PingFederate) engine private hostname PF_ADMIN_PUBLIC_HOSTNAME localhost PF (PingFederate) admin public hostname that may be used in redirects PF_ADMIN_PRIVATE_HOSTNAME pingfederate-admin PF (PingFederate) admin private hostname PA_ENGINE_PUBLIC_HOSTNAME localhost PA (PingAccess) engine prublic hostname that may be used in redirects PA_ENGINE_PRIVATE_HOSTNAME pingaccess PA (PingAccess) engine private hostname PA_ADMIN_PUBLIC_HOSTNAME localhost PA (PingAccess) admin public hostname that may be used in redirects PA_ADMIN_PRIVATE_HOSTNAME pingaccess-admin PA (PingAccess) admin private hostname ROOT_USER administrator the default administrative user for PingData ROOT_USER_DN cn=${ROOT_USER} ENV ${BASE}/.profile MOTD_URL https://raw.githubusercontent.com/pingidentity/pingidentity-devops-getting-started/master/motd/motd.json Instructs the image to pull the MOTD json from the followig URL. If this MOTD_URL variable is empty, then no motd will be downloaded. The format of this MOTD file must match the example provided in the url: https://raw.githubusercontent.com/pingidentity/pingidentity-devops-getting-started/master/motd/motd.json PS1 \\${PING_PRODUCT}:\\h:\\w\\n> Default shell prompt (i.e. productName:hostname:workingDir) PING_CONTAINER_PRIVILEGED true Whether to run the process as Root or not if set to false, user spec can be left to default (uid:9031, gid:9999) or a custom uid can be passed with PING_CONTAINER_UID and PING_CONTAINER_GID PING_CONTAINER_UID 9031 The user ID the product will use if PING_CONTAINER_PRIVILEGED is set to false PING_CONTAINER_GID 9999 The group ID the product will use if PING_CONTAINER_PRIVILEGED is set to false PING_CONTAINER_UNAME ping The user name the product will use if PING_CONTAINER_PRIVILEGED is set to false and a user with that ID does not exist already PING_CONTAINER_GNAME identity The group name the product will use if PING_CONTAINER_PRIVILEGED is set to false and a group with that ID does not exist already JAVA_HOME /opt/java PATH ${JAVA_HOME}/bin:${BASE}:${SERVER_ROOT_DIR}/bin:${PATH} ## Docker Container Hook Scripts Please go here for details on all pingbase hook scripts This document is auto-generated from pingbase/Dockerfile Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"PingBase"},{"location":"docker-images/pingbase/#ping-identity-docker-image-pingbase","text":"This docker image provides a base image for all Ping Identity DevOps product images.","title":"Ping Identity Docker Image - pingbase"},{"location":"docker-images/pingbase/#environment-variables","text":"The following environment ENV variables can be used with this image. ENV Variable Default Description IMAGE_VERSION ${IMAGE_VERSION} Image version, set by build process of the docker build IMAGE_GIT_REV ${IMAGE_GIT_REV} Image git revision, set by build process of the docker build PING_IDENTITY_ACCEPT_EULA NO Must be set to 'YES' for the container to start BASE ${BASE:-/opt} Location of the top level directory where everything is located in image/container IN_DIR ${BASE}/in Location of a local server-profile volume OUT_DIR ${BASE}/out Path to the runtime volume SERVER_BITS_DIR ${BASE}/server Path to the server bits BAK_DIR ${BASE}/backup Path to a volume generically used to export or backup data LOGS_DIR ${BASE}/logs Path to a volume generically used for logging PING_IDENTITY_DEVOPS_FILE devops-secret File name for devops-creds passed as a Docker secret STAGING_DIR ${BASE}/staging Path to the staging area where the remote and local server profiles can be merged STAGING_MANIFEST ${BASE}/staging-manifest.txt Path to a manifest of files expected in the staging dir on first image startup CLEAN_STAGING_DIR true Whether to clean the staging dir when the image starts SECRETS_DIR /run/secrets Default path to the secrets TOPOLOGY_FILE ${STAGING_DIR}/topology.json Path to the topology file HOOKS_DIR ${STAGING_DIR}/hooks Path where all the hooks scripts are stored CONTAINER_ENV ${STAGING_DIR}/.env Environment Property file use to share variables between scripts in container SERVER_PROFILE_DIR /tmp/server-profile Path where the remote server profile is checked out or cloned before being staged prior to being applied on the runtime SERVER_PROFILE_URL A valid git HTTPS URL (not ssh) SERVER_PROFILE_URL_REDACT true SERVER_PROFILE_BRANCH A valid git branch (optional) SERVER_PROFILE_PATH The subdirectory in the git repo SERVER_PROFILE_UPDATE false Whether to update the server profile upon container restart SERVER_ROOT_DIR ${OUT_DIR}/instance Path from which the runtime executes SECURITY_CHECKS_STRICT false Requires strict checks on security SECURITY_CHECKS_FILENAME .jwk .pin Perform a check for filenames that may violate security (i.e. secret material) UNSAFE_CONTINUE_ON_ERROR If this is set to true, then the container will provide a hard warning and continue. LICENSE_DIR ${SERVER_ROOT_DIR} License directory and filename STARTUP_COMMAND The command that the entrypoint will execute in the foreground to instantiate the container STARTUP_FOREGROUND_OPTS The command-line options to provide to the the startup command when the container starts with the server in the foreground. This is the normal start flow for the container STARTUP_BACKGROUND_OPTS The command-line options to provide to the the startup command when the container starts with the server in the background. This is the debug start flow for the container PING_IDENTITY_DEVOPS_KEY_REDACT true TAIL_LOG_FILES A whitespace separated list of log files to tail to the container standard output - DO NOT USE WILDCARDS like /path/to/logs/*.log TAIL_LOG_PARALLEL Set to true to use parallel for the invocation of the tail utility COLORIZE_LOGS true If 'true', the output logs will be colorized with GREENs and REDs, otherwise, no colorization will be done. This is good for tools that monitor logs and colorization gets in the way. LOCATION Docker Location default value LOCATION_VALIDATION true Any string denoting a logical/physical location MAX_HEAP_SIZE 384m Heap size (for java products) JVM_TUNING AGGRESSIVE VERBOSE false Triggers verbose messages in scripts using the set -x option. PING_DEBUG false Set the server in debug mode, with increased output PING_PRODUCT The name of Ping product. Should be overridden by child images. PING_PRODUCT_VALIDATION true i.e. PingFederate,PingDirectory ADDITIONAL_SETUP_ARGS List of setup arguments passed to Ping Data setup-arguments.txt file LDAP_PORT 389 Port over which to communicate for LDAP LDAPS_PORT 636 Port over which to communicate for LDAPS HTTPS_PORT 443 Port over which to communicate for HTTPS JMX_PORT 689 Port for monitoring over JMX protocol ORCHESTRATION_TYPE The type of orchestration tool used to run the container, normally set in the deployment (.yaml) file. Expected values include: - compose - swarm - kubernetes Defaults to blank (i.e. No type is set) USER_BASE_DN dc=example,dc=com DOLLAR '$' PD_ENGINE_PUBLIC_HOSTNAME localhost PD (PingDirectory) public hostname that may be used in redirects PD_ENGINE_PRIVATE_HOSTNAME pingdirectory PD (PingDirectory) private hostname PDP_ENGINE_PUBLIC_HOSTNAME localhost PDP (PingDirectoryProxy) public hostname that may be used in redirects PDP_ENGINE_PRIVATE_HOSTNAME pingdirectoryproxy PDP (PingDirectoryProxy) private hostname PDS_ENGINE_PUBLIC_HOSTNAME localhost PDS (PingDataSync) public hostname that may be used in redirects PDS_ENGINE_PRIVATE_HOSTNAME pingdatasync PDS (PingDataSync) private hostname PDG_ENGINE_PUBLIC_HOSTNAME localhost PDG (PingDataGovernance) public hostname that may be used in redirects PDG_ENGINE_PRIVATE_HOSTNAME pingdatagovernance PDG (PingDataGovernance) private hostname PDGP_ENGINE_PUBLIC_HOSTNAME localhost PDGP (PingDataGovernance-PAP) public hostname that may be used in redirects PDGP_ENGINE_PRIVATE_HOSTNAME pingdatagovernancepap PDGP (PingDataGovernance-PAP) private hostname PF_ENGINE_PUBLIC_HOSTNAME localhost PF (PingFederate) engine public hostname that may be used in redirects PF_ENGINE_PRIVATE_HOSTNAME pingfederate PF (PingFederate) engine private hostname PF_ADMIN_PUBLIC_HOSTNAME localhost PF (PingFederate) admin public hostname that may be used in redirects PF_ADMIN_PRIVATE_HOSTNAME pingfederate-admin PF (PingFederate) admin private hostname PA_ENGINE_PUBLIC_HOSTNAME localhost PA (PingAccess) engine prublic hostname that may be used in redirects PA_ENGINE_PRIVATE_HOSTNAME pingaccess PA (PingAccess) engine private hostname PA_ADMIN_PUBLIC_HOSTNAME localhost PA (PingAccess) admin public hostname that may be used in redirects PA_ADMIN_PRIVATE_HOSTNAME pingaccess-admin PA (PingAccess) admin private hostname ROOT_USER administrator the default administrative user for PingData ROOT_USER_DN cn=${ROOT_USER} ENV ${BASE}/.profile MOTD_URL https://raw.githubusercontent.com/pingidentity/pingidentity-devops-getting-started/master/motd/motd.json Instructs the image to pull the MOTD json from the followig URL. If this MOTD_URL variable is empty, then no motd will be downloaded. The format of this MOTD file must match the example provided in the url: https://raw.githubusercontent.com/pingidentity/pingidentity-devops-getting-started/master/motd/motd.json PS1 \\${PING_PRODUCT}:\\h:\\w\\n> Default shell prompt (i.e. productName:hostname:workingDir) PING_CONTAINER_PRIVILEGED true Whether to run the process as Root or not if set to false, user spec can be left to default (uid:9031, gid:9999) or a custom uid can be passed with PING_CONTAINER_UID and PING_CONTAINER_GID PING_CONTAINER_UID 9031 The user ID the product will use if PING_CONTAINER_PRIVILEGED is set to false PING_CONTAINER_GID 9999 The group ID the product will use if PING_CONTAINER_PRIVILEGED is set to false PING_CONTAINER_UNAME ping The user name the product will use if PING_CONTAINER_PRIVILEGED is set to false and a user with that ID does not exist already PING_CONTAINER_GNAME identity The group name the product will use if PING_CONTAINER_PRIVILEGED is set to false and a group with that ID does not exist already JAVA_HOME /opt/java PATH ${JAVA_HOME}/bin:${BASE}:${SERVER_ROOT_DIR}/bin:${PATH} ## Docker Container Hook Scripts Please go here for details on all pingbase hook scripts This document is auto-generated from pingbase/Dockerfile Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Environment Variables"},{"location":"docker-images/pingbase/hooks/","text":"Ping Identity DevOps pingbase Hooks \u00b6 There are no default hooks defined for the pingbase image. Hooks defined by parent images (i.e. pingcommon/pingdatacommon) will be inherited by this image. Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingbase` Hooks"},{"location":"docker-images/pingbase/hooks/#ping-identity-devops-pingbase-hooks","text":"There are no default hooks defined for the pingbase image. Hooks defined by parent images (i.e. pingcommon/pingdatacommon) will be inherited by this image. Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingbase Hooks"},{"location":"docker-images/pingcentral/","text":"Ping Identity DevOps Docker Image - pingcentral \u00b6 This docker image includes the Ping Identity PingCentral product binaries and associated hook scripts to create and run PingCentral in a container. Related Docker Images \u00b6 pingidentity/pingbase - Parent Image > This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingcommon - Common Ping files (i.e. hook scripts) pingidentity/pingdownloader - Used to download product bits Environment Variables \u00b6 In addition to environment variables inherited from pingidentity/pingbase , the following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} PING_CENTRAL_SERVER_PORT 9022 PING_PRODUCT PingCentral LICENSE_DIR ${SERVER_ROOT_DIR}/conf LICENSE_FILE_NAME pingcentral.lic LICENSE_SHORT_NAME PC LICENSE_VERSION ${LICENSE_VERSION} STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/run.sh TAIL_LOG_FILES ${SERVER_ROOT_DIR}/log/application.log PING_CENTRAL_LOG_LEVEL INFO PING_CENTRAL_BLIND_TRUST false PING_CENTRAL_VERIFY_HOSTNAME true ## Ports Exposed The following ports are exposed from the container. If a variable is used, then it may come from a parent container - 9022 Running a PingCentral container \u00b6 To run a PingCentral container with your devops configuration file: ```shell docker run -Pt \\ --name pingcentral \\ --env-file ~/.pingidentity/devops \\ --env PING_IDENTITY_ACCEPT_EULA=YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingcentral:edge or with long options in the background: ```shell docker run \\ --name pingcentral \\ --publish 9022:9022 \\ --detach \\ --env-file ~/.pingidentity/devops \\ --env PING_IDENTITY_ACCEPT_EULA=YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingcentral:edge or if you want to specify everything yourself: docker run \\ --name pingcentral \\ --publish 9022 :9022 \\ --detach \\ --env PING_IDENTITY_ACCEPT_EULA = YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingcentral:edge Follow Docker logs with: docker logs -f pingcentral If using the command above with the embedded server profile , log in with: * https://localhost:9022/ * Username: Administrator * Password: 2Federate Docker Container Hook Scripts \u00b6 Please go here for details on all pingcentral hook scripts This document is auto-generated from pingcentral/Dockerfile Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"PingCentral"},{"location":"docker-images/pingcentral/#ping-identity-devops-docker-image-pingcentral","text":"This docker image includes the Ping Identity PingCentral product binaries and associated hook scripts to create and run PingCentral in a container.","title":"Ping Identity DevOps Docker Image - pingcentral"},{"location":"docker-images/pingcentral/#related-docker-images","text":"pingidentity/pingbase - Parent Image > This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingcommon - Common Ping files (i.e. hook scripts) pingidentity/pingdownloader - Used to download product bits","title":"Related Docker Images"},{"location":"docker-images/pingcentral/#environment-variables","text":"In addition to environment variables inherited from pingidentity/pingbase , the following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} PING_CENTRAL_SERVER_PORT 9022 PING_PRODUCT PingCentral LICENSE_DIR ${SERVER_ROOT_DIR}/conf LICENSE_FILE_NAME pingcentral.lic LICENSE_SHORT_NAME PC LICENSE_VERSION ${LICENSE_VERSION} STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/run.sh TAIL_LOG_FILES ${SERVER_ROOT_DIR}/log/application.log PING_CENTRAL_LOG_LEVEL INFO PING_CENTRAL_BLIND_TRUST false PING_CENTRAL_VERIFY_HOSTNAME true ## Ports Exposed The following ports are exposed from the container. If a variable is used, then it may come from a parent container - 9022","title":"Environment Variables"},{"location":"docker-images/pingcentral/#running-a-pingcentral-container","text":"To run a PingCentral container with your devops configuration file: ```shell docker run -Pt \\ --name pingcentral \\ --env-file ~/.pingidentity/devops \\ --env PING_IDENTITY_ACCEPT_EULA=YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingcentral:edge or with long options in the background: ```shell docker run \\ --name pingcentral \\ --publish 9022:9022 \\ --detach \\ --env-file ~/.pingidentity/devops \\ --env PING_IDENTITY_ACCEPT_EULA=YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingcentral:edge or if you want to specify everything yourself: docker run \\ --name pingcentral \\ --publish 9022 :9022 \\ --detach \\ --env PING_IDENTITY_ACCEPT_EULA = YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingcentral:edge Follow Docker logs with: docker logs -f pingcentral If using the command above with the embedded server profile , log in with: * https://localhost:9022/ * Username: Administrator * Password: 2Federate","title":"Running a PingCentral container"},{"location":"docker-images/pingcentral/#docker-container-hook-scripts","text":"Please go here for details on all pingcentral hook scripts This document is auto-generated from pingcentral/Dockerfile Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Docker Container Hook Scripts"},{"location":"docker-images/pingcentral/hooks/","text":"Ping Identity DevOps pingcentral Hooks \u00b6 There are no default hooks defined for the pingcentral image. Hooks defined by parent images (i.e. pingcommon/pingdatacommon) will be inherited by this image. Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcentral` Hooks"},{"location":"docker-images/pingcentral/hooks/#ping-identity-devops-pingcentral-hooks","text":"There are no default hooks defined for the pingcentral image. Hooks defined by parent images (i.e. pingcommon/pingdatacommon) will be inherited by this image. Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcentral Hooks"},{"location":"docker-images/pingcommon/","text":"Ping Identity Docker Image - pingcommon \u00b6 This docker image provides a busybox image to house the base hook scripts and default entrypoint.sh used throughout the Ping Identity DevOps product images. Docker Container Hook Scripts \u00b6 Please go here for details on all pingcommon hook scripts This document is auto-generated from pingcommon/Dockerfile Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"PingCommon"},{"location":"docker-images/pingcommon/#ping-identity-docker-image-pingcommon","text":"This docker image provides a busybox image to house the base hook scripts and default entrypoint.sh used throughout the Ping Identity DevOps product images.","title":"Ping Identity Docker Image - pingcommon"},{"location":"docker-images/pingcommon/#docker-container-hook-scripts","text":"Please go here for details on all pingcommon hook scripts This document is auto-generated from pingcommon/Dockerfile Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Docker Container Hook Scripts"},{"location":"docker-images/pingcommon/hooks/","text":"Ping Identity DevOps pingcommon Hooks \u00b6 List of available hooks: * 01-start-server.sh * 02-get-remote-server-profile.sh * 03-build-run-plan.sh * 04-check-variables.sh * 05-expand-templates.sh * 06-copy-product-bits.sh * 07-apply-server-profile.sh * 09-build-motd.sh * 10-start-sequence.sh * 17-check-license.sh * 18-setup-sequence.sh * 20-restart-sequence.sh * 50-before-post-start.sh * 80-post-start.sh * 90-shutdown-sequence.sh * pingcommon.lib.sh * pingsecrets.lib.sh * pingstate.lib.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from pingcommon/opt/staging/hooks Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hooks"},{"location":"docker-images/pingcommon/hooks/#ping-identity-devops-pingcommon-hooks","text":"List of available hooks: * 01-start-server.sh * 02-get-remote-server-profile.sh * 03-build-run-plan.sh * 04-check-variables.sh * 05-expand-templates.sh * 06-copy-product-bits.sh * 07-apply-server-profile.sh * 09-build-motd.sh * 10-start-sequence.sh * 17-check-license.sh * 18-setup-sequence.sh * 20-restart-sequence.sh * 50-before-post-start.sh * 80-post-start.sh * 90-shutdown-sequence.sh * pingcommon.lib.sh * pingsecrets.lib.sh * pingstate.lib.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from pingcommon/opt/staging/hooks Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hooks"},{"location":"docker-images/pingcommon/hooks/01-start-server.sh/","text":"Ping Identity DevOps pingcommon Hook - 01-start-server.sh \u00b6 This document is auto-generated from pingcommon/opt/staging/hooks/01-start-server.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `01-start-server.sh`"},{"location":"docker-images/pingcommon/hooks/01-start-server.sh/#ping-identity-devops-pingcommon-hook-01-start-serversh","text":"This document is auto-generated from pingcommon/opt/staging/hooks/01-start-server.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - 01-start-server.sh"},{"location":"docker-images/pingcommon/hooks/02-get-remote-server-profile.sh/","text":"Ping Identity DevOps pingcommon Hook - 02-get-remote-server-profile.sh \u00b6 This hook will get bits from a git repo based on SERVER_PROFILE_* variables passed to the container. If no SERVER_PROFILES are passed, then nothing will occur when running this hook. These bits will be placed into the STAGING_DIR location (defaults to ${BASE_DIR}/staging). Server Profiles may be layered to copy in profiles from a parent/ancestor server profile. An example might be a layer of profiles that look like: - Dev Environment Configs (DEV_CONFIG) - Dev Certificates (DEV_CERT) - Base Configs (BASE) This would result in a set of SERVER_PROFILE variables that looks like: - SERVER_PROFILE_URL=...git url of DEV_CONFIG... - SERVER_PROFILE_PARENT=DEV_CERT - SERVER_PROFILE_DEV_CERT_URL=...git url of DEV_CERT... - SERVER_PROFILE_DEV_CERT_PARENT=BASE - SERVER_PROFILE_BASE_URL=...git url of BASE... In this example, the bits for BASE would be pulled, followed by DEV_CERT, followed by DEV_CONFIG If other source maintenance repositories are used (i.e. bitbucket, s3, ...) then this hook could be overridden by a different hook This document is auto-generated from pingcommon/opt/staging/hooks/02-get-remote-server-profile.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `02-get-remote-server-profile.sh`"},{"location":"docker-images/pingcommon/hooks/02-get-remote-server-profile.sh/#ping-identity-devops-pingcommon-hook-02-get-remote-server-profilesh","text":"This hook will get bits from a git repo based on SERVER_PROFILE_* variables passed to the container. If no SERVER_PROFILES are passed, then nothing will occur when running this hook. These bits will be placed into the STAGING_DIR location (defaults to ${BASE_DIR}/staging). Server Profiles may be layered to copy in profiles from a parent/ancestor server profile. An example might be a layer of profiles that look like: - Dev Environment Configs (DEV_CONFIG) - Dev Certificates (DEV_CERT) - Base Configs (BASE) This would result in a set of SERVER_PROFILE variables that looks like: - SERVER_PROFILE_URL=...git url of DEV_CONFIG... - SERVER_PROFILE_PARENT=DEV_CERT - SERVER_PROFILE_DEV_CERT_URL=...git url of DEV_CERT... - SERVER_PROFILE_DEV_CERT_PARENT=BASE - SERVER_PROFILE_BASE_URL=...git url of BASE... In this example, the bits for BASE would be pulled, followed by DEV_CERT, followed by DEV_CONFIG If other source maintenance repositories are used (i.e. bitbucket, s3, ...) then this hook could be overridden by a different hook This document is auto-generated from pingcommon/opt/staging/hooks/02-get-remote-server-profile.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - 02-get-remote-server-profile.sh"},{"location":"docker-images/pingcommon/hooks/03-build-run-plan.sh/","text":"Ping Identity DevOps pingcommon Hook - 03-build-run-plan.sh \u00b6 This script will building a run plan for the server as it starts up Options for the RUN_PLAN and the PD_STATE are as follows: RUN_PLAN (Initially set to UNKNOWN) START - Instructs the container to start from scratch. This is primarily because a SERVER_ROOT_DIR (i.e. /opt/out/instance) isn't preseent. RESTART - Instructs the container to restart. This is primarily because the SERVER_ROOT_DIR (i.e. /opt/out/instance) is prsent. NOTE: It will be common for products to override this hook to provide RUN_PLAN directions based on product specifics. This document is auto-generated from pingcommon/opt/staging/hooks/03-build-run-plan.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `03-build-run-plan.sh`"},{"location":"docker-images/pingcommon/hooks/03-build-run-plan.sh/#ping-identity-devops-pingcommon-hook-03-build-run-plansh","text":"This script will building a run plan for the server as it starts up Options for the RUN_PLAN and the PD_STATE are as follows: RUN_PLAN (Initially set to UNKNOWN) START - Instructs the container to start from scratch. This is primarily because a SERVER_ROOT_DIR (i.e. /opt/out/instance) isn't preseent. RESTART - Instructs the container to restart. This is primarily because the SERVER_ROOT_DIR (i.e. /opt/out/instance) is prsent. NOTE: It will be common for products to override this hook to provide RUN_PLAN directions based on product specifics. This document is auto-generated from pingcommon/opt/staging/hooks/03-build-run-plan.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - 03-build-run-plan.sh"},{"location":"docker-images/pingcommon/hooks/04-check-variables.sh/","text":"Ping Identity DevOps pingcommon Hook - 04-check-variables.sh \u00b6 This document is auto-generated from pingcommon/opt/staging/hooks/04-check-variables.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `04-check-variables.sh`"},{"location":"docker-images/pingcommon/hooks/04-check-variables.sh/#ping-identity-devops-pingcommon-hook-04-check-variablessh","text":"This document is auto-generated from pingcommon/opt/staging/hooks/04-check-variables.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - 04-check-variables.sh"},{"location":"docker-images/pingcommon/hooks/05-expand-templates.sh/","text":"Ping Identity DevOps pingcommon Hook - 05-expand-templates.sh \u00b6 Using the envsubst command, this will look through any files in the STAGING_DIR that end in .subst or .subst.default and substitute any variables the files with the the value of those variables, if the variable is set. Variables may come from (in order of precedence): - The '.env' file from the profiles and intra container env variables - The environment variables or env-file passed to continaer on startup - The container's os If a .zip file ends with .zip.subst (especially useful for pingfederate for example with data.zip) then: - file will be unzipped - any files ending in .subst will be processed to substiture variables - zipped back up in to the same file without the .subst suffix If a file ends with .subst.default (intended to only be expanded as a default if the file is not found) then it will be substituted: - If the RUN_PLAN==START and the file is not found in staging - If the RUN_PLAN==RESTART and the file is found in staging or the OUT_DIR Note: If a string of $name is sould be ignored during a substitution, then A special vabiable ${ DOLLAR } should be used. This is not required any longer and deprecated, but available for any older server-profile versions. Example: ${ DOLLAR }{username} ==> ${username} This document is auto-generated from pingcommon/opt/staging/hooks/05-expand-templates.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `05-expand-templates.sh`"},{"location":"docker-images/pingcommon/hooks/05-expand-templates.sh/#ping-identity-devops-pingcommon-hook-05-expand-templatessh","text":"Using the envsubst command, this will look through any files in the STAGING_DIR that end in .subst or .subst.default and substitute any variables the files with the the value of those variables, if the variable is set. Variables may come from (in order of precedence): - The '.env' file from the profiles and intra container env variables - The environment variables or env-file passed to continaer on startup - The container's os If a .zip file ends with .zip.subst (especially useful for pingfederate for example with data.zip) then: - file will be unzipped - any files ending in .subst will be processed to substiture variables - zipped back up in to the same file without the .subst suffix If a file ends with .subst.default (intended to only be expanded as a default if the file is not found) then it will be substituted: - If the RUN_PLAN==START and the file is not found in staging - If the RUN_PLAN==RESTART and the file is found in staging or the OUT_DIR Note: If a string of $name is sould be ignored during a substitution, then A special vabiable ${ DOLLAR } should be used. This is not required any longer and deprecated, but available for any older server-profile versions. Example: ${ DOLLAR }{username} ==> ${username} This document is auto-generated from pingcommon/opt/staging/hooks/05-expand-templates.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - 05-expand-templates.sh"},{"location":"docker-images/pingcommon/hooks/06-copy-product-bits.sh/","text":"Ping Identity DevOps pingcommon Hook - 06-copy-product-bits.sh \u00b6 Copies the server bits from the image into the SERVER_ROOT_DIR if it is a new fresh container. This document is auto-generated from pingcommon/opt/staging/hooks/06-copy-product-bits.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `06-copy-product-bits.sh`"},{"location":"docker-images/pingcommon/hooks/06-copy-product-bits.sh/#ping-identity-devops-pingcommon-hook-06-copy-product-bitssh","text":"Copies the server bits from the image into the SERVER_ROOT_DIR if it is a new fresh container. This document is auto-generated from pingcommon/opt/staging/hooks/06-copy-product-bits.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - 06-copy-product-bits.sh"},{"location":"docker-images/pingcommon/hooks/07-apply-server-profile.sh/","text":"Ping Identity DevOps pingcommon Hook - 07-apply-server-profile.sh \u00b6 The server-profiles from: * remote (i.e. git) and * local (i.e. /opt/in) have been merged into the ${STAGING_DIR}/instance (ie. /opt/staging/instance). This is a candidate to be installed or overwritten into the ${SERVER_ROOT_DIR} if one of the following items are true: * Start of a new server (i.e. RUN_PLAN=START) * Restart of a server with SERVER_PROFILE_UPDATE==true To force the overwrite of files on a restart, ensure that the variable: SERVER_PROFILE_UPDATE=true is passed. This document is auto-generated from pingcommon/opt/staging/hooks/07-apply-server-profile.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `07-apply-server-profile.sh`"},{"location":"docker-images/pingcommon/hooks/07-apply-server-profile.sh/#ping-identity-devops-pingcommon-hook-07-apply-server-profilesh","text":"The server-profiles from: * remote (i.e. git) and * local (i.e. /opt/in) have been merged into the ${STAGING_DIR}/instance (ie. /opt/staging/instance). This is a candidate to be installed or overwritten into the ${SERVER_ROOT_DIR} if one of the following items are true: * Start of a new server (i.e. RUN_PLAN=START) * Restart of a server with SERVER_PROFILE_UPDATE==true To force the overwrite of files on a restart, ensure that the variable: SERVER_PROFILE_UPDATE=true is passed. This document is auto-generated from pingcommon/opt/staging/hooks/07-apply-server-profile.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - 07-apply-server-profile.sh"},{"location":"docker-images/pingcommon/hooks/08-get-secrets.sh/","text":"Ping Identity DevOps pingcommon Hook - 08-get-secrets.sh \u00b6 Gets secrets from secret management solution * Hashicorp Vault This document auto-generated from pingcommon/opt/staging/hooks/08-get-secrets.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `08-get-secrets.sh`"},{"location":"docker-images/pingcommon/hooks/08-get-secrets.sh/#ping-identity-devops-pingcommon-hook-08-get-secretssh","text":"Gets secrets from secret management solution * Hashicorp Vault This document auto-generated from pingcommon/opt/staging/hooks/08-get-secrets.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - 08-get-secrets.sh"},{"location":"docker-images/pingcommon/hooks/09-build-motd.sh/","text":"Ping Identity DevOps pingcommon Hook - 09-build-motd.sh \u00b6 Creates a message of the day (MOTD) file based on information prvoded by: * Docker Varibbles * Github MOTD file from PingIdentity Devops Repo * Server-Profile motd file This document is auto-generated from pingcommon/opt/staging/hooks/09-build-motd.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `09-build-motd.sh`"},{"location":"docker-images/pingcommon/hooks/09-build-motd.sh/#ping-identity-devops-pingcommon-hook-09-build-motdsh","text":"Creates a message of the day (MOTD) file based on information prvoded by: * Docker Varibbles * Github MOTD file from PingIdentity Devops Repo * Server-Profile motd file This document is auto-generated from pingcommon/opt/staging/hooks/09-build-motd.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - 09-build-motd.sh"},{"location":"docker-images/pingcommon/hooks/10-start-sequence.sh/","text":"Ping Identity DevOps pingcommon Hook - 10-start-sequence.sh \u00b6 This document is auto-generated from pingcommon/opt/staging/hooks/10-start-sequence.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `10-start-sequence.sh`"},{"location":"docker-images/pingcommon/hooks/10-start-sequence.sh/#ping-identity-devops-pingcommon-hook-10-start-sequencesh","text":"This document is auto-generated from pingcommon/opt/staging/hooks/10-start-sequence.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - 10-start-sequence.sh"},{"location":"docker-images/pingcommon/hooks/11-before-copying-bits.sh/","text":"Ping Identity DevOps pingcommon Hook - 11-before-copying-bits.sh \u00b6 This script may be implemented to do some house keeping on the product bits before they are copied over to the runtime location This document auto-generated from pingcommon/hooks/11-before-copying-bits.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `11-before-copying-bits.sh`"},{"location":"docker-images/pingcommon/hooks/11-before-copying-bits.sh/#ping-identity-devops-pingcommon-hook-11-before-copying-bitssh","text":"This script may be implemented to do some house keeping on the product bits before they are copied over to the runtime location This document auto-generated from pingcommon/hooks/11-before-copying-bits.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - 11-before-copying-bits.sh"},{"location":"docker-images/pingcommon/hooks/12-before-applying-server-profile.sh/","text":"Ping Identity DevOps pingcommon Hook - 12-before-applying-server-profile.sh \u00b6 This script is called after the product bits have been copied over to the runtime location and before the remote server profile gets applied on to the staging area if the remote server profile is to be layered over a local sever profile provided via the ${IN_DIR} volume mount, you could use this hook to manipulate the local server profile in the staging area to avoid certain file from being overridden for example This document auto-generated from pingcommon/hooks/12-before-applying-server-profile.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `12-before-applying-server-profile.sh`"},{"location":"docker-images/pingcommon/hooks/12-before-applying-server-profile.sh/#ping-identity-devops-pingcommon-hook-12-before-applying-server-profilesh","text":"This script is called after the product bits have been copied over to the runtime location and before the remote server profile gets applied on to the staging area if the remote server profile is to be layered over a local sever profile provided via the ${IN_DIR} volume mount, you could use this hook to manipulate the local server profile in the staging area to avoid certain file from being overridden for example This document auto-generated from pingcommon/hooks/12-before-applying-server-profile.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - 12-before-applying-server-profile.sh"},{"location":"docker-images/pingcommon/hooks/14-get-remote-server-profile.sh/","text":"Ping Identity DevOps pingcommon Hook - 14-get-remote-server-profile.sh \u00b6 This hook will get bits from a git repo based on SERVER_PROFILE_* variables passed to the container. If no SERVER_PROFILES are passed, then nothing will occur when running this hook. These bits will be placed into the STAGING_DIR location (defaults to ${BASE_DIR}/staging). Server Profiles may be layered to copy in profils from a parent/ancestor server profile. An example might be a layer of profiles that look like: Dev Environment Configs (DEV_CONFIG) Dev Certificates (DEV_CERT) Base Configs (BASE) This would result in a set of SERVER_PROFILE variables that looks like: - SERVER_PROFILE_URL=...git url of DEV_CONFIG... - SERVER_PROFILE_PARENT=DEV_CERT - SERVER_PROFILE_DEV_CERT_URL=...git url of DEV_CERT... - SERVER_PROFILE_DEV_CERT_PARENT=BASE - SERVER_PROFILE_BASE_URL=...git url of BASE... In this example, the bits for BASE would be pulled, followed by DEV_CERT, followed by DEV_CONFIG If other source maintenance repositories are used (i.e. bitbucket, s3, ...) then this hook could be overridden by a different hook This document auto-generated from pingcommon/hooks/14-get-remote-server-profile.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `14-get-remote-server-profile.sh`"},{"location":"docker-images/pingcommon/hooks/14-get-remote-server-profile.sh/#ping-identity-devops-pingcommon-hook-14-get-remote-server-profilesh","text":"This hook will get bits from a git repo based on SERVER_PROFILE_* variables passed to the container. If no SERVER_PROFILES are passed, then nothing will occur when running this hook. These bits will be placed into the STAGING_DIR location (defaults to ${BASE_DIR}/staging). Server Profiles may be layered to copy in profils from a parent/ancestor server profile. An example might be a layer of profiles that look like: Dev Environment Configs (DEV_CONFIG) Dev Certificates (DEV_CERT) Base Configs (BASE) This would result in a set of SERVER_PROFILE variables that looks like: - SERVER_PROFILE_URL=...git url of DEV_CONFIG... - SERVER_PROFILE_PARENT=DEV_CERT - SERVER_PROFILE_DEV_CERT_URL=...git url of DEV_CERT... - SERVER_PROFILE_DEV_CERT_PARENT=BASE - SERVER_PROFILE_BASE_URL=...git url of BASE... In this example, the bits for BASE would be pulled, followed by DEV_CERT, followed by DEV_CONFIG If other source maintenance repositories are used (i.e. bitbucket, s3, ...) then this hook could be overridden by a different hook This document auto-generated from pingcommon/hooks/14-get-remote-server-profile.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - 14-get-remote-server-profile.sh"},{"location":"docker-images/pingcommon/hooks/15-expand-templates.sh/","text":"Ping Identity DevOps pingcommon Hook - 15-expand-templates.sh \u00b6 Using the envsubst command, this will look through any files that end in subst and substitute any variables the files with the the value of those variables. Variables may come from (in order of precedence): - The 'env_vars' file from the profiles - The environment variables or env-file passed to continaer on startup - The container's os Note: If a string of $name is sould be ignored during a substitution, then A special vabiable ${ DOLLAR } should be used. Example: ${ DOLLAR }{username} ==> ${username} If a .zip file ends with .zip.subst then: - file will be unzipped - any files ending in .subst will be processed to substiture variables - zipped back up in to the same file without the .subst suffix This is especially useful for pingfederate for example with data.zip This document auto-generated from pingcommon/hooks/15-expand-templates.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `15-expand-templates.sh`"},{"location":"docker-images/pingcommon/hooks/15-expand-templates.sh/#ping-identity-devops-pingcommon-hook-15-expand-templatessh","text":"Using the envsubst command, this will look through any files that end in subst and substitute any variables the files with the the value of those variables. Variables may come from (in order of precedence): - The 'env_vars' file from the profiles - The environment variables or env-file passed to continaer on startup - The container's os Note: If a string of $name is sould be ignored during a substitution, then A special vabiable ${ DOLLAR } should be used. Example: ${ DOLLAR }{username} ==> ${username} If a .zip file ends with .zip.subst then: - file will be unzipped - any files ending in .subst will be processed to substiture variables - zipped back up in to the same file without the .subst suffix This is especially useful for pingfederate for example with data.zip This document auto-generated from pingcommon/hooks/15-expand-templates.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - 15-expand-templates.sh"},{"location":"docker-images/pingcommon/hooks/16-apply-server-profile.sh/","text":"Ping Identity DevOps pingcommon Hook - 16-apply-server-profile.sh \u00b6 Once both the remote (i.e. git) and local server-profiles have been merged then we can push that out to the instance. This will override any files found in the ${OUT_DIR}/instance directory. This document auto-generated from pingcommon/hooks/16-apply-server-profile.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `16-apply-server-profile.sh`"},{"location":"docker-images/pingcommon/hooks/16-apply-server-profile.sh/#ping-identity-devops-pingcommon-hook-16-apply-server-profilesh","text":"Once both the remote (i.e. git) and local server-profiles have been merged then we can push that out to the instance. This will override any files found in the ${OUT_DIR}/instance directory. This document auto-generated from pingcommon/hooks/16-apply-server-profile.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - 16-apply-server-profile.sh"},{"location":"docker-images/pingcommon/hooks/17-check-license.sh/","text":"Ping Identity DevOps pingcommon Hook - 17-check-license.sh \u00b6 Check for license file - If LICENSE_FILE found make call to check-license api unless MUTE_LICENSE_VERIFICATION set to true - If LICENSE_FILE not found and PING_IDENTITY_DEVOPS_USER and PING_IDENTITY_DEVOPS_KEY defined make call to obtain a license from license server This document is auto-generated from pingcommon/opt/staging/hooks/17-check-license.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `17-check-license.sh`"},{"location":"docker-images/pingcommon/hooks/17-check-license.sh/#ping-identity-devops-pingcommon-hook-17-check-licensesh","text":"Check for license file - If LICENSE_FILE found make call to check-license api unless MUTE_LICENSE_VERIFICATION set to true - If LICENSE_FILE not found and PING_IDENTITY_DEVOPS_USER and PING_IDENTITY_DEVOPS_KEY defined make call to obtain a license from license server This document is auto-generated from pingcommon/opt/staging/hooks/17-check-license.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - 17-check-license.sh"},{"location":"docker-images/pingcommon/hooks/18-setup-sequence.sh/","text":"Ping Identity DevOps pingcommon Hook - 18-setup-sequence.sh \u00b6 This hook may be used to set the server if there is a setup procedure Note: The PingData (i.e. Directory, DataSync, DataGovernance, DirectoryProxy) products will all provide this This document is auto-generated from pingcommon/opt/staging/hooks/18-setup-sequence.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `18-setup-sequence.sh`"},{"location":"docker-images/pingcommon/hooks/18-setup-sequence.sh/#ping-identity-devops-pingcommon-hook-18-setup-sequencesh","text":"This hook may be used to set the server if there is a setup procedure Note: The PingData (i.e. Directory, DataSync, DataGovernance, DirectoryProxy) products will all provide this This document is auto-generated from pingcommon/opt/staging/hooks/18-setup-sequence.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - 18-setup-sequence.sh"},{"location":"docker-images/pingcommon/hooks/20-restart-sequence.sh/","text":"Ping Identity DevOps pingcommon Hook - 20-restart-sequence.sh \u00b6 This hook is called when the container has been built in a prior startup and a configuration has been found. This document is auto-generated from pingcommon/opt/staging/hooks/20-restart-sequence.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `20-restart-sequence.sh`"},{"location":"docker-images/pingcommon/hooks/20-restart-sequence.sh/#ping-identity-devops-pingcommon-hook-20-restart-sequencesh","text":"This hook is called when the container has been built in a prior startup and a configuration has been found. This document is auto-generated from pingcommon/opt/staging/hooks/20-restart-sequence.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - 20-restart-sequence.sh"},{"location":"docker-images/pingcommon/hooks/21-update-server-profile.sh/","text":"Ping Identity DevOps pingcommon Hook - 21-update-server-profile.sh \u00b6 This document auto-generated from pingcommon/opt/staging/hooks/21-update-server-profile.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `21-update-server-profile.sh`"},{"location":"docker-images/pingcommon/hooks/21-update-server-profile.sh/#ping-identity-devops-pingcommon-hook-21-update-server-profilesh","text":"This document auto-generated from pingcommon/opt/staging/hooks/21-update-server-profile.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - 21-update-server-profile.sh"},{"location":"docker-images/pingcommon/hooks/50-before-post-start.sh/","text":"Ping Identity DevOps pingcommon Hook - 50-before-post-start.sh \u00b6 This is called after the start or restart sequence has finished and before the server within the container starts This document is auto-generated from pingcommon/opt/staging/hooks/50-before-post-start.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `50-before-post-start.sh`"},{"location":"docker-images/pingcommon/hooks/50-before-post-start.sh/#ping-identity-devops-pingcommon-hook-50-before-post-startsh","text":"This is called after the start or restart sequence has finished and before the server within the container starts This document is auto-generated from pingcommon/opt/staging/hooks/50-before-post-start.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - 50-before-post-start.sh"},{"location":"docker-images/pingcommon/hooks/80-post-start.sh/","text":"Ping Identity DevOps pingcommon Hook - 80-post-start.sh \u00b6 This script is started in the background immediately before the server within the container is started This is useful to implement any logic that needs to occur after the server is up and running For example, enabling replication in PingDirectory, initializing Sync Pipes in PingDataSync or issuing admin API calls to PingFederate or PingAccess This document is auto-generated from pingcommon/opt/staging/hooks/80-post-start.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `80-post-start.sh`"},{"location":"docker-images/pingcommon/hooks/80-post-start.sh/#ping-identity-devops-pingcommon-hook-80-post-startsh","text":"This script is started in the background immediately before the server within the container is started This is useful to implement any logic that needs to occur after the server is up and running For example, enabling replication in PingDirectory, initializing Sync Pipes in PingDataSync or issuing admin API calls to PingFederate or PingAccess This document is auto-generated from pingcommon/opt/staging/hooks/80-post-start.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - 80-post-start.sh"},{"location":"docker-images/pingcommon/hooks/90-shutdown-sequence.sh/","text":"Ping Identity DevOps pingcommon Hook - 90-shutdown-sequence.sh \u00b6 This script may be implemented to gracefully shutdown the container Note: this is most useful in Kubernetes but can be called arbitrarily by by control/config frameworks This document is auto-generated from pingcommon/opt/staging/hooks/90-shutdown-sequence.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `90-shutdown-sequence.sh`"},{"location":"docker-images/pingcommon/hooks/90-shutdown-sequence.sh/#ping-identity-devops-pingcommon-hook-90-shutdown-sequencesh","text":"This script may be implemented to gracefully shutdown the container Note: this is most useful in Kubernetes but can be called arbitrarily by by control/config frameworks This document is auto-generated from pingcommon/opt/staging/hooks/90-shutdown-sequence.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - 90-shutdown-sequence.sh"},{"location":"docker-images/pingcommon/hooks/pingcommon.lib.sh/","text":"Ping Identity DevOps pingcommon Hook - pingcommon.lib.sh \u00b6 This document is auto-generated from pingcommon/opt/staging/hooks/pingcommon.lib.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `pingcommon.lib.sh`"},{"location":"docker-images/pingcommon/hooks/pingcommon.lib.sh/#ping-identity-devops-pingcommon-hook-pingcommonlibsh","text":"This document is auto-generated from pingcommon/opt/staging/hooks/pingcommon.lib.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - pingcommon.lib.sh"},{"location":"docker-images/pingcommon/hooks/pingsecrets.lib.sh/","text":"Ping Identity DevOps pingcommon Hook - pingsecrets.lib.sh \u00b6 This document is auto-generated from pingcommon/opt/staging/hooks/pingsecrets.lib.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `pingsecrets.lib.sh`"},{"location":"docker-images/pingcommon/hooks/pingsecrets.lib.sh/#ping-identity-devops-pingcommon-hook-pingsecretslibsh","text":"This document is auto-generated from pingcommon/opt/staging/hooks/pingsecrets.lib.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - pingsecrets.lib.sh"},{"location":"docker-images/pingcommon/hooks/pingstate.lib.sh/","text":"Ping Identity DevOps pingcommon Hook - pingstate.lib.sh \u00b6 This document is auto-generated from pingcommon/opt/staging/hooks/pingstate.lib.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingcommon` Hook - `pingstate.lib.sh`"},{"location":"docker-images/pingcommon/hooks/pingstate.lib.sh/#ping-identity-devops-pingcommon-hook-pingstatelibsh","text":"This document is auto-generated from pingcommon/opt/staging/hooks/pingstate.lib.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingcommon Hook - pingstate.lib.sh"},{"location":"docker-images/pingdatacommon/","text":"Ping Identity Docker Image - pingdatacommon \u00b6 This docker image provides a busybox image based off of pingidentity/pingcommon to house the base hook scripts used throughout the Ping Identity DevOps PingData product images. Related Docker Images \u00b6 pingidentity/pingcommon - Parent Image Environment Variables \u00b6 The following environment ENV variables can be used with this image. ENV Variable Default Description PD_PROFILE ${STAGING_DIR}/pd.profile ## Docker Container Hook Scripts Please go here for details on all pingdatacommon hook scripts This document is auto-generated from pingdatacommon/Dockerfile Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"PingDataCommon"},{"location":"docker-images/pingdatacommon/#ping-identity-docker-image-pingdatacommon","text":"This docker image provides a busybox image based off of pingidentity/pingcommon to house the base hook scripts used throughout the Ping Identity DevOps PingData product images.","title":"Ping Identity Docker Image - pingdatacommon"},{"location":"docker-images/pingdatacommon/#related-docker-images","text":"pingidentity/pingcommon - Parent Image","title":"Related Docker Images"},{"location":"docker-images/pingdatacommon/#environment-variables","text":"The following environment ENV variables can be used with this image. ENV Variable Default Description PD_PROFILE ${STAGING_DIR}/pd.profile ## Docker Container Hook Scripts Please go here for details on all pingdatacommon hook scripts This document is auto-generated from pingdatacommon/Dockerfile Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Environment Variables"},{"location":"docker-images/pingdatacommon/hooks/","text":"Ping Identity DevOps pingdatacommon Hooks \u00b6 List of available hooks: * 03-build-run-plan.sh * 18-setup-sequence.sh * 181-install-extensions.sh * 183-run-setup.sh * 185-apply-tools-properties.sh * 20-restart-sequence.sh * pingdata.lib.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from pingdatacommon/opt/staging/hooks Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatacommon` Hooks"},{"location":"docker-images/pingdatacommon/hooks/#ping-identity-devops-pingdatacommon-hooks","text":"List of available hooks: * 03-build-run-plan.sh * 18-setup-sequence.sh * 181-install-extensions.sh * 183-run-setup.sh * 185-apply-tools-properties.sh * 20-restart-sequence.sh * pingdata.lib.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from pingdatacommon/opt/staging/hooks Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatacommon Hooks"},{"location":"docker-images/pingdatacommon/hooks/03-build-run-plan.sh/","text":"Ping Identity DevOps pingdatacommon Hook - 03-build-run-plan.sh \u00b6 This scrip is called to check if there is an existing server and if so, it will return a 1, else 0 This document is auto-generated from pingdatacommon/opt/staging/hooks/03-build-run-plan.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatacommon` Hook - `03-build-run-plan.sh`"},{"location":"docker-images/pingdatacommon/hooks/03-build-run-plan.sh/#ping-identity-devops-pingdatacommon-hook-03-build-run-plansh","text":"This scrip is called to check if there is an existing server and if so, it will return a 1, else 0 This document is auto-generated from pingdatacommon/opt/staging/hooks/03-build-run-plan.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatacommon Hook - 03-build-run-plan.sh"},{"location":"docker-images/pingdatacommon/hooks/18-setup-sequence.sh/","text":"Ping Identity DevOps pingdatacommon Hook - 18-setup-sequence.sh \u00b6 Quarterbacks all the scripts associated with the setup of a PingData product This document is auto-generated from pingdatacommon/opt/staging/hooks/18-setup-sequence.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatacommon` Hook - `18-setup-sequence.sh`"},{"location":"docker-images/pingdatacommon/hooks/18-setup-sequence.sh/#ping-identity-devops-pingdatacommon-hook-18-setup-sequencesh","text":"Quarterbacks all the scripts associated with the setup of a PingData product This document is auto-generated from pingdatacommon/opt/staging/hooks/18-setup-sequence.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatacommon Hook - 18-setup-sequence.sh"},{"location":"docker-images/pingdatacommon/hooks/181-install-extensions.sh/","text":"Ping Identity DevOps pingdatacommon Hook - 181-install-extensions.sh \u00b6 This document is auto-generated from pingdatacommon/opt/staging/hooks/181-install-extensions.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatacommon` Hook - `181-install-extensions.sh`"},{"location":"docker-images/pingdatacommon/hooks/181-install-extensions.sh/#ping-identity-devops-pingdatacommon-hook-181-install-extensionssh","text":"This document is auto-generated from pingdatacommon/opt/staging/hooks/181-install-extensions.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatacommon Hook - 181-install-extensions.sh"},{"location":"docker-images/pingdatacommon/hooks/182-template-to-ldif.sh/","text":"Ping Identity DevOps pingdatacommon Hook - 182-template-to-ldif.sh \u00b6 This hook will import data into the PingDirectory if there are data files included in the server profile data directory. If a .template file is provided, then makeldif will be run to create the .ldif file to be imported. To be implemented by the downstream product (i.e. pingdirectory) This document auto-generated from pingdatacommon/hooks/182-template-to-ldif.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatacommon` Hook - `182-template-to-ldif.sh`"},{"location":"docker-images/pingdatacommon/hooks/182-template-to-ldif.sh/#ping-identity-devops-pingdatacommon-hook-182-template-to-ldifsh","text":"This hook will import data into the PingDirectory if there are data files included in the server profile data directory. If a .template file is provided, then makeldif will be run to create the .ldif file to be imported. To be implemented by the downstream product (i.e. pingdirectory) This document auto-generated from pingdatacommon/hooks/182-template-to-ldif.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatacommon Hook - 182-template-to-ldif.sh"},{"location":"docker-images/pingdatacommon/hooks/183-run-setup.sh/","text":"Ping Identity DevOps pingdatacommon Hook - 183-run-setup.sh \u00b6 This document is auto-generated from pingdatacommon/opt/staging/hooks/183-run-setup.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatacommon` Hook - `183-run-setup.sh`"},{"location":"docker-images/pingdatacommon/hooks/183-run-setup.sh/#ping-identity-devops-pingdatacommon-hook-183-run-setupsh","text":"This document is auto-generated from pingdatacommon/opt/staging/hooks/183-run-setup.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatacommon Hook - 183-run-setup.sh"},{"location":"docker-images/pingdatacommon/hooks/184-generate-topology-descriptor.sh/","text":"Ping Identity DevOps pingdatacommon Hook - 184-generate-topology-descriptor.sh \u00b6 This document auto-generated from pingdatacommon/hooks/184-generate-topology-descriptor.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatacommon` Hook - `184-generate-topology-descriptor.sh`"},{"location":"docker-images/pingdatacommon/hooks/184-generate-topology-descriptor.sh/#ping-identity-devops-pingdatacommon-hook-184-generate-topology-descriptorsh","text":"This document auto-generated from pingdatacommon/hooks/184-generate-topology-descriptor.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatacommon Hook - 184-generate-topology-descriptor.sh"},{"location":"docker-images/pingdatacommon/hooks/185-apply-tools-properties.sh/","text":"Ping Identity DevOps pingdatacommon Hook - 185-apply-tools-properties.sh \u00b6 This document is auto-generated from pingdatacommon/opt/staging/hooks/185-apply-tools-properties.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatacommon` Hook - `185-apply-tools-properties.sh`"},{"location":"docker-images/pingdatacommon/hooks/185-apply-tools-properties.sh/#ping-identity-devops-pingdatacommon-hook-185-apply-tools-propertiessh","text":"This document is auto-generated from pingdatacommon/opt/staging/hooks/185-apply-tools-properties.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatacommon Hook - 185-apply-tools-properties.sh"},{"location":"docker-images/pingdatacommon/hooks/186-install-extensions.sh/","text":"Ping Identity DevOps pingdatacommon Hook - 186-install-extensions.sh \u00b6 This document auto-generated from pingdatacommon/hooks/186-install-extensions.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatacommon` Hook - `186-install-extensions.sh`"},{"location":"docker-images/pingdatacommon/hooks/186-install-extensions.sh/#ping-identity-devops-pingdatacommon-hook-186-install-extensionssh","text":"This document auto-generated from pingdatacommon/hooks/186-install-extensions.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatacommon Hook - 186-install-extensions.sh"},{"location":"docker-images/pingdatacommon/hooks/187-before-configuration.sh/","text":"Ping Identity DevOps pingdatacommon Hook - 187-before-configuration.sh \u00b6 This document auto-generated from pingdatacommon/hooks/187-before-configuration.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatacommon` Hook - `187-before-configuration.sh`"},{"location":"docker-images/pingdatacommon/hooks/187-before-configuration.sh/#ping-identity-devops-pingdatacommon-hook-187-before-configurationsh","text":"This document auto-generated from pingdatacommon/hooks/187-before-configuration.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatacommon Hook - 187-before-configuration.sh"},{"location":"docker-images/pingdatacommon/hooks/188-apply-configuration.sh/","text":"Ping Identity DevOps pingdatacommon Hook - 188-apply-configuration.sh \u00b6 This document auto-generated from pingdatacommon/hooks/188-apply-configuration.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatacommon` Hook - `188-apply-configuration.sh`"},{"location":"docker-images/pingdatacommon/hooks/188-apply-configuration.sh/#ping-identity-devops-pingdatacommon-hook-188-apply-configurationsh","text":"This document auto-generated from pingdatacommon/hooks/188-apply-configuration.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatacommon Hook - 188-apply-configuration.sh"},{"location":"docker-images/pingdatacommon/hooks/189-import-data.sh/","text":"Ping Identity DevOps pingdatacommon Hook - 189-import-data.sh \u00b6 This document auto-generated from pingdatacommon/hooks/189-import-data.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatacommon` Hook - `189-import-data.sh`"},{"location":"docker-images/pingdatacommon/hooks/189-import-data.sh/#ping-identity-devops-pingdatacommon-hook-189-import-datash","text":"This document auto-generated from pingdatacommon/hooks/189-import-data.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatacommon Hook - 189-import-data.sh"},{"location":"docker-images/pingdatacommon/hooks/20-restart-sequence.sh/","text":"Ping Identity DevOps pingdatacommon Hook - 20-restart-sequence.sh \u00b6 This hook is called when the container has been built in a prior startup and a configuration has been found. This document is auto-generated from pingdatacommon/opt/staging/hooks/20-restart-sequence.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatacommon` Hook - `20-restart-sequence.sh`"},{"location":"docker-images/pingdatacommon/hooks/20-restart-sequence.sh/#ping-identity-devops-pingdatacommon-hook-20-restart-sequencesh","text":"This hook is called when the container has been built in a prior startup and a configuration has been found. This document is auto-generated from pingdatacommon/opt/staging/hooks/20-restart-sequence.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatacommon Hook - 20-restart-sequence.sh"},{"location":"docker-images/pingdatacommon/hooks/21-update-server-profile.sh/","text":"Ping Identity DevOps pingdatacommon Hook - 21-update-server-profile.sh \u00b6 This document auto-generated from pingdatacommon/opt/staging/hooks/21-update-server-profile.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatacommon` Hook - `21-update-server-profile.sh`"},{"location":"docker-images/pingdatacommon/hooks/21-update-server-profile.sh/#ping-identity-devops-pingdatacommon-hook-21-update-server-profilesh","text":"This document auto-generated from pingdatacommon/opt/staging/hooks/21-update-server-profile.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatacommon Hook - 21-update-server-profile.sh"},{"location":"docker-images/pingdatacommon/hooks/pingdata.lib.sh/","text":"Ping Identity DevOps pingdatacommon Hook - pingdata.lib.sh \u00b6 This document is auto-generated from pingdatacommon/opt/staging/hooks/pingdata.lib.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatacommon` Hook - `pingdata.lib.sh`"},{"location":"docker-images/pingdatacommon/hooks/pingdata.lib.sh/#ping-identity-devops-pingdatacommon-hook-pingdatalibsh","text":"This document is auto-generated from pingdatacommon/opt/staging/hooks/pingdata.lib.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatacommon Hook - pingdata.lib.sh"},{"location":"docker-images/pingdataconsole/","text":"Ping Identity Docker Image - pingdataconsole \u00b6 This docker image provides a tomcat image with the PingDataConsole deployed to be used in configuration of the PingData products. Related Docker Images \u00b6 pingidentity/pingdownloader - Image used to download ping product tomcat:8-jre8-alpine - Tomcat engine to serve PingDataConsole .war file Environment Variables \u00b6 The following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} Flag to force a run of dsjavaproperties --initialize. When this is false, the java.properties file will only be regenerated on a restart when there is a change in JVM or a change in the product-specific java options, such as changing the MAX_HEAP_SIZE value. HTTP_PORT 8080 HTTPS_PORT 8443 STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/catalina.sh STARTUP_FOREGROUND_OPTS run STARTUP_BACKGROUND_OPTS start ## Run To run a PingDataConsole container: docker run \\ --name pingdataconsole \\ --publish ${ HTTPS_PORT } : ${ HTTPS_PORT } \\ --detach \\ --env PING_IDENTITY_ACCEPT_EULA = YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingdataconsole:edge Follow Docker logs with: docker logs -f pingdataconsole If using the command above with the embedded server profile , log in with: * http://localhost:${HTTPS_PORT}/console/login Server: pingdirectory:636 Username: administrator Password: 2FederateM0re make sure you have a PingDirectory running Docker Container Hook Scripts \u00b6 Please go here for details on all pingdataconsole hook scripts This document is auto-generated from pingdataconsole/Dockerfile Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"PingDataConsole"},{"location":"docker-images/pingdataconsole/#ping-identity-docker-image-pingdataconsole","text":"This docker image provides a tomcat image with the PingDataConsole deployed to be used in configuration of the PingData products.","title":"Ping Identity Docker Image - pingdataconsole"},{"location":"docker-images/pingdataconsole/#related-docker-images","text":"pingidentity/pingdownloader - Image used to download ping product tomcat:8-jre8-alpine - Tomcat engine to serve PingDataConsole .war file","title":"Related Docker Images"},{"location":"docker-images/pingdataconsole/#environment-variables","text":"The following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} Flag to force a run of dsjavaproperties --initialize. When this is false, the java.properties file will only be regenerated on a restart when there is a change in JVM or a change in the product-specific java options, such as changing the MAX_HEAP_SIZE value. HTTP_PORT 8080 HTTPS_PORT 8443 STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/catalina.sh STARTUP_FOREGROUND_OPTS run STARTUP_BACKGROUND_OPTS start ## Run To run a PingDataConsole container: docker run \\ --name pingdataconsole \\ --publish ${ HTTPS_PORT } : ${ HTTPS_PORT } \\ --detach \\ --env PING_IDENTITY_ACCEPT_EULA = YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingdataconsole:edge Follow Docker logs with: docker logs -f pingdataconsole If using the command above with the embedded server profile , log in with: * http://localhost:${HTTPS_PORT}/console/login Server: pingdirectory:636 Username: administrator Password: 2FederateM0re make sure you have a PingDirectory running","title":"Environment Variables"},{"location":"docker-images/pingdataconsole/#docker-container-hook-scripts","text":"Please go here for details on all pingdataconsole hook scripts This document is auto-generated from pingdataconsole/Dockerfile Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Docker Container Hook Scripts"},{"location":"docker-images/pingdataconsole/hooks/","text":"Ping Identity DevOps pingdataconsole Hooks \u00b6 List of available hooks: * 02-get-remote-server-profile.sh.post * 04-check-variables.sh * 17-check-license.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from pingdataconsole/opt/staging/hooks Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdataconsole` Hooks"},{"location":"docker-images/pingdataconsole/hooks/#ping-identity-devops-pingdataconsole-hooks","text":"List of available hooks: * 02-get-remote-server-profile.sh.post * 04-check-variables.sh * 17-check-license.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from pingdataconsole/opt/staging/hooks Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdataconsole Hooks"},{"location":"docker-images/pingdataconsole/hooks/02-get-remote-server-profile.sh.post/","text":"Ping Identity DevOps pingdataconsole Hook - 02-get-remote-server-profile.sh.post \u00b6 This hook provides final steps to setup Ping Data Console. This document is auto-generated from pingdataconsole/opt/staging/hooks/02-get-remote-server-profile.sh.post Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdataconsole` Hook - `02-get-remote-server-profile.sh.post`"},{"location":"docker-images/pingdataconsole/hooks/02-get-remote-server-profile.sh.post/#ping-identity-devops-pingdataconsole-hook-02-get-remote-server-profileshpost","text":"This hook provides final steps to setup Ping Data Console. This document is auto-generated from pingdataconsole/opt/staging/hooks/02-get-remote-server-profile.sh.post Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdataconsole Hook - 02-get-remote-server-profile.sh.post"},{"location":"docker-images/pingdataconsole/hooks/04-check-variables.sh/","text":"Ping Identity DevOps pingdataconsole Hook - 04-check-variables.sh \u00b6 This document is auto-generated from pingdataconsole/opt/staging/hooks/04-check-variables.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdataconsole` Hook - `04-check-variables.sh`"},{"location":"docker-images/pingdataconsole/hooks/04-check-variables.sh/#ping-identity-devops-pingdataconsole-hook-04-check-variablessh","text":"This document is auto-generated from pingdataconsole/opt/staging/hooks/04-check-variables.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdataconsole Hook - 04-check-variables.sh"},{"location":"docker-images/pingdataconsole/hooks/17-check-license.sh/","text":"Ping Identity DevOps pingdataconsole Hook - 17-check-license.sh \u00b6 This document is auto-generated from pingdataconsole/opt/staging/hooks/17-check-license.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdataconsole` Hook - `17-check-license.sh`"},{"location":"docker-images/pingdataconsole/hooks/17-check-license.sh/#ping-identity-devops-pingdataconsole-hook-17-check-licensesh","text":"This document is auto-generated from pingdataconsole/opt/staging/hooks/17-check-license.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdataconsole Hook - 17-check-license.sh"},{"location":"docker-images/pingdatagovernance/","text":"Ping Identity DevOps Docker Image - pingdatagovernance \u00b6 This docker image includes the Ping Identity PingDataGovernance product binaries and associated hook scripts to create and run a PingDataGovernance instance or instances. Related Docker Images \u00b6 pingidentity/pingbase - Parent Image > This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingdatacommon - Common Ping files (i.e. hook scripts) pingidentity/pingdownloader - Used to download product bits Environment Variables \u00b6 In addition to environment variables inherited from pingidentity/pingbase , the following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} PING_PRODUCT PingDataGovernance PingIdentity license version Ping product name LICENSE_FILE_NAME PingDataGovernance.lic Name of License File LICENSE_SHORT_NAME PG Short name used when retrieving license from License Server LICENSE_VERSION ${LICENSE_VERSION} Version used when retrieving license from License Server MAX_HEAP_SIZE 1g Minimal Heap size required for Ping DataGovernance STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/start-server STARTUP_FOREGROUND_OPTS --nodetach Adding lockdown mode so non administrative connections be made until server has been started with replication enabled STARTUP_BACKGROUND_OPTS Adding lockdown mode so non administrative connections be made until server has been started with replication enabled ROOT_USER_PASSWORD_FILE Location of file with the root user password (i.e. cn=directory manager). Defaults to the /SECRETS_DIR/root-user-password ENCRYPTION_PASSWORD_FILE Location of file with the passphrase for setting up encryption Defaults to the /SECRETS_DIR/encryption-password TAIL_LOG_FILES ${SERVER_ROOT_DIR}/logs/access Files tailed once container has started PD_PROFILE ${STAGING_DIR}/pd.profile ## Ports Exposed The following ports are exposed from the container. If a variable is used, then it may come from a parent container - ${LDAP_PORT} - ${LDAPS_PORT} - ${HTTPS_PORT} - ${JMX_PORT} - 5005 Running a PingDataGovernance container \u00b6 The easiest way to test test a simple standalone image of PingDataGovernance is to cut/paste the following command into a terminal on a machine with docker. docker run \\ --name pingdatagovernance \\ --publish 1389:389 \\ --publish 8443:443 \\ --detach \\ --env SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git \\ --env SERVER_PROFILE_PATH=getting-started/pingdatagovernance \\ --env PING_IDENTITY_ACCEPT_EULA=YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingdatagovernance:edge You can view the Docker logs with the command: docker logs -f pingdatagovernance You should see the output from a PingDataGovernance install and configuration, ending with a message the the PingDataGovernance has started. After it starts, you will see some typical access logs. Simply Ctrl-C after to stop tailing the logs. Stopping/Removing the container \u00b6 To stop the container: docker container stop pingdatagovernance To remove the container: docker container rm -f pingdatagovernance Docker Container Hook Scripts \u00b6 Please go here for details on all pingdatagovernance hook scripts This document is auto-generated from pingdatagovernance/Dockerfile Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"PingDataGovernance"},{"location":"docker-images/pingdatagovernance/#ping-identity-devops-docker-image-pingdatagovernance","text":"This docker image includes the Ping Identity PingDataGovernance product binaries and associated hook scripts to create and run a PingDataGovernance instance or instances.","title":"Ping Identity DevOps Docker Image - pingdatagovernance"},{"location":"docker-images/pingdatagovernance/#related-docker-images","text":"pingidentity/pingbase - Parent Image > This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingdatacommon - Common Ping files (i.e. hook scripts) pingidentity/pingdownloader - Used to download product bits","title":"Related Docker Images"},{"location":"docker-images/pingdatagovernance/#environment-variables","text":"In addition to environment variables inherited from pingidentity/pingbase , the following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} PING_PRODUCT PingDataGovernance PingIdentity license version Ping product name LICENSE_FILE_NAME PingDataGovernance.lic Name of License File LICENSE_SHORT_NAME PG Short name used when retrieving license from License Server LICENSE_VERSION ${LICENSE_VERSION} Version used when retrieving license from License Server MAX_HEAP_SIZE 1g Minimal Heap size required for Ping DataGovernance STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/start-server STARTUP_FOREGROUND_OPTS --nodetach Adding lockdown mode so non administrative connections be made until server has been started with replication enabled STARTUP_BACKGROUND_OPTS Adding lockdown mode so non administrative connections be made until server has been started with replication enabled ROOT_USER_PASSWORD_FILE Location of file with the root user password (i.e. cn=directory manager). Defaults to the /SECRETS_DIR/root-user-password ENCRYPTION_PASSWORD_FILE Location of file with the passphrase for setting up encryption Defaults to the /SECRETS_DIR/encryption-password TAIL_LOG_FILES ${SERVER_ROOT_DIR}/logs/access Files tailed once container has started PD_PROFILE ${STAGING_DIR}/pd.profile ## Ports Exposed The following ports are exposed from the container. If a variable is used, then it may come from a parent container - ${LDAP_PORT} - ${LDAPS_PORT} - ${HTTPS_PORT} - ${JMX_PORT} - 5005","title":"Environment Variables"},{"location":"docker-images/pingdatagovernance/#running-a-pingdatagovernance-container","text":"The easiest way to test test a simple standalone image of PingDataGovernance is to cut/paste the following command into a terminal on a machine with docker. docker run \\ --name pingdatagovernance \\ --publish 1389:389 \\ --publish 8443:443 \\ --detach \\ --env SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git \\ --env SERVER_PROFILE_PATH=getting-started/pingdatagovernance \\ --env PING_IDENTITY_ACCEPT_EULA=YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingdatagovernance:edge You can view the Docker logs with the command: docker logs -f pingdatagovernance You should see the output from a PingDataGovernance install and configuration, ending with a message the the PingDataGovernance has started. After it starts, you will see some typical access logs. Simply Ctrl-C after to stop tailing the logs.","title":"Running a PingDataGovernance container"},{"location":"docker-images/pingdatagovernance/#stoppingremoving-the-container","text":"To stop the container: docker container stop pingdatagovernance To remove the container: docker container rm -f pingdatagovernance","title":"Stopping/Removing the container"},{"location":"docker-images/pingdatagovernance/#docker-container-hook-scripts","text":"Please go here for details on all pingdatagovernance hook scripts This document is auto-generated from pingdatagovernance/Dockerfile Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Docker Container Hook Scripts"},{"location":"docker-images/pingdatagovernance/hooks/","text":"Ping Identity DevOps pingdatagovernance Hooks \u00b6 There are no default hooks defined for the pingdatagovernance image. Hooks defined by parent images (i.e. pingcommon/pingdatacommon) will be inherited by this image. Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatagovernance` Hooks"},{"location":"docker-images/pingdatagovernance/hooks/#ping-identity-devops-pingdatagovernance-hooks","text":"There are no default hooks defined for the pingdatagovernance image. Hooks defined by parent images (i.e. pingcommon/pingdatacommon) will be inherited by this image. Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatagovernance Hooks"},{"location":"docker-images/pingdatagovernance/hooks/183-run-setup.sh/","text":"Ping Identity DevOps pingdatagovernance Hook - 183-run-setup.sh \u00b6 This document auto-generated from pingdatagovernance/hooks/183-run-setup.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatagovernance` Hook - `183-run-setup.sh`"},{"location":"docker-images/pingdatagovernance/hooks/183-run-setup.sh/#ping-identity-devops-pingdatagovernance-hook-183-run-setupsh","text":"This document auto-generated from pingdatagovernance/hooks/183-run-setup.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatagovernance Hook - 183-run-setup.sh"},{"location":"docker-images/pingdatagovernancepap/","text":"Ping Identity DevOps Docker Image - pingdatagovernancepap \u00b6 This docker image includes the Ping Identity PingDataGovernance PAP product binaries and associated hook scripts to create and run a PingDataGovernance PAP instance. Related Docker Images \u00b6 pingidentity/pingbase - Parent Image > This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingdatacommon - Common Ping files (i.e. hook scripts) pingidentity/pingdownloader - Used to download product bits Environment Variables \u00b6 In addition to environment variables inherited from pingidentity/pingbase , the following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} PING_PRODUCT PingDataGovernance-PAP PingIdentity license version Ping product name LICENSE_FILE_NAME PingDataGovernance.lic Name of License File LICENSE_SHORT_NAME PG Short name used when retrieving license from License Server LICENSE_VERSION ${LICENSE_VERSION} Version used when retrieving license from License Server MAX_HEAP_SIZE 384m Minimal Heap size required for Ping DataGovernance PAP STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/start-server STARTUP_FOREGROUND_OPTS --nodetach Adding lockdown mode so non administrative connections be made until server has been started with replication enabled STARTUP_BACKGROUND_OPTS Adding lockdown mode so non administrative connections be made until server has been started with replication enabled TAIL_LOG_PARALLEL y TAIL_LOG_FILES \"${SERVER_ROOT_DIR}/logs/datagovernance-pap.log \\ Files tailed once container has started REST_API_HOSTNAME localhost Hostname used for the REST API (deprecated, use PING_EXTERNAL_BASE_URL instead) DECISION_POINT_SHARED_SECRET 2FederateM0re Define shared secret between PDG and PAP ## Ports Exposed The following ports are exposed from the container. If a variable is used, then it may come from a parent container - ${HTTPS_PORT} Running a PingDataGovernance PAP container \u00b6 A PingDataGovernance PAP may be set up in one of two modes: Demo mode : Uses insecure username/password authentication. OIDC mode : Uses an OpenID Connect provider for authentication. To run a PingDataGovernance PAP container in demo mode: docker run \\ --name pingdatagovernancepap \\ --env PING_EXTERNAL_BASE_URL=my-pap-hostname:8443 \\ --publish 8443:443 \\ --detach \\ --env PING_IDENTITY_ACCEPT_EULA=YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingdatagovernancepap:edge Log in with: * https://my-pap-hostname:8443/ * Username: admin * Password: password123 To run a PingDataGovernance PAP container in OpenID Connect mode, specify the PING_OIDC_CONFIGURATION_ENDPOINT and PING_CLIENT_ID environment variables: docker run \\ --name pingdatagovernancepap \\ --env PING_EXTERNAL_BASE_URL=my-pap-hostname:8443 \\ --env PING_OIDC_CONFIGURATION_ENDPOINT=https://my-oidc-provider/.well-known/openid-configuration \\ --env PING_CLIENT_ID=b1929abc-e108-4b4f-83d467059fa1 \\ --publish 8443:443 \\ --detach \\ --env PING_IDENTITY_ACCEPT_EULA=YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingdatagovernancepap:edge Note: If both PING_OIDC_CONFIGURATION_ENDPOINT and PING_CLIENT_ID are not specified, then the PAP will be set up in demo mode. Log in with: * https://my-pap-hostname:8443/ * Provide credentials as prompted by the OIDC provider Follow Docker logs with: docker logs -f pingdatagovernancepap Specifying the external hostname and port \u00b6 The PAP consists of a client-side application that runs in the user's web browser and a backend REST API service that runs within the container. So that the client-side application can successfully make API calls to the backend, the PAP must be configured with an externally accessible hostname:port. If the PAP is configured in OIDC mode, then the external hostname:port pair is also needed so that the PAP can correctly generate its OIDC redirect URI. Use the PING_EXTERNAL_BASE_URL environment variable to specify the PAP's external hostname and port using the form hostname[:port] , where hostname is the hostname of the Docker host and port is the PAP container's published port. If the published port is 443, then it should be omitted. For example: docker run \\ --name pingdatagovernancepap \\ --env PING_EXTERNAL_BASE_URL=my-pap-hostname:8443 \\ --publish 8443:443 \\ --detach \\ --env PING_IDENTITY_ACCEPT_EULA=YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingdatagovernancepap:edge Changing the default periodic database backup schedule and location \u00b6 The PAP performs periodic backups of the policy database. The results are placed in the policy-backup directory underneath the instance root. Use the PING_BACKUP_SCHEDULE environment variable to specify the PAP's periodic database backup schedule in the form of a cron expression. The cron expression will be evaluated against the container timezone, UTC. Use the PING_H2_BACKUP_DIR environment variable to change the backup output directory. For example, to perform backups daily at UTC noon and place backups in /opt/out/backup : docker run \\ --name pingdatagovernancepap \\ --env PING_EXTERNAL_BASE_URL=my-pap-hostname:8443 \\ --env PING_BACKUP_SCHEDULE=\"0 0 12 * * ?\" \\ --env PING_H2_BACKUP_DIR=/opt/out/backup \\ --publish 8443:443 \\ --detach \\ pingidentity/pingdatagovernancepap:edge Docker Container Hook Scripts \u00b6 Please go here for details on all pingdatagovernancepap hook scripts This document is auto-generated from pingdatagovernancepap/Dockerfile Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"PingDataGovernance PAP"},{"location":"docker-images/pingdatagovernancepap/#ping-identity-devops-docker-image-pingdatagovernancepap","text":"This docker image includes the Ping Identity PingDataGovernance PAP product binaries and associated hook scripts to create and run a PingDataGovernance PAP instance.","title":"Ping Identity DevOps Docker Image - pingdatagovernancepap"},{"location":"docker-images/pingdatagovernancepap/#related-docker-images","text":"pingidentity/pingbase - Parent Image > This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingdatacommon - Common Ping files (i.e. hook scripts) pingidentity/pingdownloader - Used to download product bits","title":"Related Docker Images"},{"location":"docker-images/pingdatagovernancepap/#environment-variables","text":"In addition to environment variables inherited from pingidentity/pingbase , the following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} PING_PRODUCT PingDataGovernance-PAP PingIdentity license version Ping product name LICENSE_FILE_NAME PingDataGovernance.lic Name of License File LICENSE_SHORT_NAME PG Short name used when retrieving license from License Server LICENSE_VERSION ${LICENSE_VERSION} Version used when retrieving license from License Server MAX_HEAP_SIZE 384m Minimal Heap size required for Ping DataGovernance PAP STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/start-server STARTUP_FOREGROUND_OPTS --nodetach Adding lockdown mode so non administrative connections be made until server has been started with replication enabled STARTUP_BACKGROUND_OPTS Adding lockdown mode so non administrative connections be made until server has been started with replication enabled TAIL_LOG_PARALLEL y TAIL_LOG_FILES \"${SERVER_ROOT_DIR}/logs/datagovernance-pap.log \\ Files tailed once container has started REST_API_HOSTNAME localhost Hostname used for the REST API (deprecated, use PING_EXTERNAL_BASE_URL instead) DECISION_POINT_SHARED_SECRET 2FederateM0re Define shared secret between PDG and PAP ## Ports Exposed The following ports are exposed from the container. If a variable is used, then it may come from a parent container - ${HTTPS_PORT}","title":"Environment Variables"},{"location":"docker-images/pingdatagovernancepap/#running-a-pingdatagovernance-pap-container","text":"A PingDataGovernance PAP may be set up in one of two modes: Demo mode : Uses insecure username/password authentication. OIDC mode : Uses an OpenID Connect provider for authentication. To run a PingDataGovernance PAP container in demo mode: docker run \\ --name pingdatagovernancepap \\ --env PING_EXTERNAL_BASE_URL=my-pap-hostname:8443 \\ --publish 8443:443 \\ --detach \\ --env PING_IDENTITY_ACCEPT_EULA=YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingdatagovernancepap:edge Log in with: * https://my-pap-hostname:8443/ * Username: admin * Password: password123 To run a PingDataGovernance PAP container in OpenID Connect mode, specify the PING_OIDC_CONFIGURATION_ENDPOINT and PING_CLIENT_ID environment variables: docker run \\ --name pingdatagovernancepap \\ --env PING_EXTERNAL_BASE_URL=my-pap-hostname:8443 \\ --env PING_OIDC_CONFIGURATION_ENDPOINT=https://my-oidc-provider/.well-known/openid-configuration \\ --env PING_CLIENT_ID=b1929abc-e108-4b4f-83d467059fa1 \\ --publish 8443:443 \\ --detach \\ --env PING_IDENTITY_ACCEPT_EULA=YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingdatagovernancepap:edge Note: If both PING_OIDC_CONFIGURATION_ENDPOINT and PING_CLIENT_ID are not specified, then the PAP will be set up in demo mode. Log in with: * https://my-pap-hostname:8443/ * Provide credentials as prompted by the OIDC provider Follow Docker logs with: docker logs -f pingdatagovernancepap","title":"Running a PingDataGovernance PAP container"},{"location":"docker-images/pingdatagovernancepap/#specifying-the-external-hostname-and-port","text":"The PAP consists of a client-side application that runs in the user's web browser and a backend REST API service that runs within the container. So that the client-side application can successfully make API calls to the backend, the PAP must be configured with an externally accessible hostname:port. If the PAP is configured in OIDC mode, then the external hostname:port pair is also needed so that the PAP can correctly generate its OIDC redirect URI. Use the PING_EXTERNAL_BASE_URL environment variable to specify the PAP's external hostname and port using the form hostname[:port] , where hostname is the hostname of the Docker host and port is the PAP container's published port. If the published port is 443, then it should be omitted. For example: docker run \\ --name pingdatagovernancepap \\ --env PING_EXTERNAL_BASE_URL=my-pap-hostname:8443 \\ --publish 8443:443 \\ --detach \\ --env PING_IDENTITY_ACCEPT_EULA=YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingdatagovernancepap:edge","title":"Specifying the external hostname and port"},{"location":"docker-images/pingdatagovernancepap/#changing-the-default-periodic-database-backup-schedule-and-location","text":"The PAP performs periodic backups of the policy database. The results are placed in the policy-backup directory underneath the instance root. Use the PING_BACKUP_SCHEDULE environment variable to specify the PAP's periodic database backup schedule in the form of a cron expression. The cron expression will be evaluated against the container timezone, UTC. Use the PING_H2_BACKUP_DIR environment variable to change the backup output directory. For example, to perform backups daily at UTC noon and place backups in /opt/out/backup : docker run \\ --name pingdatagovernancepap \\ --env PING_EXTERNAL_BASE_URL=my-pap-hostname:8443 \\ --env PING_BACKUP_SCHEDULE=\"0 0 12 * * ?\" \\ --env PING_H2_BACKUP_DIR=/opt/out/backup \\ --publish 8443:443 \\ --detach \\ pingidentity/pingdatagovernancepap:edge","title":"Changing the default periodic database backup schedule and location"},{"location":"docker-images/pingdatagovernancepap/#docker-container-hook-scripts","text":"Please go here for details on all pingdatagovernancepap hook scripts This document is auto-generated from pingdatagovernancepap/Dockerfile Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Docker Container Hook Scripts"},{"location":"docker-images/pingdatagovernancepap/hooks/","text":"Ping Identity DevOps pingdatagovernancepap Hooks \u00b6 List of available hooks: * 18-setup-sequence.sh * 183-run-setup.sh * 80-post-start.sh * 81-install-policies.sh * pingdatagovernancepap.lib.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from pingdatagovernancepap/opt/staging/hooks Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatagovernancepap` Hooks"},{"location":"docker-images/pingdatagovernancepap/hooks/#ping-identity-devops-pingdatagovernancepap-hooks","text":"List of available hooks: * 18-setup-sequence.sh * 183-run-setup.sh * 80-post-start.sh * 81-install-policies.sh * pingdatagovernancepap.lib.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from pingdatagovernancepap/opt/staging/hooks Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatagovernancepap Hooks"},{"location":"docker-images/pingdatagovernancepap/hooks/18-setup-sequence.sh/","text":"Ping Identity DevOps pingdatagovernancepap Hook - 18-setup-sequence.sh \u00b6 Quarterbacks all the scripts associated with the setup of a PingData product This document is auto-generated from pingdatagovernancepap/opt/staging/hooks/18-setup-sequence.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatagovernancepap` Hook - `18-setup-sequence.sh`"},{"location":"docker-images/pingdatagovernancepap/hooks/18-setup-sequence.sh/#ping-identity-devops-pingdatagovernancepap-hook-18-setup-sequencesh","text":"Quarterbacks all the scripts associated with the setup of a PingData product This document is auto-generated from pingdatagovernancepap/opt/staging/hooks/18-setup-sequence.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatagovernancepap Hook - 18-setup-sequence.sh"},{"location":"docker-images/pingdatagovernancepap/hooks/183-run-setup.sh/","text":"Ping Identity DevOps pingdatagovernancepap Hook - 183-run-setup.sh \u00b6 This document is auto-generated from pingdatagovernancepap/opt/staging/hooks/183-run-setup.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatagovernancepap` Hook - `183-run-setup.sh`"},{"location":"docker-images/pingdatagovernancepap/hooks/183-run-setup.sh/#ping-identity-devops-pingdatagovernancepap-hook-183-run-setupsh","text":"This document is auto-generated from pingdatagovernancepap/opt/staging/hooks/183-run-setup.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatagovernancepap Hook - 183-run-setup.sh"},{"location":"docker-images/pingdatagovernancepap/hooks/80-post-start.sh/","text":"Ping Identity DevOps pingdatagovernancepap Hook - 80-post-start.sh \u00b6 This script is used to import any configurations that are needed after PingDataGovernance-PAP starts This document is auto-generated from pingdatagovernancepap/opt/staging/hooks/80-post-start.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatagovernancepap` Hook - `80-post-start.sh`"},{"location":"docker-images/pingdatagovernancepap/hooks/80-post-start.sh/#ping-identity-devops-pingdatagovernancepap-hook-80-post-startsh","text":"This script is used to import any configurations that are needed after PingDataGovernance-PAP starts This document is auto-generated from pingdatagovernancepap/opt/staging/hooks/80-post-start.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatagovernancepap Hook - 80-post-start.sh"},{"location":"docker-images/pingdatagovernancepap/hooks/81-install-policies.sh/","text":"Ping Identity DevOps pingdatagovernancepap Hook - 81-install-policies.sh \u00b6 This document is auto-generated from pingdatagovernancepap/opt/staging/hooks/81-install-policies.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatagovernancepap` Hook - `81-install-policies.sh`"},{"location":"docker-images/pingdatagovernancepap/hooks/81-install-policies.sh/#ping-identity-devops-pingdatagovernancepap-hook-81-install-policiessh","text":"This document is auto-generated from pingdatagovernancepap/opt/staging/hooks/81-install-policies.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatagovernancepap Hook - 81-install-policies.sh"},{"location":"docker-images/pingdatagovernancepap/hooks/pingdatagovernancepap.lib.sh/","text":"Ping Identity DevOps pingdatagovernancepap Hook - pingdatagovernancepap.lib.sh \u00b6 This document is auto-generated from pingdatagovernancepap/opt/staging/hooks/pingdatagovernancepap.lib.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatagovernancepap` Hook - `pingdatagovernancepap.lib.sh`"},{"location":"docker-images/pingdatagovernancepap/hooks/pingdatagovernancepap.lib.sh/#ping-identity-devops-pingdatagovernancepap-hook-pingdatagovernancepaplibsh","text":"This document is auto-generated from pingdatagovernancepap/opt/staging/hooks/pingdatagovernancepap.lib.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatagovernancepap Hook - pingdatagovernancepap.lib.sh"},{"location":"docker-images/pingdatasync/","text":"Ping Identity DevOps Docker Image - pingdatasync \u00b6 This docker image includes the Ping Identity PingDataSync product binaries and associated hook scripts to create and run a PingDataSync instance. Related Docker Images \u00b6 pingidentity/pingbase - Parent Image > This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingdatacommon - Common Ping files (i.e. hook scripts) pingidentity/pingdownloader - Used to download product bits Environment Variables \u00b6 In addition to environment variables inherited from pingidentity/pingbase , the following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} TAIL_LOG_FILES ${SERVER_ROOT_DIR}/logs/sync PingIdentity license version LICENSE_FILE_NAME PingDirectory.lic LICENSE_SHORT_NAME PD LICENSE_VERSION ${LICENSE_VERSION} PING_PRODUCT PingDataSync STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/start-server STARTUP_FOREGROUND_OPTS --nodetach RETRY_TIMEOUT_SECONDS 180 The default retry timeout in seconds for manage-topology and remove-defunct-server ADMIN_USER_NAME admin Failover administrative user ROOT_USER_PASSWORD_FILE Location of file with the root user password (i.e. cn=directory manager). Defaults to the /SECRETS_DIR/root-user-password ADMIN_USER_PASSWORD_FILE PD_PROFILE ${STAGING_DIR}/pd.profile ## Ports Exposed The following ports are exposed from the container. If a variable is used, then it may come from a parent container - ${LDAP_PORT} - ${LDAPS_PORT} - ${HTTPS_PORT} - ${JMX_PORT} Running a PingDataSync container \u00b6 docker run \\ --name pingdatasync \\ --publish 1389:389 \\ --publish 8443:443 \\ --detach \\ --env SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git \\ --env SERVER_PROFILE_PATH=simple-sync/pingdatasync \\ --env PING_IDENTITY_ACCEPT_EULA=YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingdatasync:edge Docker Container Hook Scripts \u00b6 Please go here for details on all pingdatasync hook scripts This document is auto-generated from pingdatasync/Dockerfile Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"PingDataSync"},{"location":"docker-images/pingdatasync/#ping-identity-devops-docker-image-pingdatasync","text":"This docker image includes the Ping Identity PingDataSync product binaries and associated hook scripts to create and run a PingDataSync instance.","title":"Ping Identity DevOps Docker Image - pingdatasync"},{"location":"docker-images/pingdatasync/#related-docker-images","text":"pingidentity/pingbase - Parent Image > This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingdatacommon - Common Ping files (i.e. hook scripts) pingidentity/pingdownloader - Used to download product bits","title":"Related Docker Images"},{"location":"docker-images/pingdatasync/#environment-variables","text":"In addition to environment variables inherited from pingidentity/pingbase , the following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} TAIL_LOG_FILES ${SERVER_ROOT_DIR}/logs/sync PingIdentity license version LICENSE_FILE_NAME PingDirectory.lic LICENSE_SHORT_NAME PD LICENSE_VERSION ${LICENSE_VERSION} PING_PRODUCT PingDataSync STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/start-server STARTUP_FOREGROUND_OPTS --nodetach RETRY_TIMEOUT_SECONDS 180 The default retry timeout in seconds for manage-topology and remove-defunct-server ADMIN_USER_NAME admin Failover administrative user ROOT_USER_PASSWORD_FILE Location of file with the root user password (i.e. cn=directory manager). Defaults to the /SECRETS_DIR/root-user-password ADMIN_USER_PASSWORD_FILE PD_PROFILE ${STAGING_DIR}/pd.profile ## Ports Exposed The following ports are exposed from the container. If a variable is used, then it may come from a parent container - ${LDAP_PORT} - ${LDAPS_PORT} - ${HTTPS_PORT} - ${JMX_PORT}","title":"Environment Variables"},{"location":"docker-images/pingdatasync/#running-a-pingdatasync-container","text":"docker run \\ --name pingdatasync \\ --publish 1389:389 \\ --publish 8443:443 \\ --detach \\ --env SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git \\ --env SERVER_PROFILE_PATH=simple-sync/pingdatasync \\ --env PING_IDENTITY_ACCEPT_EULA=YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingdatasync:edge","title":"Running a PingDataSync container"},{"location":"docker-images/pingdatasync/#docker-container-hook-scripts","text":"Please go here for details on all pingdatasync hook scripts This document is auto-generated from pingdatasync/Dockerfile Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Docker Container Hook Scripts"},{"location":"docker-images/pingdatasync/hooks/","text":"Ping Identity DevOps pingdatasync Hooks \u00b6 List of available hooks: * 03-build-run-plan.sh * 80-post-start.sh * 90-shutdown-sequence.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from pingdatasync/opt/staging/hooks Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatasync` Hooks"},{"location":"docker-images/pingdatasync/hooks/#ping-identity-devops-pingdatasync-hooks","text":"List of available hooks: * 03-build-run-plan.sh * 80-post-start.sh * 90-shutdown-sequence.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from pingdatasync/opt/staging/hooks Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatasync Hooks"},{"location":"docker-images/pingdatasync/hooks/03-build-run-plan.sh/","text":"Ping Identity DevOps pingdatasync Hook - 03-build-run-plan.sh \u00b6 This script is called to determine the plan for the server as it starts up. This document is auto-generated from pingdatasync/opt/staging/hooks/03-build-run-plan.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatasync` Hook - `03-build-run-plan.sh`"},{"location":"docker-images/pingdatasync/hooks/03-build-run-plan.sh/#ping-identity-devops-pingdatasync-hook-03-build-run-plansh","text":"This script is called to determine the plan for the server as it starts up. This document is auto-generated from pingdatasync/opt/staging/hooks/03-build-run-plan.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatasync Hook - 03-build-run-plan.sh"},{"location":"docker-images/pingdatasync/hooks/183-run-setup.sh/","text":"Ping Identity DevOps pingdatasync Hook - 183-run-setup.sh \u00b6 This document auto-generated from pingdatasync/hooks/183-run-setup.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatasync` Hook - `183-run-setup.sh`"},{"location":"docker-images/pingdatasync/hooks/183-run-setup.sh/#ping-identity-devops-pingdatasync-hook-183-run-setupsh","text":"This document auto-generated from pingdatasync/hooks/183-run-setup.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatasync Hook - 183-run-setup.sh"},{"location":"docker-images/pingdatasync/hooks/80-post-start.sh/","text":"Ping Identity DevOps pingdatasync Hook - 80-post-start.sh \u00b6 This script is mostly the same as the 80-post-start.sh hook in the * Enabling PingDataSync failover This document is auto-generated from pingdatasync/opt/staging/hooks/80-post-start.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatasync` Hook - `80-post-start.sh`"},{"location":"docker-images/pingdatasync/hooks/80-post-start.sh/#ping-identity-devops-pingdatasync-hook-80-post-startsh","text":"This script is mostly the same as the 80-post-start.sh hook in the * Enabling PingDataSync failover This document is auto-generated from pingdatasync/opt/staging/hooks/80-post-start.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatasync Hook - 80-post-start.sh"},{"location":"docker-images/pingdatasync/hooks/90-shutdown-sequence.sh/","text":"Ping Identity DevOps pingdatasync Hook - 90-shutdown-sequence.sh \u00b6 This script handles removing the server from the topology during a shutdown. This document is auto-generated from pingdatasync/opt/staging/hooks/90-shutdown-sequence.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdatasync` Hook - `90-shutdown-sequence.sh`"},{"location":"docker-images/pingdatasync/hooks/90-shutdown-sequence.sh/#ping-identity-devops-pingdatasync-hook-90-shutdown-sequencesh","text":"This script handles removing the server from the topology during a shutdown. This document is auto-generated from pingdatasync/opt/staging/hooks/90-shutdown-sequence.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdatasync Hook - 90-shutdown-sequence.sh"},{"location":"docker-images/pingdelegator/","text":"Ping Identity Docker Image - pingdelegator \u00b6 This docker image provides an NGINX instance with PingDelegator that can be used in administering PingDirectory Users/Groups. Related Docker Images \u00b6 pingidentity/pingbase - Parent Image > This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingcommon - Common Ping files (i.e. hook scripts) pingidentity/pingdownloader - Used to download product bits Environment Variables \u00b6 In addition to environment variables inherited from pingidentity/pingbase , the following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} PD_DELEGATOR_PUBLIC_HOSTNAME localhost PD_DELEGATOR_HTTP_PORT 6080 PD_DELEGATOR_HTTPS_PORT 6443 PING_CONTAINER_PRIVILEGED false Container uid/gid and uname/gname that are required for nginx PING_CONTAINER_UID 100 PING_CONTAINER_GID 101 PING_CONTAINER_UNAME nginx PING_CONTAINER_GNAME nginx PF_ENGINE_PUBLIC_HOSTNAME localhost The hostname for the public Ping Federate instance used for SSO. PF_ENGINE_PUBLIC_PORT 9031 The port for the public Ping Federate instance used for SSO. NOTE: If using port 443 along with a base URL with no specified port, set to an empty string. PF_DELEGATOR_CLIENTID dadmin The client id that was set up with Ping Federate for Ping Delegator. PD_ENGINE_PUBLIC_HOSTNAME localhost The hostname for the DS instance the app will be interfacing with. PD_ENGINE_PUBLIC_PORT 1443 The HTTPS port for the DS instance the app will be interfacing with. PD_DELEGATOR_TIMEOUT_LENGTH_MINS 30 The length of time (in minutes) until the session will require a new login attempt PD_DELEGATOR_HEADER_BAR_LOGO The filename used as the logo in the header bar, relative to this application's build directory. Note about logos: The size of the image will be scaled down to fit 22px of height and a max-width of 150px. For best results, it is advised to make the image close to this height and width ratio as well as to crop out any blank spacing around the logo to maximize its presentation. e.g. '${SERVER_ROOT_DIR}/html/delegator/images/my_company_logo.png' PD_DELEGATOR_DADMIN_API_NAMESPACE The namespace for the Delegated Admin API on the DS instance. In most cases, this does not need to be set here. e.g. 'dadmin/v2' PD_DELEGATOR_PROFILE_SCOPE_ENABLED false Set to true if the \"profile\" scope is supported for the Delegated Admin OIDC client on PingFederate and you wish to use it to show the current user's name in the navigation. STARTUP_COMMAND nginx STARTUP_FOREGROUND_OPTS -c ${SERVER_ROOT_DIR}/etc/nginx.conf STARTUP_BACKGROUND_OPTS ${STARTUP_FOREGROUND_OPTS} ## Run To run a PingDelegator container with HTTPS_PORT=6443 (6443 is simply a convetion for PingDelegator so conflicts are reduced with other container HTTPS ports): docker run \\ --name pingdelegator \\ --publish 6443 :6443 \\ --detach \\ --env PING_IDENTITY_ACCEPT_EULA = YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingdelegator:edge PingDelegator does require running instances of PingFederate/PingDirectory. To run the an example deployment of PingDelegator in docker-compose, the ping-devops tool can be used: ping-devops docker start simplestack Docker Container Hook Scripts \u00b6 Please go here for details on all pingdelegator hook scripts This document is auto-generated from pingdelegator/Dockerfile Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"PingDelegator"},{"location":"docker-images/pingdelegator/#ping-identity-docker-image-pingdelegator","text":"This docker image provides an NGINX instance with PingDelegator that can be used in administering PingDirectory Users/Groups.","title":"Ping Identity Docker Image - pingdelegator"},{"location":"docker-images/pingdelegator/#related-docker-images","text":"pingidentity/pingbase - Parent Image > This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingcommon - Common Ping files (i.e. hook scripts) pingidentity/pingdownloader - Used to download product bits","title":"Related Docker Images"},{"location":"docker-images/pingdelegator/#environment-variables","text":"In addition to environment variables inherited from pingidentity/pingbase , the following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} PD_DELEGATOR_PUBLIC_HOSTNAME localhost PD_DELEGATOR_HTTP_PORT 6080 PD_DELEGATOR_HTTPS_PORT 6443 PING_CONTAINER_PRIVILEGED false Container uid/gid and uname/gname that are required for nginx PING_CONTAINER_UID 100 PING_CONTAINER_GID 101 PING_CONTAINER_UNAME nginx PING_CONTAINER_GNAME nginx PF_ENGINE_PUBLIC_HOSTNAME localhost The hostname for the public Ping Federate instance used for SSO. PF_ENGINE_PUBLIC_PORT 9031 The port for the public Ping Federate instance used for SSO. NOTE: If using port 443 along with a base URL with no specified port, set to an empty string. PF_DELEGATOR_CLIENTID dadmin The client id that was set up with Ping Federate for Ping Delegator. PD_ENGINE_PUBLIC_HOSTNAME localhost The hostname for the DS instance the app will be interfacing with. PD_ENGINE_PUBLIC_PORT 1443 The HTTPS port for the DS instance the app will be interfacing with. PD_DELEGATOR_TIMEOUT_LENGTH_MINS 30 The length of time (in minutes) until the session will require a new login attempt PD_DELEGATOR_HEADER_BAR_LOGO The filename used as the logo in the header bar, relative to this application's build directory. Note about logos: The size of the image will be scaled down to fit 22px of height and a max-width of 150px. For best results, it is advised to make the image close to this height and width ratio as well as to crop out any blank spacing around the logo to maximize its presentation. e.g. '${SERVER_ROOT_DIR}/html/delegator/images/my_company_logo.png' PD_DELEGATOR_DADMIN_API_NAMESPACE The namespace for the Delegated Admin API on the DS instance. In most cases, this does not need to be set here. e.g. 'dadmin/v2' PD_DELEGATOR_PROFILE_SCOPE_ENABLED false Set to true if the \"profile\" scope is supported for the Delegated Admin OIDC client on PingFederate and you wish to use it to show the current user's name in the navigation. STARTUP_COMMAND nginx STARTUP_FOREGROUND_OPTS -c ${SERVER_ROOT_DIR}/etc/nginx.conf STARTUP_BACKGROUND_OPTS ${STARTUP_FOREGROUND_OPTS} ## Run To run a PingDelegator container with HTTPS_PORT=6443 (6443 is simply a convetion for PingDelegator so conflicts are reduced with other container HTTPS ports): docker run \\ --name pingdelegator \\ --publish 6443 :6443 \\ --detach \\ --env PING_IDENTITY_ACCEPT_EULA = YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingdelegator:edge PingDelegator does require running instances of PingFederate/PingDirectory. To run the an example deployment of PingDelegator in docker-compose, the ping-devops tool can be used: ping-devops docker start simplestack","title":"Environment Variables"},{"location":"docker-images/pingdelegator/#docker-container-hook-scripts","text":"Please go here for details on all pingdelegator hook scripts This document is auto-generated from pingdelegator/Dockerfile Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Docker Container Hook Scripts"},{"location":"docker-images/pingdelegator/hooks/","text":"Ping Identity DevOps pingdelegator Hooks \u00b6 List of available hooks: * 02-get-remote-server-profile.sh.post * 04-check-variables.sh * 17-check-license.sh * 50-before-post-start.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from pingdelegator/opt/staging/hooks Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdelegator` Hooks"},{"location":"docker-images/pingdelegator/hooks/#ping-identity-devops-pingdelegator-hooks","text":"List of available hooks: * 02-get-remote-server-profile.sh.post * 04-check-variables.sh * 17-check-license.sh * 50-before-post-start.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from pingdelegator/opt/staging/hooks Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdelegator Hooks"},{"location":"docker-images/pingdelegator/hooks/02-get-remote-server-profile.sh.post/","text":"Ping Identity DevOps pingdelegator Hook - 02-get-remote-server-profile.sh.post \u00b6 This hook may be used to set the server if there is a setup procedure Note: The PingData (i.e. Directory, DataSync, DataGovernance, DirectoryProxy) products will all provide this This document is auto-generated from pingdelegator/opt/staging/hooks/02-get-remote-server-profile.sh.post Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdelegator` Hook - `02-get-remote-server-profile.sh.post`"},{"location":"docker-images/pingdelegator/hooks/02-get-remote-server-profile.sh.post/#ping-identity-devops-pingdelegator-hook-02-get-remote-server-profileshpost","text":"This hook may be used to set the server if there is a setup procedure Note: The PingData (i.e. Directory, DataSync, DataGovernance, DirectoryProxy) products will all provide this This document is auto-generated from pingdelegator/opt/staging/hooks/02-get-remote-server-profile.sh.post Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdelegator Hook - 02-get-remote-server-profile.sh.post"},{"location":"docker-images/pingdelegator/hooks/04-check-variables.sh/","text":"Ping Identity DevOps pingdelegator Hook - 04-check-variables.sh \u00b6 This document is auto-generated from pingdelegator/opt/staging/hooks/04-check-variables.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdelegator` Hook - `04-check-variables.sh`"},{"location":"docker-images/pingdelegator/hooks/04-check-variables.sh/#ping-identity-devops-pingdelegator-hook-04-check-variablessh","text":"This document is auto-generated from pingdelegator/opt/staging/hooks/04-check-variables.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdelegator Hook - 04-check-variables.sh"},{"location":"docker-images/pingdelegator/hooks/17-check-license.sh/","text":"Ping Identity DevOps pingdelegator Hook - 17-check-license.sh \u00b6 This document is auto-generated from pingdelegator/opt/staging/hooks/17-check-license.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdelegator` Hook - `17-check-license.sh`"},{"location":"docker-images/pingdelegator/hooks/17-check-license.sh/#ping-identity-devops-pingdelegator-hook-17-check-licensesh","text":"This document is auto-generated from pingdelegator/opt/staging/hooks/17-check-license.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdelegator Hook - 17-check-license.sh"},{"location":"docker-images/pingdelegator/hooks/50-before-post-start.sh/","text":"Ping Identity DevOps pingdelegator Hook - 50-before-post-start.sh \u00b6 This is called after the start or restart sequence has finished and before the server within the container starts This document is auto-generated from pingdelegator/opt/staging/hooks/50-before-post-start.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdelegator` Hook - `50-before-post-start.sh`"},{"location":"docker-images/pingdelegator/hooks/50-before-post-start.sh/#ping-identity-devops-pingdelegator-hook-50-before-post-startsh","text":"This is called after the start or restart sequence has finished and before the server within the container starts This document is auto-generated from pingdelegator/opt/staging/hooks/50-before-post-start.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdelegator Hook - 50-before-post-start.sh"},{"location":"docker-images/pingdirectory/","text":"Ping Identity DevOps Docker Image - pingdirectory \u00b6 This docker image includes the Ping Identity PingDirectory product binaries and associated hook scripts to create and run a PingDirectory instance or instances. Related Docker Images \u00b6 pingidentity/pingbase - Parent Image > This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingdatacommon - Common Ping files (i.e. hook scripts) pingidentity/pingdownloader - Used to download product bits Environment Variables \u00b6 In addition to environment variables inherited from pingidentity/pingbase , the following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} PING_PRODUCT PingDirectory PingIdentity license version Ping product name LICENSE_FILE_NAME PingDirectory.lic Name of License File LICENSE_SHORT_NAME PD Short name used when retrieving license from License Server LICENSE_VERSION ${LICENSE_VERSION} Version used when retrieving license from License Server REPLICATION_PORT 8989 Default PingDirectory Replication Port ADMIN_USER_NAME admin Replication administrative user STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/start-server PD_DELEGATOR_PUBLIC_HOSTNAME localhost Public hostname of the DA app STARTUP_FOREGROUND_OPTS --nodetach Adding lockdown mode so non administrative connections be made until server has been started with replication enabled STARTUP_BACKGROUND_OPTS Adding lockdown mode so non administrative connections be made until server has been started with replication enabled ROOT_USER_PASSWORD_FILE Location of file with the root user password (i.e. cn=directory manager). Defaults to the /SECRETS_DIR/root-user-password ADMIN_USER_PASSWORD_FILE Location of file with the admin password, used as the password replication admin Defaults to the /SECRETS_DIR/admin-user-password ENCRYPTION_PASSWORD_FILE Location of file with the passphrase for setting up encryption Defaults to the /SECRETS_DIR/encryption-password TAIL_LOG_FILES \"${SERVER_ROOT_DIR}/logs/access \\ Files tailed once container has started MAKELDIF_USERS 0 Number of users to auto-populate using make-ldif templates RETRY_TIMEOUT_SECONDS 180 The default retry timeout in seconds for dsreplication and remove-defunct-server DISABLE_SCHEMA_REPLICATION false Flag to disable schema replication. In a DevOps environment, schema comes from configuration. So it does not need to be replicated. PD_PROFILE ${STAGING_DIR}/pd.profile PD_REBUILD_ON_RESTART false Force a rebuild (replace-profile) of a PingDirectoy on restart. Used when changes are made outside of the PD_PROFILE ## Ports Exposed The following ports are exposed from the container. If a variable is used, then it may come from a parent container - ${LDAP_PORT} - ${LDAPS_PORT} - ${HTTPS_PORT} - ${JMX_PORT} - 5005 Running a PingDirectory container \u00b6 The easiest way to test test a simple standalone image of PingDirectory is to cut/paste the following command into a terminal on a machine with docker. docker run \\ --name pingdirectory \\ --publish 1389:389 \\ --publish 8443:443 \\ --detach \\ --env SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git \\ --env SERVER_PROFILE_PATH=getting-started/pingdirectory \\ --env PING_IDENTITY_ACCEPT_EULA=YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingdirectory:edge You can view the Docker logs with the command: docker logs -f pingdirectory You should see the ouptut from a PingDirectory install and configuration, ending with a message the the PingDirectory has started. After it starts, you will see some typical access logs. Simply Ctrl-C afer to stop tailing the logs. Running a sample 100/sec search rate test \u00b6 With the PingDirectory running from the previous section, you can run a searchrate job that will send load to the directory at a rate if 100/sec using the following command. docker exec -it pingdirectory \\ /opt/out/instance/bin/searchrate \\ -b dc=example,dc=com \\ --scope sub \\ --filter \"(uid=user.[1-9])\" \\ --attribute mail \\ --numThreads 2 \\ --ratePerSecond 100 Connecting with an LDAP Client \u00b6 Connect an LDAP Client (such as Apache Directory Studio) to this container using the default ports and credentials LDAP Port 1389 (mapped to 389) LDAP Base DN dc=example,dc=com Root Username cn=administrator Root Password 2FederateM0re Stopping/Removing the container \u00b6 To stop the container: docker container stop pingdirectory To remove the container: docker container rm -f pingdirectory Docker Container Hook Scripts \u00b6 Please go here for details on all pingdirectory hook scripts This document is auto-generated from pingdirectory/Dockerfile Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"PingDirectory"},{"location":"docker-images/pingdirectory/#ping-identity-devops-docker-image-pingdirectory","text":"This docker image includes the Ping Identity PingDirectory product binaries and associated hook scripts to create and run a PingDirectory instance or instances.","title":"Ping Identity DevOps Docker Image - pingdirectory"},{"location":"docker-images/pingdirectory/#related-docker-images","text":"pingidentity/pingbase - Parent Image > This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingdatacommon - Common Ping files (i.e. hook scripts) pingidentity/pingdownloader - Used to download product bits","title":"Related Docker Images"},{"location":"docker-images/pingdirectory/#environment-variables","text":"In addition to environment variables inherited from pingidentity/pingbase , the following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} PING_PRODUCT PingDirectory PingIdentity license version Ping product name LICENSE_FILE_NAME PingDirectory.lic Name of License File LICENSE_SHORT_NAME PD Short name used when retrieving license from License Server LICENSE_VERSION ${LICENSE_VERSION} Version used when retrieving license from License Server REPLICATION_PORT 8989 Default PingDirectory Replication Port ADMIN_USER_NAME admin Replication administrative user STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/start-server PD_DELEGATOR_PUBLIC_HOSTNAME localhost Public hostname of the DA app STARTUP_FOREGROUND_OPTS --nodetach Adding lockdown mode so non administrative connections be made until server has been started with replication enabled STARTUP_BACKGROUND_OPTS Adding lockdown mode so non administrative connections be made until server has been started with replication enabled ROOT_USER_PASSWORD_FILE Location of file with the root user password (i.e. cn=directory manager). Defaults to the /SECRETS_DIR/root-user-password ADMIN_USER_PASSWORD_FILE Location of file with the admin password, used as the password replication admin Defaults to the /SECRETS_DIR/admin-user-password ENCRYPTION_PASSWORD_FILE Location of file with the passphrase for setting up encryption Defaults to the /SECRETS_DIR/encryption-password TAIL_LOG_FILES \"${SERVER_ROOT_DIR}/logs/access \\ Files tailed once container has started MAKELDIF_USERS 0 Number of users to auto-populate using make-ldif templates RETRY_TIMEOUT_SECONDS 180 The default retry timeout in seconds for dsreplication and remove-defunct-server DISABLE_SCHEMA_REPLICATION false Flag to disable schema replication. In a DevOps environment, schema comes from configuration. So it does not need to be replicated. PD_PROFILE ${STAGING_DIR}/pd.profile PD_REBUILD_ON_RESTART false Force a rebuild (replace-profile) of a PingDirectoy on restart. Used when changes are made outside of the PD_PROFILE ## Ports Exposed The following ports are exposed from the container. If a variable is used, then it may come from a parent container - ${LDAP_PORT} - ${LDAPS_PORT} - ${HTTPS_PORT} - ${JMX_PORT} - 5005","title":"Environment Variables"},{"location":"docker-images/pingdirectory/#running-a-pingdirectory-container","text":"The easiest way to test test a simple standalone image of PingDirectory is to cut/paste the following command into a terminal on a machine with docker. docker run \\ --name pingdirectory \\ --publish 1389:389 \\ --publish 8443:443 \\ --detach \\ --env SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git \\ --env SERVER_PROFILE_PATH=getting-started/pingdirectory \\ --env PING_IDENTITY_ACCEPT_EULA=YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingdirectory:edge You can view the Docker logs with the command: docker logs -f pingdirectory You should see the ouptut from a PingDirectory install and configuration, ending with a message the the PingDirectory has started. After it starts, you will see some typical access logs. Simply Ctrl-C afer to stop tailing the logs.","title":"Running a PingDirectory container"},{"location":"docker-images/pingdirectory/#running-a-sample-100sec-search-rate-test","text":"With the PingDirectory running from the previous section, you can run a searchrate job that will send load to the directory at a rate if 100/sec using the following command. docker exec -it pingdirectory \\ /opt/out/instance/bin/searchrate \\ -b dc=example,dc=com \\ --scope sub \\ --filter \"(uid=user.[1-9])\" \\ --attribute mail \\ --numThreads 2 \\ --ratePerSecond 100","title":"Running a sample 100/sec search rate test"},{"location":"docker-images/pingdirectory/#connecting-with-an-ldap-client","text":"Connect an LDAP Client (such as Apache Directory Studio) to this container using the default ports and credentials LDAP Port 1389 (mapped to 389) LDAP Base DN dc=example,dc=com Root Username cn=administrator Root Password 2FederateM0re","title":"Connecting with an LDAP Client"},{"location":"docker-images/pingdirectory/#stoppingremoving-the-container","text":"To stop the container: docker container stop pingdirectory To remove the container: docker container rm -f pingdirectory","title":"Stopping/Removing the container"},{"location":"docker-images/pingdirectory/#docker-container-hook-scripts","text":"Please go here for details on all pingdirectory hook scripts This document is auto-generated from pingdirectory/Dockerfile Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Docker Container Hook Scripts"},{"location":"docker-images/pingdirectory/hooks/","text":"Ping Identity DevOps pingdirectory Hooks \u00b6 List of available hooks: * 03-build-run-plan.sh * 182-pre-setup.sh * 20-restart-sequence.sh * 80-post-start.sh * 90-shutdown-sequence.sh * pingdirectory.lib.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from pingdirectory/opt/staging/hooks Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectory` Hooks"},{"location":"docker-images/pingdirectory/hooks/#ping-identity-devops-pingdirectory-hooks","text":"List of available hooks: * 03-build-run-plan.sh * 182-pre-setup.sh * 20-restart-sequence.sh * 80-post-start.sh * 90-shutdown-sequence.sh * pingdirectory.lib.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from pingdirectory/opt/staging/hooks Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectory Hooks"},{"location":"docker-images/pingdirectory/hooks/03-build-run-plan.sh/","text":"Ping Identity DevOps pingdirectory Hook - 03-build-run-plan.sh \u00b6 This script is called to determine the plan for the server as it starts up. This document is auto-generated from pingdirectory/opt/staging/hooks/03-build-run-plan.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectory` Hook - `03-build-run-plan.sh`"},{"location":"docker-images/pingdirectory/hooks/03-build-run-plan.sh/#ping-identity-devops-pingdirectory-hook-03-build-run-plansh","text":"This script is called to determine the plan for the server as it starts up. This document is auto-generated from pingdirectory/opt/staging/hooks/03-build-run-plan.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectory Hook - 03-build-run-plan.sh"},{"location":"docker-images/pingdirectory/hooks/15-expand-templates.sh/","text":"Ping Identity DevOps pingdirectory Hook - 15-expand-templates.sh \u00b6 This will short circut the upper level pingcommon This document auto-generated from pingdirectory/hooks/15-expand-templates.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectory` Hook - `15-expand-templates.sh`"},{"location":"docker-images/pingdirectory/hooks/15-expand-templates.sh/#ping-identity-devops-pingdirectory-hook-15-expand-templatessh","text":"This will short circut the upper level pingcommon This document auto-generated from pingdirectory/hooks/15-expand-templates.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectory Hook - 15-expand-templates.sh"},{"location":"docker-images/pingdirectory/hooks/182-pre-setup.sh/","text":"Ping Identity DevOps pingdirectory Hook - 182-pre-setup.sh \u00b6 This document is auto-generated from pingdirectory/opt/staging/hooks/182-pre-setup.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectory` Hook - `182-pre-setup.sh`"},{"location":"docker-images/pingdirectory/hooks/182-pre-setup.sh/#ping-identity-devops-pingdirectory-hook-182-pre-setupsh","text":"This document is auto-generated from pingdirectory/opt/staging/hooks/182-pre-setup.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectory Hook - 182-pre-setup.sh"},{"location":"docker-images/pingdirectory/hooks/182-template-to-ldif.sh/","text":"Ping Identity DevOps pingdirectory Hook - 182-template-to-ldif.sh \u00b6 This hook will make-ldif from templates to ldif file in the same directory that will be used during the manage-profile setup This document auto-generated from pingdirectory/hooks/182-template-to-ldif.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectory` Hook - `182-template-to-ldif.sh`"},{"location":"docker-images/pingdirectory/hooks/182-template-to-ldif.sh/#ping-identity-devops-pingdirectory-hook-182-template-to-ldifsh","text":"This hook will make-ldif from templates to ldif file in the same directory that will be used during the manage-profile setup This document auto-generated from pingdirectory/hooks/182-template-to-ldif.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectory Hook - 182-template-to-ldif.sh"},{"location":"docker-images/pingdirectory/hooks/183-run-setup.sh/","text":"Ping Identity DevOps pingdirectory Hook - 183-run-setup.sh \u00b6 This document auto-generated from pingdirectory/hooks/183-run-setup.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectory` Hook - `183-run-setup.sh`"},{"location":"docker-images/pingdirectory/hooks/183-run-setup.sh/#ping-identity-devops-pingdirectory-hook-183-run-setupsh","text":"This document auto-generated from pingdirectory/hooks/183-run-setup.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectory Hook - 183-run-setup.sh"},{"location":"docker-images/pingdirectory/hooks/186-install-extensions.sh/","text":"Ping Identity DevOps pingdirectory Hook - 186-install-extensions.sh \u00b6 This document auto-generated from pingdirectory/hooks/186-install-extensions.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectory` Hook - `186-install-extensions.sh`"},{"location":"docker-images/pingdirectory/hooks/186-install-extensions.sh/#ping-identity-devops-pingdirectory-hook-186-install-extensionssh","text":"This document auto-generated from pingdirectory/hooks/186-install-extensions.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectory Hook - 186-install-extensions.sh"},{"location":"docker-images/pingdirectory/hooks/188-apply-configuration.sh/","text":"Ping Identity DevOps pingdirectory Hook - 188-apply-configuration.sh \u00b6 This document auto-generated from pingdirectory/opt/staging/hooks/188-apply-configuration.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectory` Hook - `188-apply-configuration.sh`"},{"location":"docker-images/pingdirectory/hooks/188-apply-configuration.sh/#ping-identity-devops-pingdirectory-hook-188-apply-configurationsh","text":"This document auto-generated from pingdirectory/opt/staging/hooks/188-apply-configuration.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectory Hook - 188-apply-configuration.sh"},{"location":"docker-images/pingdirectory/hooks/189-import-data.sh/","text":"Ping Identity DevOps pingdirectory Hook - 189-import-data.sh \u00b6 This hook will import data into the PingDirectory if there are data files included in the server profile data directory. If a .template file is provided, then makeldif will be run to create the .ldif file to be imported. If there are any skipped or rejected entries, an error message will be printed and the container will exit, unless the environment variable PD_IMPORT_CONTINUE_ON_ERROR=true is provided when the container is run. This document auto-generated from pingdirectory/hooks/189-import-data.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectory` Hook - `189-import-data.sh`"},{"location":"docker-images/pingdirectory/hooks/189-import-data.sh/#ping-identity-devops-pingdirectory-hook-189-import-datash","text":"This hook will import data into the PingDirectory if there are data files included in the server profile data directory. If a .template file is provided, then makeldif will be run to create the .ldif file to be imported. If there are any skipped or rejected entries, an error message will be printed and the container will exit, unless the environment variable PD_IMPORT_CONTINUE_ON_ERROR=true is provided when the container is run. This document auto-generated from pingdirectory/hooks/189-import-data.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectory Hook - 189-import-data.sh"},{"location":"docker-images/pingdirectory/hooks/20-restart-sequence.sh/","text":"Ping Identity DevOps pingdirectory Hook - 20-restart-sequence.sh \u00b6 This hook is called when the container has been built in a prior startup and a configuration has been found. This document is auto-generated from pingdirectory/opt/staging/hooks/20-restart-sequence.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectory` Hook - `20-restart-sequence.sh`"},{"location":"docker-images/pingdirectory/hooks/20-restart-sequence.sh/#ping-identity-devops-pingdirectory-hook-20-restart-sequencesh","text":"This hook is called when the container has been built in a prior startup and a configuration has been found. This document is auto-generated from pingdirectory/opt/staging/hooks/20-restart-sequence.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectory Hook - 20-restart-sequence.sh"},{"location":"docker-images/pingdirectory/hooks/80-post-start.sh/","text":"Ping Identity DevOps pingdirectory Hook - 80-post-start.sh \u00b6 This hook configures pingdirectory replication * Enabling Replication * Get the new current topology * Initialize replication This document is auto-generated from pingdirectory/opt/staging/hooks/80-post-start.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectory` Hook - `80-post-start.sh`"},{"location":"docker-images/pingdirectory/hooks/80-post-start.sh/#ping-identity-devops-pingdirectory-hook-80-post-startsh","text":"This hook configures pingdirectory replication * Enabling Replication * Get the new current topology * Initialize replication This document is auto-generated from pingdirectory/opt/staging/hooks/80-post-start.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectory Hook - 80-post-start.sh"},{"location":"docker-images/pingdirectory/hooks/81-generate-topology-json.sh/","text":"Ping Identity DevOps pingdirectory Hook - 81-generate-topology-json.sh \u00b6 This document auto-generated from pingdirectory/hooks/81-generate-topology-json.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectory` Hook - `81-generate-topology-json.sh`"},{"location":"docker-images/pingdirectory/hooks/81-generate-topology-json.sh/#ping-identity-devops-pingdirectory-hook-81-generate-topology-jsonsh","text":"This document auto-generated from pingdirectory/hooks/81-generate-topology-json.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectory Hook - 81-generate-topology-json.sh"},{"location":"docker-images/pingdirectory/hooks/81-repair-topology.sh/","text":"Ping Identity DevOps pingdirectory Hook - 81-repair-topology.sh \u00b6 This document auto-generated from pingdirectory/hooks/81-repair-topology.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectory` Hook - `81-repair-topology.sh`"},{"location":"docker-images/pingdirectory/hooks/81-repair-topology.sh/#ping-identity-devops-pingdirectory-hook-81-repair-topologysh","text":"This document auto-generated from pingdirectory/hooks/81-repair-topology.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectory Hook - 81-repair-topology.sh"},{"location":"docker-images/pingdirectory/hooks/90-shutdown-sequence.sh/","text":"Ping Identity DevOps pingdirectory Hook - 90-shutdown-sequence.sh \u00b6 This script handles removing the server from the topology during a shutdown. This document is auto-generated from pingdirectory/opt/staging/hooks/90-shutdown-sequence.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectory` Hook - `90-shutdown-sequence.sh`"},{"location":"docker-images/pingdirectory/hooks/90-shutdown-sequence.sh/#ping-identity-devops-pingdirectory-hook-90-shutdown-sequencesh","text":"This script handles removing the server from the topology during a shutdown. This document is auto-generated from pingdirectory/opt/staging/hooks/90-shutdown-sequence.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectory Hook - 90-shutdown-sequence.sh"},{"location":"docker-images/pingdirectory/hooks/pingdirectory.lib.sh/","text":"Ping Identity DevOps pingdirectory Hook - pingdirectory.lib.sh \u00b6 This document is auto-generated from pingdirectory/opt/staging/hooks/pingdirectory.lib.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectory` Hook - `pingdirectory.lib.sh`"},{"location":"docker-images/pingdirectory/hooks/pingdirectory.lib.sh/#ping-identity-devops-pingdirectory-hook-pingdirectorylibsh","text":"This document is auto-generated from pingdirectory/opt/staging/hooks/pingdirectory.lib.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectory Hook - pingdirectory.lib.sh"},{"location":"docker-images/pingdirectoryproxy/","text":"Ping Identity DevOps Docker Image - pingdirectoryproxy \u00b6 This docker image includes the Ping Identity PingDirectoryProxy product binaries and associated hook scripts to create and run a PingDirectoryProxy instance or instances. Related Docker Images \u00b6 pingidentity/pingbase - Parent Image > This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingdatacommon - Common Ping files (i.e. hook scripts) pingidentity/pingdownloader - Used to download product bits Environment Variables \u00b6 The following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} PING_PRODUCT PingDirectoryProxy PingIdentity license version Ping product name LICENSE_FILE_NAME PingDirectory.lic Name of License File LICENSE_SHORT_NAME PD Short name used when retrieving license from License Server LICENSE_VERSION ${LICENSE_VERSION} Version used when retrieving license from License Server ADMIN_USER_NAME admin Replication administrative user STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/start-server PD_DELEGATOR_PUBLIC_HOSTNAME localhost Public hostname of the DA app STARTUP_FOREGROUND_OPTS --nodetach Adding lockdown mode so non administrative connections be made until server has been started with replication enabled STARTUP_BACKGROUND_OPTS Adding lockdown mode so non administrative connections be made until server has been started with replication enabled ROOT_USER_PASSWORD_FILE Location of file with the root user password (i.e. cn=directory manager). Defaults to the /SECRETS_DIR/root-user-password TAIL_LOG_FILES \"${SERVER_ROOT_DIR}/logs/access \\ Files tailed once container has started PD_PROFILE ${STAGING_DIR}/pd.profile ## Ports Exposed The following ports are exposed from the container. If a variable is used, then it may come from a parent container - ${LDAP_PORT} - ${LDAPS_PORT} - ${HTTPS_PORT} - ${JMX_PORT} - 5005 Running a PingDirectoryProxy container \u00b6 The easiest way to test test a simple standalone image of PingDirectoryProxy is to cut/paste the following command into a terminal on a machine with docker. docker run \\ --name pingdirectoryproxy \\ --publish 1389:389 \\ --publish 8443:443 \\ --detach \\ --env SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git \\ --env SERVER_PROFILE_PATH=baseline/pingdirectoryproxy \\ --env PING_IDENTITY_ACCEPT_EULA=YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingdirectoryproxy:edge You can view the Docker logs with the command: docker logs -f pingdirectoryproxy You should see the output from a PingDirectoryProxy install and configuration, ending with a message the the PingDirectoryProxy has started. After it starts, you will see some typical access logs. Simply Ctrl-C afer to stop tailing the logs. Running a sample 100/sec search rate test \u00b6 With the PingDirectoryProxy running from the previous section, you can run a searchrate job that will send load to the directory at a rate if 100/sec using the following command. docker exec -it pingdirectoryproxy \\ /opt/out/instance/bin/searchrate \\ -b dc=example,dc=com \\ --scope sub \\ --filter \"(uid=user.[1-9])\" \\ --attribute mail \\ --numThreads 2 \\ --ratePerSecond 100 Connecting with an LDAP Client \u00b6 Connect an LDAP Client (such as Apache Directory Studio) to this container using the default ports and credentials LDAP Port 1389 (mapped to 389) LDAP Base DN dc=example,dc=com Root Username cn=administrator Root Password 2FederateM0re Stopping/Removing the container \u00b6 To stop the container: docker container stop pingdirectoryproxy To remove the container: docker container rm -f pingdirectoryproxy Docker Container Hook Scripts \u00b6 Please go here for details on all pingdirectoryproxy hook scripts This document is auto-generated from pingdirectoryproxy/Dockerfile Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"PingDirectoryProxy"},{"location":"docker-images/pingdirectoryproxy/#ping-identity-devops-docker-image-pingdirectoryproxy","text":"This docker image includes the Ping Identity PingDirectoryProxy product binaries and associated hook scripts to create and run a PingDirectoryProxy instance or instances.","title":"Ping Identity DevOps Docker Image - pingdirectoryproxy"},{"location":"docker-images/pingdirectoryproxy/#related-docker-images","text":"pingidentity/pingbase - Parent Image > This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingdatacommon - Common Ping files (i.e. hook scripts) pingidentity/pingdownloader - Used to download product bits","title":"Related Docker Images"},{"location":"docker-images/pingdirectoryproxy/#environment-variables","text":"The following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} PING_PRODUCT PingDirectoryProxy PingIdentity license version Ping product name LICENSE_FILE_NAME PingDirectory.lic Name of License File LICENSE_SHORT_NAME PD Short name used when retrieving license from License Server LICENSE_VERSION ${LICENSE_VERSION} Version used when retrieving license from License Server ADMIN_USER_NAME admin Replication administrative user STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/start-server PD_DELEGATOR_PUBLIC_HOSTNAME localhost Public hostname of the DA app STARTUP_FOREGROUND_OPTS --nodetach Adding lockdown mode so non administrative connections be made until server has been started with replication enabled STARTUP_BACKGROUND_OPTS Adding lockdown mode so non administrative connections be made until server has been started with replication enabled ROOT_USER_PASSWORD_FILE Location of file with the root user password (i.e. cn=directory manager). Defaults to the /SECRETS_DIR/root-user-password TAIL_LOG_FILES \"${SERVER_ROOT_DIR}/logs/access \\ Files tailed once container has started PD_PROFILE ${STAGING_DIR}/pd.profile ## Ports Exposed The following ports are exposed from the container. If a variable is used, then it may come from a parent container - ${LDAP_PORT} - ${LDAPS_PORT} - ${HTTPS_PORT} - ${JMX_PORT} - 5005","title":"Environment Variables"},{"location":"docker-images/pingdirectoryproxy/#running-a-pingdirectoryproxy-container","text":"The easiest way to test test a simple standalone image of PingDirectoryProxy is to cut/paste the following command into a terminal on a machine with docker. docker run \\ --name pingdirectoryproxy \\ --publish 1389:389 \\ --publish 8443:443 \\ --detach \\ --env SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git \\ --env SERVER_PROFILE_PATH=baseline/pingdirectoryproxy \\ --env PING_IDENTITY_ACCEPT_EULA=YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingdirectoryproxy:edge You can view the Docker logs with the command: docker logs -f pingdirectoryproxy You should see the output from a PingDirectoryProxy install and configuration, ending with a message the the PingDirectoryProxy has started. After it starts, you will see some typical access logs. Simply Ctrl-C afer to stop tailing the logs.","title":"Running a PingDirectoryProxy container"},{"location":"docker-images/pingdirectoryproxy/#running-a-sample-100sec-search-rate-test","text":"With the PingDirectoryProxy running from the previous section, you can run a searchrate job that will send load to the directory at a rate if 100/sec using the following command. docker exec -it pingdirectoryproxy \\ /opt/out/instance/bin/searchrate \\ -b dc=example,dc=com \\ --scope sub \\ --filter \"(uid=user.[1-9])\" \\ --attribute mail \\ --numThreads 2 \\ --ratePerSecond 100","title":"Running a sample 100/sec search rate test"},{"location":"docker-images/pingdirectoryproxy/#connecting-with-an-ldap-client","text":"Connect an LDAP Client (such as Apache Directory Studio) to this container using the default ports and credentials LDAP Port 1389 (mapped to 389) LDAP Base DN dc=example,dc=com Root Username cn=administrator Root Password 2FederateM0re","title":"Connecting with an LDAP Client"},{"location":"docker-images/pingdirectoryproxy/#stoppingremoving-the-container","text":"To stop the container: docker container stop pingdirectoryproxy To remove the container: docker container rm -f pingdirectoryproxy","title":"Stopping/Removing the container"},{"location":"docker-images/pingdirectoryproxy/#docker-container-hook-scripts","text":"Please go here for details on all pingdirectoryproxy hook scripts This document is auto-generated from pingdirectoryproxy/Dockerfile Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Docker Container Hook Scripts"},{"location":"docker-images/pingdirectoryproxy/hooks/","text":"Ping Identity DevOps pingdirectoryproxy Hooks \u00b6 There are no default hooks defined for the pingdirectoryproxy image. Hooks defined by parent images (i.e. pingcommon/pingdatacommon) will be inherited by this image. Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectoryproxy` Hooks"},{"location":"docker-images/pingdirectoryproxy/hooks/#ping-identity-devops-pingdirectoryproxy-hooks","text":"There are no default hooks defined for the pingdirectoryproxy image. Hooks defined by parent images (i.e. pingcommon/pingdatacommon) will be inherited by this image. Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectoryproxy Hooks"},{"location":"docker-images/pingdirectoryproxy/hooks/03-build-run-plan.sh/","text":"Ping Identity DevOps pingdirectoryproxy Hook - 03-build-run-plan.sh \u00b6 This script is called to check if there is an existing server and if so, it will return a 1, else 0 Goal of building a run plan is to provide a plan for the server as it starts up Options for the RUN_PLAN and the PD_STATE are as follows: RUN_PLAN (Initially set to UNKNOWN) START - Instructs the container to start from scratch. This is primarily because a server.uuid file is not present. RESTART - Instructs the container to restart an existing directory. This is primarily because an existing server.uuid file is prsent. PD_STATE (Initially set to UNKNOWN) SETUP - Specifies that the server should be setup UPDATE - Specifies that the server should be updated GENISIS - A very special case when the server is determined to be the SEED Server and initial server should be setup and data imported This document auto-generated from pingdirectoryproxy/hooks/03-build-run-plan.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectoryproxy` Hook - `03-build-run-plan.sh`"},{"location":"docker-images/pingdirectoryproxy/hooks/03-build-run-plan.sh/#ping-identity-devops-pingdirectoryproxy-hook-03-build-run-plansh","text":"This script is called to check if there is an existing server and if so, it will return a 1, else 0 Goal of building a run plan is to provide a plan for the server as it starts up Options for the RUN_PLAN and the PD_STATE are as follows: RUN_PLAN (Initially set to UNKNOWN) START - Instructs the container to start from scratch. This is primarily because a server.uuid file is not present. RESTART - Instructs the container to restart an existing directory. This is primarily because an existing server.uuid file is prsent. PD_STATE (Initially set to UNKNOWN) SETUP - Specifies that the server should be setup UPDATE - Specifies that the server should be updated GENISIS - A very special case when the server is determined to be the SEED Server and initial server should be setup and data imported This document auto-generated from pingdirectoryproxy/hooks/03-build-run-plan.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectoryproxy Hook - 03-build-run-plan.sh"},{"location":"docker-images/pingdirectoryproxy/hooks/183-run-setup.sh/","text":"Ping Identity DevOps pingdirectoryproxy Hook - 183-run-setup.sh \u00b6 This document auto-generated from pingdirectoryproxy/hooks/183-run-setup.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectoryproxy` Hook - `183-run-setup.sh`"},{"location":"docker-images/pingdirectoryproxy/hooks/183-run-setup.sh/#ping-identity-devops-pingdirectoryproxy-hook-183-run-setupsh","text":"This document auto-generated from pingdirectoryproxy/hooks/183-run-setup.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectoryproxy Hook - 183-run-setup.sh"},{"location":"docker-images/pingdirectoryproxy/hooks/188-apply-configuration.sh/","text":"Ping Identity DevOps pingdirectoryproxy Hook - 188-apply-configuration.sh \u00b6 This document auto-generated from pingdirectoryproxy/hooks/188-apply-configuration.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectoryproxy` Hook - `188-apply-configuration.sh`"},{"location":"docker-images/pingdirectoryproxy/hooks/188-apply-configuration.sh/#ping-identity-devops-pingdirectoryproxy-hook-188-apply-configurationsh","text":"This document auto-generated from pingdirectoryproxy/hooks/188-apply-configuration.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectoryproxy Hook - 188-apply-configuration.sh"},{"location":"docker-images/pingdirectoryproxy/hooks/20-restart-sequence.sh/","text":"Ping Identity DevOps pingdirectoryproxy Hook - 20-restart-sequence.sh \u00b6 This hook is called when the container has been built in a prior startup and a configuration has been found. This document auto-generated from pingdirectoryproxy/hooks/20-restart-sequence.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectoryproxy` Hook - `20-restart-sequence.sh`"},{"location":"docker-images/pingdirectoryproxy/hooks/20-restart-sequence.sh/#ping-identity-devops-pingdirectoryproxy-hook-20-restart-sequencesh","text":"This hook is called when the container has been built in a prior startup and a configuration has been found. This document auto-generated from pingdirectoryproxy/hooks/20-restart-sequence.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectoryproxy Hook - 20-restart-sequence.sh"},{"location":"docker-images/pingdirectoryproxy/hooks/80-post-start.sh/","text":"Ping Identity DevOps pingdirectoryproxy Hook - 80-post-start.sh \u00b6 This hook runs through the followig phases: * Ensures the PingDirectoryProxy service has been started an accepts queries. * Updates the Server Instance hostname/ldaps-port This document auto-generated from pingdirectoryproxy/hooks/80-post-start.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectoryproxy` Hook - `80-post-start.sh`"},{"location":"docker-images/pingdirectoryproxy/hooks/80-post-start.sh/#ping-identity-devops-pingdirectoryproxy-hook-80-post-startsh","text":"This hook runs through the followig phases: * Ensures the PingDirectoryProxy service has been started an accepts queries. * Updates the Server Instance hostname/ldaps-port This document auto-generated from pingdirectoryproxy/hooks/80-post-start.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectoryproxy Hook - 80-post-start.sh"},{"location":"docker-images/pingdirectoryproxy/hooks/81-generate-topology-json.sh/","text":"Ping Identity DevOps pingdirectoryproxy Hook - 81-generate-topology-json.sh \u00b6 This document auto-generated from pingdirectoryproxy/hooks/81-generate-topology-json.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectoryproxy` Hook - `81-generate-topology-json.sh`"},{"location":"docker-images/pingdirectoryproxy/hooks/81-generate-topology-json.sh/#ping-identity-devops-pingdirectoryproxy-hook-81-generate-topology-jsonsh","text":"This document auto-generated from pingdirectoryproxy/hooks/81-generate-topology-json.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectoryproxy Hook - 81-generate-topology-json.sh"},{"location":"docker-images/pingdirectoryproxy/hooks/81-repair-topology.sh/","text":"Ping Identity DevOps pingdirectoryproxy Hook - 81-repair-topology.sh \u00b6 This document auto-generated from pingdirectoryproxy/hooks/81-repair-topology.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectoryproxy` Hook - `81-repair-topology.sh`"},{"location":"docker-images/pingdirectoryproxy/hooks/81-repair-topology.sh/#ping-identity-devops-pingdirectoryproxy-hook-81-repair-topologysh","text":"This document auto-generated from pingdirectoryproxy/hooks/81-repair-topology.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectoryproxy Hook - 81-repair-topology.sh"},{"location":"docker-images/pingdirectoryproxy/hooks/pingdirectory.lib.sh/","text":"Ping Identity DevOps pingdirectoryproxy Hook - pingdirectory.lib.sh \u00b6 This document auto-generated from pingdirectoryproxy/hooks/pingdirectory.lib.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdirectoryproxy` Hook - `pingdirectory.lib.sh`"},{"location":"docker-images/pingdirectoryproxy/hooks/pingdirectory.lib.sh/#ping-identity-devops-pingdirectoryproxy-hook-pingdirectorylibsh","text":"This document auto-generated from pingdirectoryproxy/hooks/pingdirectory.lib.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdirectoryproxy Hook - pingdirectory.lib.sh"},{"location":"docker-images/pingdownloader/","text":"Ping Identity Docker Image - pingdownloader \u00b6 This docker image provides an alpine image to help with the download of Ping product images and licenses. Related Docker Images \u00b6 alpine - Parent Image Usage \u00b6 docker run --env PING_IDENTITY_DEVOPS_USER --env PING_IDENTITY_DEVOPS_KEY pingidentity/pingdownloader -p <product_name> Options \u00b6 -v, --version: the version of the product to download. by default, the downloader will pull the latest version -c, --conserve-name: use this option to conserve the original file name. By default, the downloader will rename the file product.zip -n, --dry-run: this will cause the URL to be displayed but the the bits not to be downloaded Examples \u00b6 Download the latest PingDirectory docker run --env PING_IDENTITY_DEVOPS_USER --env PING_IDENTITY_DEVOPS_KEY pingidentity/pingdownloader -p PingDirectory Download a specific version of PingDirectory docker run --env PING_IDENTITY_DEVOPS_USER --env PING_IDENTITY_DEVOPS_KEY pingidentity/pingdownloader -p PingDirectory -v 7.3.0.0 Download a product to /tmp on the host, as opposed to /tmp in the PingDownloader container docker run --env PING_IDENTITY_DEVOPS_USER --env PING_IDENTITY_DEVOPS_KEY --rm -v /tmp:/tmp pingidentity/pingdownloader -p PingFederate Docker Container Hook Scripts \u00b6 Please go here for details on all pingdownloader hook scripts This document is auto-generated from pingdownloader/Dockerfile Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"PingDownloader"},{"location":"docker-images/pingdownloader/#ping-identity-docker-image-pingdownloader","text":"This docker image provides an alpine image to help with the download of Ping product images and licenses.","title":"Ping Identity Docker Image - pingdownloader"},{"location":"docker-images/pingdownloader/#related-docker-images","text":"alpine - Parent Image","title":"Related Docker Images"},{"location":"docker-images/pingdownloader/#usage","text":"docker run --env PING_IDENTITY_DEVOPS_USER --env PING_IDENTITY_DEVOPS_KEY pingidentity/pingdownloader -p <product_name>","title":"Usage"},{"location":"docker-images/pingdownloader/#options","text":"-v, --version: the version of the product to download. by default, the downloader will pull the latest version -c, --conserve-name: use this option to conserve the original file name. By default, the downloader will rename the file product.zip -n, --dry-run: this will cause the URL to be displayed but the the bits not to be downloaded","title":"Options"},{"location":"docker-images/pingdownloader/#examples","text":"Download the latest PingDirectory docker run --env PING_IDENTITY_DEVOPS_USER --env PING_IDENTITY_DEVOPS_KEY pingidentity/pingdownloader -p PingDirectory Download a specific version of PingDirectory docker run --env PING_IDENTITY_DEVOPS_USER --env PING_IDENTITY_DEVOPS_KEY pingidentity/pingdownloader -p PingDirectory -v 7.3.0.0 Download a product to /tmp on the host, as opposed to /tmp in the PingDownloader container docker run --env PING_IDENTITY_DEVOPS_USER --env PING_IDENTITY_DEVOPS_KEY --rm -v /tmp:/tmp pingidentity/pingdownloader -p PingFederate","title":"Examples"},{"location":"docker-images/pingdownloader/#docker-container-hook-scripts","text":"Please go here for details on all pingdownloader hook scripts This document is auto-generated from pingdownloader/Dockerfile Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Docker Container Hook Scripts"},{"location":"docker-images/pingdownloader/hooks/","text":"Ping Identity DevOps pingdownloader Hooks \u00b6 There are no default hooks defined for the pingdownloader image. Hooks defined by parent images (i.e. pingcommon/pingdatacommon) will be inherited by this image. Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingdownloader` Hooks"},{"location":"docker-images/pingdownloader/hooks/#ping-identity-devops-pingdownloader-hooks","text":"There are no default hooks defined for the pingdownloader image. Hooks defined by parent images (i.e. pingcommon/pingdatacommon) will be inherited by this image. Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingdownloader Hooks"},{"location":"docker-images/pingfederate/","text":"Ping Identity DevOps Docker Image - pingfederate \u00b6 This docker image includes the Ping Identity PingFederate product binaries and associated hook scripts to create and run both PingFederate Admin and Engine nodes. Related Docker Images \u00b6 pingidentity/pingbase - Parent Image > This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingcommon - Common Ping files (i.e. hook scripts) pingidentity/pingdownloader - Used to download product bits Environment Variables \u00b6 In addition to environment variables inherited from pingidentity/pingbase , the following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} PING_PRODUCT PingFederate LICENSE_DIR ${SERVER_ROOT_DIR}/server/default/conf LICENSE_FILE_NAME pingfederate.lic LICENSE_SHORT_NAME PF LICENSE_VERSION ${LICENSE_VERSION} STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/run.sh TAIL_LOG_FILES ${SERVER_ROOT_DIR}/log/server.log PF_LOG_SIZE_MAX 10000 KB Defines the log file size max for ALL appenders PF_LOG_NUMBER 2 Defines the maximum of log files to retain upon rotation PF_ADMIN_PORT 9999 Defines the port on which the PingFederate administrative console and API runs. PF_ENGINE_PORT 9031 Defines the port on which PingFederate listens for encrypted HTTPS (SSL/TLS) traffic. PF_ENGINE_DEBUG false Flag to turn on PingFederate Engine debugging Used in run.sh PF_ADMIN_DEBUG false Flag to turn on PingFederate Admin debugging Used in run.sh PF_DEBUG_PORT 9030 Defines the port on which PingFederate opens up a java debugging port. Used in run.sh OPERATIONAL_MODE STANDALONE Operational Mode Indicates the operational mode of the runtime server in run.properties Options include STANDALONE, CLUSTERED_CONSOLE, CLUSTERED_ENGINE. PF_CONSOLE_AUTHENTICATION Defines mechanism for console authentication in run.properties. Options include none, native, LDAP, cert, RADIUS, OIDC. If not set, default is native. PF_ADMIN_API_AUTHENTICATION Defines mechanism for admin api authentication in run.properties. Options include none, native, LDAP, cert, RADIUS, OIDC. If not set, default is native. HSM_MODE OFF Hardware Security Module Mode in run.properties Options include OFF, AWSCLOUDHSM, NCIPHER, LUNA, BCFIPS. PF_LDAP_USERNAME cn=pingfederate This is the username for an account within the LDAP Directory Server that can be used to perform user lookups for authentication and other user level search operations. Set if PF_CONSOLE_AUTHENTICATION or PF_ADMIN_API_AUTHENTICATION=LDAP PF_LDAP_PASSWORD OBF:JWE:eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2Iiwia2lkIjoiRW1JY1UxOVdueSIsInZlcnNpb24iOiI5LjIuMS4xIn0..euBO0bawJz3XC_plAjxECg.yF7BpnCTPZlpZUo21WQ5IQ.YlLtlJTxXhrp3LsxyQDo5g This is the password for the Username specified above. This property should be obfuscated using the 'obfuscate.sh' utility. Set if PF_CONSOLE_AUTHENTICATION or PF_ADMIN_API_AUTHENTICATION=LDAP CLUSTER_BIND_ADDRESS NON_LOOPBACK IP address for cluster communication. Set to NON_LOOPBACK to allow the system to choose an available non-loopback IP address. PF_PROVISIONER_MODE OFF Provisioner Mode in run.properties Options include OFF, STANDALONE, FAILOVER. PF_PROVISIONER_NODE_ID 1 Provisioner Node ID in run.properties Initial active provisioning server node ID is 1 PF_PROVISIONER_GRACE_PERIOD 600 Provisioner Failover Grace Period in run.properties Grace period, in seconds. Default 600 seconds BULK_CONFIG_DIR ${OUT_DIR}/instance/bulk-config BULK_CONFIG_FILE data.json ## Ports Exposed The following ports are exposed from the container. If a variable is used, then it may come from a parent container - 9031 - 9999 Running a PingFederate container \u00b6 To run a PingFederate container: docker run \\ --name pingfederate \\ --publish 9999 :9999 \\ --detach \\ --env SERVER_PROFILE_URL = https://github.com/pingidentity/pingidentity-server-profiles.git \\ --env SERVER_PROFILE_PATH = getting-started/pingfederate \\ --env PING_IDENTITY_ACCEPT_EULA = YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingfederate:edge Follow Docker logs with: docker logs -f pingfederate If using the command above with the embedded server profile , log in with: * https://localhost:9999/pingfederate/app * Username: Administrator * Password: 2FederateM0re Docker Container Hook Scripts \u00b6 Please go here for details on all pingfederate hook scripts This document is auto-generated from pingfederate/Dockerfile Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"PingFederate"},{"location":"docker-images/pingfederate/#ping-identity-devops-docker-image-pingfederate","text":"This docker image includes the Ping Identity PingFederate product binaries and associated hook scripts to create and run both PingFederate Admin and Engine nodes.","title":"Ping Identity DevOps Docker Image - pingfederate"},{"location":"docker-images/pingfederate/#related-docker-images","text":"pingidentity/pingbase - Parent Image > This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingcommon - Common Ping files (i.e. hook scripts) pingidentity/pingdownloader - Used to download product bits","title":"Related Docker Images"},{"location":"docker-images/pingfederate/#environment-variables","text":"In addition to environment variables inherited from pingidentity/pingbase , the following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} PING_PRODUCT PingFederate LICENSE_DIR ${SERVER_ROOT_DIR}/server/default/conf LICENSE_FILE_NAME pingfederate.lic LICENSE_SHORT_NAME PF LICENSE_VERSION ${LICENSE_VERSION} STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/run.sh TAIL_LOG_FILES ${SERVER_ROOT_DIR}/log/server.log PF_LOG_SIZE_MAX 10000 KB Defines the log file size max for ALL appenders PF_LOG_NUMBER 2 Defines the maximum of log files to retain upon rotation PF_ADMIN_PORT 9999 Defines the port on which the PingFederate administrative console and API runs. PF_ENGINE_PORT 9031 Defines the port on which PingFederate listens for encrypted HTTPS (SSL/TLS) traffic. PF_ENGINE_DEBUG false Flag to turn on PingFederate Engine debugging Used in run.sh PF_ADMIN_DEBUG false Flag to turn on PingFederate Admin debugging Used in run.sh PF_DEBUG_PORT 9030 Defines the port on which PingFederate opens up a java debugging port. Used in run.sh OPERATIONAL_MODE STANDALONE Operational Mode Indicates the operational mode of the runtime server in run.properties Options include STANDALONE, CLUSTERED_CONSOLE, CLUSTERED_ENGINE. PF_CONSOLE_AUTHENTICATION Defines mechanism for console authentication in run.properties. Options include none, native, LDAP, cert, RADIUS, OIDC. If not set, default is native. PF_ADMIN_API_AUTHENTICATION Defines mechanism for admin api authentication in run.properties. Options include none, native, LDAP, cert, RADIUS, OIDC. If not set, default is native. HSM_MODE OFF Hardware Security Module Mode in run.properties Options include OFF, AWSCLOUDHSM, NCIPHER, LUNA, BCFIPS. PF_LDAP_USERNAME cn=pingfederate This is the username for an account within the LDAP Directory Server that can be used to perform user lookups for authentication and other user level search operations. Set if PF_CONSOLE_AUTHENTICATION or PF_ADMIN_API_AUTHENTICATION=LDAP PF_LDAP_PASSWORD OBF:JWE:eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2Iiwia2lkIjoiRW1JY1UxOVdueSIsInZlcnNpb24iOiI5LjIuMS4xIn0..euBO0bawJz3XC_plAjxECg.yF7BpnCTPZlpZUo21WQ5IQ.YlLtlJTxXhrp3LsxyQDo5g This is the password for the Username specified above. This property should be obfuscated using the 'obfuscate.sh' utility. Set if PF_CONSOLE_AUTHENTICATION or PF_ADMIN_API_AUTHENTICATION=LDAP CLUSTER_BIND_ADDRESS NON_LOOPBACK IP address for cluster communication. Set to NON_LOOPBACK to allow the system to choose an available non-loopback IP address. PF_PROVISIONER_MODE OFF Provisioner Mode in run.properties Options include OFF, STANDALONE, FAILOVER. PF_PROVISIONER_NODE_ID 1 Provisioner Node ID in run.properties Initial active provisioning server node ID is 1 PF_PROVISIONER_GRACE_PERIOD 600 Provisioner Failover Grace Period in run.properties Grace period, in seconds. Default 600 seconds BULK_CONFIG_DIR ${OUT_DIR}/instance/bulk-config BULK_CONFIG_FILE data.json ## Ports Exposed The following ports are exposed from the container. If a variable is used, then it may come from a parent container - 9031 - 9999","title":"Environment Variables"},{"location":"docker-images/pingfederate/#running-a-pingfederate-container","text":"To run a PingFederate container: docker run \\ --name pingfederate \\ --publish 9999 :9999 \\ --detach \\ --env SERVER_PROFILE_URL = https://github.com/pingidentity/pingidentity-server-profiles.git \\ --env SERVER_PROFILE_PATH = getting-started/pingfederate \\ --env PING_IDENTITY_ACCEPT_EULA = YES \\ --env PING_IDENTITY_DEVOPS_USER \\ --env PING_IDENTITY_DEVOPS_KEY \\ --tmpfs /run/secrets \\ pingidentity/pingfederate:edge Follow Docker logs with: docker logs -f pingfederate If using the command above with the embedded server profile , log in with: * https://localhost:9999/pingfederate/app * Username: Administrator * Password: 2FederateM0re","title":"Running a PingFederate container"},{"location":"docker-images/pingfederate/#docker-container-hook-scripts","text":"Please go here for details on all pingfederate hook scripts This document is auto-generated from pingfederate/Dockerfile Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Docker Container Hook Scripts"},{"location":"docker-images/pingfederate/hooks/","text":"Ping Identity DevOps pingfederate Hooks \u00b6 List of available hooks: * 80-post-start.sh * 81-after-start-process.sh * 83-configure-admin.sh * 85-import-configuration.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from pingfederate/opt/staging/hooks Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingfederate` Hooks"},{"location":"docker-images/pingfederate/hooks/#ping-identity-devops-pingfederate-hooks","text":"List of available hooks: * 80-post-start.sh * 81-after-start-process.sh * 83-configure-admin.sh * 85-import-configuration.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from pingfederate/opt/staging/hooks Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingfederate Hooks"},{"location":"docker-images/pingfederate/hooks/80-post-start.sh/","text":"Ping Identity DevOps pingfederate Hook - 80-post-start.sh \u00b6 This script is used to import any configurations that are needed after PingFederate starts This document is auto-generated from pingfederate/opt/staging/hooks/80-post-start.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingfederate` Hook - `80-post-start.sh`"},{"location":"docker-images/pingfederate/hooks/80-post-start.sh/#ping-identity-devops-pingfederate-hook-80-post-startsh","text":"This script is used to import any configurations that are needed after PingFederate starts This document is auto-generated from pingfederate/opt/staging/hooks/80-post-start.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingfederate Hook - 80-post-start.sh"},{"location":"docker-images/pingfederate/hooks/81-after-start-process.sh/","text":"Ping Identity DevOps pingfederate Hook - 81-after-start-process.sh \u00b6 This document is auto-generated from pingfederate/opt/staging/hooks/81-after-start-process.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingfederate` Hook - `81-after-start-process.sh`"},{"location":"docker-images/pingfederate/hooks/81-after-start-process.sh/#ping-identity-devops-pingfederate-hook-81-after-start-processsh","text":"This document is auto-generated from pingfederate/opt/staging/hooks/81-after-start-process.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingfederate Hook - 81-after-start-process.sh"},{"location":"docker-images/pingfederate/hooks/83-configure-admin.sh/","text":"Ping Identity DevOps pingfederate Hook - 83-configure-admin.sh \u00b6 This document is auto-generated from pingfederate/opt/staging/hooks/83-configure-admin.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingfederate` Hook - `83-configure-admin.sh`"},{"location":"docker-images/pingfederate/hooks/83-configure-admin.sh/#ping-identity-devops-pingfederate-hook-83-configure-adminsh","text":"This document is auto-generated from pingfederate/opt/staging/hooks/83-configure-admin.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingfederate Hook - 83-configure-admin.sh"},{"location":"docker-images/pingfederate/hooks/85-import-configuration.sh/","text":"Ping Identity DevOps pingfederate Hook - 85-import-configuration.sh \u00b6 This document is auto-generated from pingfederate/opt/staging/hooks/85-import-configuration.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingfederate` Hook - `85-import-configuration.sh`"},{"location":"docker-images/pingfederate/hooks/85-import-configuration.sh/#ping-identity-devops-pingfederate-hook-85-import-configurationsh","text":"This document is auto-generated from pingfederate/opt/staging/hooks/85-import-configuration.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingfederate Hook - 85-import-configuration.sh"},{"location":"docker-images/pingintelligence/","text":"Ping Identity DevOps Docker Image - pingintelligence-ase \u00b6 This docker image includes the Ping Identity PingIntelligence API Security Enforcer product binaries and associated hook scripts to create and run PingIntelligence ASE instances. Related Docker Images \u00b6 pingidentity/pingbase - Parent Image > This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingcommon - Common Ping files (i.e. hook scripts) pingidentity/pingdownloader - Used to download product bits Environment Variables \u00b6 In addition to environment variables inherited from pingidentity/pingbase , the following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} PING_PRODUCT PingIntelligence_ASE PingIdentity license version Ping product name LICENSE_FILE_NAME PingIntelligence.lic Name of License File LICENSE_DIR ${SERVER_ROOT_DIR}/config LICENSE_SHORT_NAME pingintelligence Shortname used when retrieving license from License Server LICENSE_VERSION ${LICENSE_VERSION} Version used when retrieving license from License Server STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/start_ase.sh STARTUP_FOREGROUND_OPTS STARTUP_BACKGROUND_OPTS ROOT_USER_PASSWORD_FILE ADMIN_USER_PASSWORD_FILE ENCRYPTION_PASSWORD_FILE PING_INTELLIGENCE_ADMIN_USER admin PingIntelligence global variables PingIntelligence default administrative user (this should probably not be changed) PING_INTELLIGENCE_ADMIN_PASSWORD 2FederateM0re PingIntelligence default administrative user credentials (this should be changed) PING_INTELLIGENCE_ASE_HTTP_PORT 8000 The ASE HTTP listener port PING_INTELLIGENCE_ASE_HTTPS_PORT 8443 The ASE HTTPS listener port PING_INTELLIGENCE_ASE_MGMT_PORT 8010 the ASE management port PING_INTELLIGENCE_ASE_TIMEZONE local The timezone the ASE container is operating in PING_INTELLIGENCE_ASE_MODE inline Defines running mode for API Security Enforcer (Allowed values are inline or sideband). PING_INTELLIGENCE_ASE_ENABLE_SIDEBAND_AUTHENTICATION false Enable client-side authentication with tokens in sideband mode PING_INTELLIGENCE_ASE_HOSTNAME_REWRITE false PING_INTELLIGENCE_ASE_KEYSTORE_PASSWORD OBF:AES:sRNp0W7sSi1zrReXeHodKQ:lXcvbBhKZgDTrjQOfOkzR2mpca4bTUcwPAuerMPwvM4 PING_INTELLIGENCE_ASE_ADMIN_LOG_LEVEL 4 For controller.log and balancer.log only 1-5 (FATAL, ERROR, WARNING, INFO, DEBUG) PING_INTELLIGENCE_ASE_ENABLE_CLUSTER false enable cluster PING_INTELLIGENCE_ASE_SYSLOG_SERVER Syslog server PING_INTELLIGENCE_ASE_CA_CERT_PATH Path the to CA certificate PING_INTELLIGENCE_ASE_ENABLE_HEALTH false enable the ASE health check service PING_INTELLIGENCE_ASE_ENABLE_ABS false Set this value to true, to allow API Security Enforcer to send logs to ABS. PING_INTELLIGENCE_ASE_ENABLE_ABS_ATTACK_LIST_RETRIEVAL false Toggle ABS attack list retrieval PING_INTELLIGENCE_ASE_BLOCK_AUTODETECTED_ATTACKS false Toggle whether ASE blocks auto-detected attacks PING_INTELLIGENCE_ASE_ATTACK_LIST_REFRESH_MINUTES 10 ABS attack list retieval frequency in minutes PING_INTELLIGENCE_ASE_HOSTNAME_REFRESH_SECONDS 60 Hostname refresh interval in seconds PING_INTELLIGENCE_ASE_DECOY_ALERT_INTERVAL_MINUTES 180 Alert interval for teh decoy services PING_INTELLIGENCE_ASE_ENABLE_XFOWARDED_FOR false Toggle X-Forwarded-For PING_INTELLIGENCE_ASE_ENABLE_FIREWALL true Toggle ASE Firewall PING_INTELLIGENCE_ASE_ENABLE_SIDEBAND_KEEPALIVE false Enable connection keepalive for requests from gateway to ASE in sideband mode When enabled, ASE sends 'Connection: keep-alive' header in response When disabled, ASE sends 'Connection: close' header in response PING_INTELLIGENCE_ASE_ENABLE_GOOGLE_PUBSUB false Enable Google Pub/Sub PING_INTELLIGENCE_ASE_ENABLE_ACCESS_LOG true Toggle the access log PING_INTELLIGENCE_ASE_ENABLE_AUDIT false Toggle audit logging PING_INTELLIGENCE_ASE_FLUSH_LOG_IMMEDIATELY true Toggle whether logs are flushed to disk immediately PING_INTELLIGENCE_ASE_HTTP_PROCESS 1 The number of processes for HTTP requests PING_INTELLIGENCE_ASE_HTTPS_PROCESS 1 The number of processes for HTTPS requests PING_INTELLIGENCE_ASE_ENABLE_SSL_V3 false Toggle SSLv3 -- this should absolutely stay disabled PING_INTELLIGENCE_ASE_GOOGLE_PUBSUB_TOPIC /topic/apimetrics Google Pub/Sub topic PING_INTELLIGENCE_ASE_GOOGLE_PUBSUB_CONCURRENCY 1000 Number of concurrent connections to Google Pub/Sub (Min:1, Max:1024, default: 1000) PING_INTELLIGENCE_ASE_GOOGLE_PUBSUB_QPS 1000 Throttle the number of messages published per second. (Min: 1, Max:10000, default:1000) PING_INTELLIGENCE_ASE_GOOGLE_PUBSUB_APIKEY The API key to use to authenticate with Google PING_INTELLIGENCE_ASE_CACHE_QUEUE_SIZE 300 Maximum number of messages buffered in memory (Min: 1, Max: 10000, Default: 300) PING_INTELLIGENCE_ASE_GOOGLE_PUBSUB_TIMEOUT_SECONDS 30 Timeout in seconds to publish a message to Google Pub/Sub. (Min: 10, Max: 300, Default: 30) PING_INTELLIGENCE_TCP_SEND_BUFFER_BYTES 212992 Kernel TCP send buffer size in bytes PING_INTELLIGENCE_TCP_RECEIVE_BUFFER_BYTES 212992 enrel TCP receive buffer size in bytes PING_INTELLIGENCE_ASE_ATTACK_LIST_MEMORY 128MB PING_INTELLIGENCE_CLUSTER_PEER_NODE_CSV_LIST a comma-separated list of hostname:cluster_manager_port or IPv4_address:cluster_manager_port the ASE will try to connect to each server peer in the list PING_INTELLIGENCE_CLUSTER_ID ase_cluster The ASE cluster ID -- this must be unique PING_INTELLIGENCE_CLUSTER_MGMT_PORT 8020 The ASE cluster management port PING_INTELLIGENCE_CLUSTER_SECRET_KEY OBF:AES:nPJOh3wXQWK/BOHrtKu3G2SGiAEElOSvOFYEiWfIVSdummoFwSR8rDh2bBnhTDdJ:7LFcqXQlqkW9kldQoFg0nJoLSojnzHDbD3iAy84pT84 Secret key required to join the cluster PING_INTELLIGENCE_ABS_ENDPOINT a comma-separated list of abs nodes having hostname:port or ipv4:port as an address. PING_INTELLIGENCE_ABS_ACCESS_KEY access key for ase to authenticate with abs node PING_INTELLIGENCE_ABS_SECRET_KEY secret key for ase to authenticate with abs node PING_INTELLIGENCE_ABS_ENABLE_SSL true Setting this value to true will enable encrypted communication with ABS. PING_INTELLIGENCE_ABS_CA_CERT_PATH Configure the location of ABS's trusted CA certificates. TAIL_LOG_FILES Files tailed once container has started ## Docker Container Hook Scripts Please go here for details on all pingintelligence hook scripts This document is auto-generated from pingintelligence/Dockerfile Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"PingIntelligence"},{"location":"docker-images/pingintelligence/#ping-identity-devops-docker-image-pingintelligence-ase","text":"This docker image includes the Ping Identity PingIntelligence API Security Enforcer product binaries and associated hook scripts to create and run PingIntelligence ASE instances.","title":"Ping Identity DevOps Docker Image - pingintelligence-ase"},{"location":"docker-images/pingintelligence/#related-docker-images","text":"pingidentity/pingbase - Parent Image > This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingcommon - Common Ping files (i.e. hook scripts) pingidentity/pingdownloader - Used to download product bits","title":"Related Docker Images"},{"location":"docker-images/pingintelligence/#environment-variables","text":"In addition to environment variables inherited from pingidentity/pingbase , the following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} PING_PRODUCT PingIntelligence_ASE PingIdentity license version Ping product name LICENSE_FILE_NAME PingIntelligence.lic Name of License File LICENSE_DIR ${SERVER_ROOT_DIR}/config LICENSE_SHORT_NAME pingintelligence Shortname used when retrieving license from License Server LICENSE_VERSION ${LICENSE_VERSION} Version used when retrieving license from License Server STARTUP_COMMAND ${SERVER_ROOT_DIR}/bin/start_ase.sh STARTUP_FOREGROUND_OPTS STARTUP_BACKGROUND_OPTS ROOT_USER_PASSWORD_FILE ADMIN_USER_PASSWORD_FILE ENCRYPTION_PASSWORD_FILE PING_INTELLIGENCE_ADMIN_USER admin PingIntelligence global variables PingIntelligence default administrative user (this should probably not be changed) PING_INTELLIGENCE_ADMIN_PASSWORD 2FederateM0re PingIntelligence default administrative user credentials (this should be changed) PING_INTELLIGENCE_ASE_HTTP_PORT 8000 The ASE HTTP listener port PING_INTELLIGENCE_ASE_HTTPS_PORT 8443 The ASE HTTPS listener port PING_INTELLIGENCE_ASE_MGMT_PORT 8010 the ASE management port PING_INTELLIGENCE_ASE_TIMEZONE local The timezone the ASE container is operating in PING_INTELLIGENCE_ASE_MODE inline Defines running mode for API Security Enforcer (Allowed values are inline or sideband). PING_INTELLIGENCE_ASE_ENABLE_SIDEBAND_AUTHENTICATION false Enable client-side authentication with tokens in sideband mode PING_INTELLIGENCE_ASE_HOSTNAME_REWRITE false PING_INTELLIGENCE_ASE_KEYSTORE_PASSWORD OBF:AES:sRNp0W7sSi1zrReXeHodKQ:lXcvbBhKZgDTrjQOfOkzR2mpca4bTUcwPAuerMPwvM4 PING_INTELLIGENCE_ASE_ADMIN_LOG_LEVEL 4 For controller.log and balancer.log only 1-5 (FATAL, ERROR, WARNING, INFO, DEBUG) PING_INTELLIGENCE_ASE_ENABLE_CLUSTER false enable cluster PING_INTELLIGENCE_ASE_SYSLOG_SERVER Syslog server PING_INTELLIGENCE_ASE_CA_CERT_PATH Path the to CA certificate PING_INTELLIGENCE_ASE_ENABLE_HEALTH false enable the ASE health check service PING_INTELLIGENCE_ASE_ENABLE_ABS false Set this value to true, to allow API Security Enforcer to send logs to ABS. PING_INTELLIGENCE_ASE_ENABLE_ABS_ATTACK_LIST_RETRIEVAL false Toggle ABS attack list retrieval PING_INTELLIGENCE_ASE_BLOCK_AUTODETECTED_ATTACKS false Toggle whether ASE blocks auto-detected attacks PING_INTELLIGENCE_ASE_ATTACK_LIST_REFRESH_MINUTES 10 ABS attack list retieval frequency in minutes PING_INTELLIGENCE_ASE_HOSTNAME_REFRESH_SECONDS 60 Hostname refresh interval in seconds PING_INTELLIGENCE_ASE_DECOY_ALERT_INTERVAL_MINUTES 180 Alert interval for teh decoy services PING_INTELLIGENCE_ASE_ENABLE_XFOWARDED_FOR false Toggle X-Forwarded-For PING_INTELLIGENCE_ASE_ENABLE_FIREWALL true Toggle ASE Firewall PING_INTELLIGENCE_ASE_ENABLE_SIDEBAND_KEEPALIVE false Enable connection keepalive for requests from gateway to ASE in sideband mode When enabled, ASE sends 'Connection: keep-alive' header in response When disabled, ASE sends 'Connection: close' header in response PING_INTELLIGENCE_ASE_ENABLE_GOOGLE_PUBSUB false Enable Google Pub/Sub PING_INTELLIGENCE_ASE_ENABLE_ACCESS_LOG true Toggle the access log PING_INTELLIGENCE_ASE_ENABLE_AUDIT false Toggle audit logging PING_INTELLIGENCE_ASE_FLUSH_LOG_IMMEDIATELY true Toggle whether logs are flushed to disk immediately PING_INTELLIGENCE_ASE_HTTP_PROCESS 1 The number of processes for HTTP requests PING_INTELLIGENCE_ASE_HTTPS_PROCESS 1 The number of processes for HTTPS requests PING_INTELLIGENCE_ASE_ENABLE_SSL_V3 false Toggle SSLv3 -- this should absolutely stay disabled PING_INTELLIGENCE_ASE_GOOGLE_PUBSUB_TOPIC /topic/apimetrics Google Pub/Sub topic PING_INTELLIGENCE_ASE_GOOGLE_PUBSUB_CONCURRENCY 1000 Number of concurrent connections to Google Pub/Sub (Min:1, Max:1024, default: 1000) PING_INTELLIGENCE_ASE_GOOGLE_PUBSUB_QPS 1000 Throttle the number of messages published per second. (Min: 1, Max:10000, default:1000) PING_INTELLIGENCE_ASE_GOOGLE_PUBSUB_APIKEY The API key to use to authenticate with Google PING_INTELLIGENCE_ASE_CACHE_QUEUE_SIZE 300 Maximum number of messages buffered in memory (Min: 1, Max: 10000, Default: 300) PING_INTELLIGENCE_ASE_GOOGLE_PUBSUB_TIMEOUT_SECONDS 30 Timeout in seconds to publish a message to Google Pub/Sub. (Min: 10, Max: 300, Default: 30) PING_INTELLIGENCE_TCP_SEND_BUFFER_BYTES 212992 Kernel TCP send buffer size in bytes PING_INTELLIGENCE_TCP_RECEIVE_BUFFER_BYTES 212992 enrel TCP receive buffer size in bytes PING_INTELLIGENCE_ASE_ATTACK_LIST_MEMORY 128MB PING_INTELLIGENCE_CLUSTER_PEER_NODE_CSV_LIST a comma-separated list of hostname:cluster_manager_port or IPv4_address:cluster_manager_port the ASE will try to connect to each server peer in the list PING_INTELLIGENCE_CLUSTER_ID ase_cluster The ASE cluster ID -- this must be unique PING_INTELLIGENCE_CLUSTER_MGMT_PORT 8020 The ASE cluster management port PING_INTELLIGENCE_CLUSTER_SECRET_KEY OBF:AES:nPJOh3wXQWK/BOHrtKu3G2SGiAEElOSvOFYEiWfIVSdummoFwSR8rDh2bBnhTDdJ:7LFcqXQlqkW9kldQoFg0nJoLSojnzHDbD3iAy84pT84 Secret key required to join the cluster PING_INTELLIGENCE_ABS_ENDPOINT a comma-separated list of abs nodes having hostname:port or ipv4:port as an address. PING_INTELLIGENCE_ABS_ACCESS_KEY access key for ase to authenticate with abs node PING_INTELLIGENCE_ABS_SECRET_KEY secret key for ase to authenticate with abs node PING_INTELLIGENCE_ABS_ENABLE_SSL true Setting this value to true will enable encrypted communication with ABS. PING_INTELLIGENCE_ABS_CA_CERT_PATH Configure the location of ABS's trusted CA certificates. TAIL_LOG_FILES Files tailed once container has started ## Docker Container Hook Scripts Please go here for details on all pingintelligence hook scripts This document is auto-generated from pingintelligence/Dockerfile Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Environment Variables"},{"location":"docker-images/pingintelligence/hooks/","text":"Ping Identity DevOps pingintelligence Hooks \u00b6 List of available hooks: * 50-before-post-start.sh * 80-post-start.sh * pingintelligence.lib.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from pingintelligence/opt/staging/hooks Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingintelligence` Hooks"},{"location":"docker-images/pingintelligence/hooks/#ping-identity-devops-pingintelligence-hooks","text":"List of available hooks: * 50-before-post-start.sh * 80-post-start.sh * pingintelligence.lib.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from pingintelligence/opt/staging/hooks Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingintelligence Hooks"},{"location":"docker-images/pingintelligence/hooks/50-before-post-start.sh/","text":"Ping Identity DevOps pingintelligence Hook - 50-before-post-start.sh \u00b6 This document is auto-generated from pingintelligence/opt/staging/hooks/50-before-post-start.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingintelligence` Hook - `50-before-post-start.sh`"},{"location":"docker-images/pingintelligence/hooks/50-before-post-start.sh/#ping-identity-devops-pingintelligence-hook-50-before-post-startsh","text":"This document is auto-generated from pingintelligence/opt/staging/hooks/50-before-post-start.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingintelligence Hook - 50-before-post-start.sh"},{"location":"docker-images/pingintelligence/hooks/80-post-start.sh/","text":"Ping Identity DevOps pingintelligence Hook - 80-post-start.sh \u00b6 This hook may be used to set the server if there is a setup procedure Note: The PingData (i.e. Directory, DataSync, DataGovernance, DirectoryProxy) products will all provide this This document is auto-generated from pingintelligence/opt/staging/hooks/80-post-start.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingintelligence` Hook - `80-post-start.sh`"},{"location":"docker-images/pingintelligence/hooks/80-post-start.sh/#ping-identity-devops-pingintelligence-hook-80-post-startsh","text":"This hook may be used to set the server if there is a setup procedure Note: The PingData (i.e. Directory, DataSync, DataGovernance, DirectoryProxy) products will all provide this This document is auto-generated from pingintelligence/opt/staging/hooks/80-post-start.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingintelligence Hook - 80-post-start.sh"},{"location":"docker-images/pingintelligence/hooks/pingintelligence.lib.sh/","text":"Ping Identity DevOps pingintelligence Hook - pingintelligence.lib.sh \u00b6 This document is auto-generated from pingintelligence/opt/staging/hooks/pingintelligence.lib.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingintelligence` Hook - `pingintelligence.lib.sh`"},{"location":"docker-images/pingintelligence/hooks/pingintelligence.lib.sh/#ping-identity-devops-pingintelligence-hook-pingintelligencelibsh","text":"This document is auto-generated from pingintelligence/opt/staging/hooks/pingintelligence.lib.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingintelligence Hook - pingintelligence.lib.sh"},{"location":"docker-images/pingtoolkit/","text":"Ping Identity DevOps Docker Image - pingtoolkit \u00b6 This docker image includes the Ping Identity PingToolkit and associated hook scripts to create a container that can pull in a SERVER_PROFILE run scripts. The typical use case of this image would be an init container or a pod/container to perform tasks aside a running set of pods/containers. Related Docker Images \u00b6 pingidentity/pingbase - Parent Image > This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingcommon - Common Ping files (i.e. hook scripts) Environment Variables \u00b6 In addition to environment variables inherited from pingidentity/pingbase , the following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} PING_PRODUCT PingToolkit STARTUP_COMMAND tail STARTUP_FOREGROUND_OPTS -f /dev/null ## Docker Container Hook Scripts Please go here for details on all pingtoolkit hook scripts This document is auto-generated from pingtoolkit/Dockerfile Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"PingToolkit"},{"location":"docker-images/pingtoolkit/#ping-identity-devops-docker-image-pingtoolkit","text":"This docker image includes the Ping Identity PingToolkit and associated hook scripts to create a container that can pull in a SERVER_PROFILE run scripts. The typical use case of this image would be an init container or a pod/container to perform tasks aside a running set of pods/containers.","title":"Ping Identity DevOps Docker Image - pingtoolkit"},{"location":"docker-images/pingtoolkit/#related-docker-images","text":"pingidentity/pingbase - Parent Image > This image inherits, and can use, Environment Variables from pingidentity/pingbase pingidentity/pingcommon - Common Ping files (i.e. hook scripts)","title":"Related Docker Images"},{"location":"docker-images/pingtoolkit/#environment-variables","text":"In addition to environment variables inherited from pingidentity/pingbase , the following environment ENV variables can be used with this image. ENV Variable Default Description SHIM ${SHIM} PING_PRODUCT PingToolkit STARTUP_COMMAND tail STARTUP_FOREGROUND_OPTS -f /dev/null ## Docker Container Hook Scripts Please go here for details on all pingtoolkit hook scripts This document is auto-generated from pingtoolkit/Dockerfile Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Environment Variables"},{"location":"docker-images/pingtoolkit/hooks/","text":"Ping Identity DevOps pingtoolkit Hooks \u00b6 List of available hooks: * 17-check-license.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from pingtoolkit/opt/staging/hooks Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingtoolkit` Hooks"},{"location":"docker-images/pingtoolkit/hooks/#ping-identity-devops-pingtoolkit-hooks","text":"List of available hooks: * 17-check-license.sh These hooks will replace hooks defined by parent images (i.e. pingcommon/pingdatacommon) This document is auto-generated from pingtoolkit/opt/staging/hooks Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingtoolkit Hooks"},{"location":"docker-images/pingtoolkit/hooks/17-check-license.sh/","text":"Ping Identity DevOps pingtoolkit Hook - 17-check-license.sh \u00b6 This document is auto-generated from pingtoolkit/opt/staging/hooks/17-check-license.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps `pingtoolkit` Hook - `17-check-license.sh`"},{"location":"docker-images/pingtoolkit/hooks/17-check-license.sh/#ping-identity-devops-pingtoolkit-hook-17-check-licensesh","text":"This document is auto-generated from pingtoolkit/opt/staging/hooks/17-check-license.sh Copyright (c) 2021 Ping Identity Corporation. All rights reserved.","title":"Ping Identity DevOps pingtoolkit Hook - 17-check-license.sh"},{"location":"get-started/devopsRegistration/","text":"Ping Identity DevOps Registration \u00b6 Registering for Ping Identity's DevOps Program grants you credentials that when supplied to product containers will automatically retrieve an evaluation license upon startup. Follow the steps listed below to register Ensure you have a registered account with Ping Identity. Not sure, click the link to Sign On and follow instructions. If you don't have an account, please create one. Otherwise, sign-in. When signing in select 'Support and Community' from the account type dropdown Once logged in, you'll be directed to your profile Page In the right-side menu, click the 'REGISTER FOR DEVOPS PROGRAM' button You'll receive a confirmation message. Your credentials will be forwarded to the email address associated with your Ping Identity account. Saving Credentials Upon receiving your key, ensure that you follow the instructions below for saving these via the ping-devops utility. Example: PING_IDENTITY_DEVOPS_USER=jsmith@example.com PING_IDENTITY_DEVOPS_KEY=e9bd26ac-17e9-4133-a981-d7a7509314b2 Saving Your DevOps User and Key \u00b6 The best way to save your DevOps User/Key is to use the Ping Identity DevOps utility ping-devops . ping-devops Setup Installation instructions for ping-devops can be found in the ping-devops Tool document. To save your DevOps credentials, run ping-devops config and supply your credentials when prompted. Once ping-devops is installed and configured it will place your DEVOPS USER/KEY into a Ping Identity property file found at ~/.pingidentity/devops . with the following variable names set (see example below). PING_IDENTITY_DEVOPS_USER=jsmith@example.com PING_IDENTITY_DEVOPS_KEY=e9bd26ac-17e9-4133-a981-d7a7509314b2 You can always view these settings with the ping-devops info command after you've configured them.","title":"DevOps Registration"},{"location":"get-started/devopsRegistration/#ping-identity-devops-registration","text":"Registering for Ping Identity's DevOps Program grants you credentials that when supplied to product containers will automatically retrieve an evaluation license upon startup. Follow the steps listed below to register Ensure you have a registered account with Ping Identity. Not sure, click the link to Sign On and follow instructions. If you don't have an account, please create one. Otherwise, sign-in. When signing in select 'Support and Community' from the account type dropdown Once logged in, you'll be directed to your profile Page In the right-side menu, click the 'REGISTER FOR DEVOPS PROGRAM' button You'll receive a confirmation message. Your credentials will be forwarded to the email address associated with your Ping Identity account. Saving Credentials Upon receiving your key, ensure that you follow the instructions below for saving these via the ping-devops utility. Example: PING_IDENTITY_DEVOPS_USER=jsmith@example.com PING_IDENTITY_DEVOPS_KEY=e9bd26ac-17e9-4133-a981-d7a7509314b2","title":"Ping Identity DevOps Registration"},{"location":"get-started/devopsRegistration/#saving-your-devops-user-and-key","text":"The best way to save your DevOps User/Key is to use the Ping Identity DevOps utility ping-devops . ping-devops Setup Installation instructions for ping-devops can be found in the ping-devops Tool document. To save your DevOps credentials, run ping-devops config and supply your credentials when prompted. Once ping-devops is installed and configured it will place your DEVOPS USER/KEY into a Ping Identity property file found at ~/.pingidentity/devops . with the following variable names set (see example below). PING_IDENTITY_DEVOPS_USER=jsmith@example.com PING_IDENTITY_DEVOPS_KEY=e9bd26ac-17e9-4133-a981-d7a7509314b2 You can always view these settings with the ping-devops info command after you've configured them.","title":"Saving Your DevOps User and Key"},{"location":"get-started/devopsUserKey/","text":"Using Your Devops User and Key \u00b6 When starting one of our containers, the container will attempt to find the DevOps registration information first in the DevOps property file located in ~/.pingidentity/devops . This property file was created when you set up the DevOps environment (see Get Started . If the DevOps registration information isn't found there, the container will check for environment variables assigned in the docker run command for standalone containers or in the YAML file for a stack. Display Your Devops Information \u00b6 To display your current DevOps environment information, run the DevOps command: ping-devops info For Standalone Containers \u00b6 When using the docker run command to start a container, you can assign the --env-file argument to the file containing your DevOps registration information. For example: docker run \\ --name pingdirectory \\ --publish 1389 :389 \\ --publish 8443 :443 \\ --detach \\ --env SERVER_PROFILE_URL = https://github.com/pingidentity/pingidentity-server-profiles.git \\ --env SERVER_PROFILE_PATH = getting-started/pingdirectory \\ --env-file ~/.pingidentity/devops \\ pingidentity/pingdirectory For Stacks \u00b6 When you're going to deploy a stack, you can use either of these methods to assign the location of the file containing your DevOps registration information: The env_file configuration option. The DevOps environment variables. Pass as Env File \u00b6 Add the env_file configuration option to the YAML file for the stack. The env_file configuration option passes environment variable definitions into the container. For example: ... pingdirectory: image: pingidentity/pingdirectory env_file: - ${ HOME } /.pingidentity/devops environment: - SERVER_PROFILE_URL = https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_PATH = getting-started/pingdirectory ... Pass as Env Variables \u00b6 Add the PING_IDENTITY_DEVOPS_USER and PING_IDENTITY_DEVOPS_KEY DevOps environment variables to the YAML file for the stack. For example: ... pingdirectory: image: pingidentity/pingdirectory environment: - SERVER_PROFILE_URL = https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_PATH = getting-started/pingdirectory - PING_IDENTITY_ACCEPT_EULA = YES - PING_IDENTITY_DEVOPS_USER = jsmith@example.com - PING_IDENTITY_DEVOPS_KEY = e9bd26ac-17e9-4133-a981-d7a7509314b2 ... For Kubernetes \u00b6 Our Kubernetes examples default to look for a Kubernetes secret named devops-secret . You need to create a Kubernetes secret that contains the environment variables PING_IDENTITY_DEVOPS_USER and PING_IDENTITY_DEVOPS_KEY . If you don't already know your DevOps credentials, display these using the DevOps command: ping-devops info Generate the Kubernetes secret from your DevOps credentials using either the ping-devops utility, or manually: ping-devops generate devops-secret | kubectl apply -f - Manually: kubectl create secret generic devops-secret \\ --from-literal = PING_IDENTITY_DEVOPS_USER = \" ${ PING_IDENTITY_DEVOPS_USER } \" \\ --from-literal = PING_IDENTITY_DEVOPS_KEY = \" ${ PING_IDENTITY_DEVOPS_KEY } \"","title":"Using Your DevOps User and Key"},{"location":"get-started/devopsUserKey/#using-your-devops-user-and-key","text":"When starting one of our containers, the container will attempt to find the DevOps registration information first in the DevOps property file located in ~/.pingidentity/devops . This property file was created when you set up the DevOps environment (see Get Started . If the DevOps registration information isn't found there, the container will check for environment variables assigned in the docker run command for standalone containers or in the YAML file for a stack.","title":"Using Your Devops User and Key"},{"location":"get-started/devopsUserKey/#display-your-devops-information","text":"To display your current DevOps environment information, run the DevOps command: ping-devops info","title":"Display Your Devops Information"},{"location":"get-started/devopsUserKey/#for-standalone-containers","text":"When using the docker run command to start a container, you can assign the --env-file argument to the file containing your DevOps registration information. For example: docker run \\ --name pingdirectory \\ --publish 1389 :389 \\ --publish 8443 :443 \\ --detach \\ --env SERVER_PROFILE_URL = https://github.com/pingidentity/pingidentity-server-profiles.git \\ --env SERVER_PROFILE_PATH = getting-started/pingdirectory \\ --env-file ~/.pingidentity/devops \\ pingidentity/pingdirectory","title":"For Standalone Containers"},{"location":"get-started/devopsUserKey/#for-stacks","text":"When you're going to deploy a stack, you can use either of these methods to assign the location of the file containing your DevOps registration information: The env_file configuration option. The DevOps environment variables.","title":"For Stacks"},{"location":"get-started/devopsUserKey/#pass-as-env-file","text":"Add the env_file configuration option to the YAML file for the stack. The env_file configuration option passes environment variable definitions into the container. For example: ... pingdirectory: image: pingidentity/pingdirectory env_file: - ${ HOME } /.pingidentity/devops environment: - SERVER_PROFILE_URL = https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_PATH = getting-started/pingdirectory ...","title":"Pass as Env File"},{"location":"get-started/devopsUserKey/#pass-as-env-variables","text":"Add the PING_IDENTITY_DEVOPS_USER and PING_IDENTITY_DEVOPS_KEY DevOps environment variables to the YAML file for the stack. For example: ... pingdirectory: image: pingidentity/pingdirectory environment: - SERVER_PROFILE_URL = https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_PATH = getting-started/pingdirectory - PING_IDENTITY_ACCEPT_EULA = YES - PING_IDENTITY_DEVOPS_USER = jsmith@example.com - PING_IDENTITY_DEVOPS_KEY = e9bd26ac-17e9-4133-a981-d7a7509314b2 ...","title":"Pass as Env Variables"},{"location":"get-started/devopsUserKey/#for-kubernetes","text":"Our Kubernetes examples default to look for a Kubernetes secret named devops-secret . You need to create a Kubernetes secret that contains the environment variables PING_IDENTITY_DEVOPS_USER and PING_IDENTITY_DEVOPS_KEY . If you don't already know your DevOps credentials, display these using the DevOps command: ping-devops info Generate the Kubernetes secret from your DevOps credentials using either the ping-devops utility, or manually: ping-devops generate devops-secret | kubectl apply -f - Manually: kubectl create secret generic devops-secret \\ --from-literal = PING_IDENTITY_DEVOPS_USER = \" ${ PING_IDENTITY_DEVOPS_USER } \" \\ --from-literal = PING_IDENTITY_DEVOPS_KEY = \" ${ PING_IDENTITY_DEVOPS_KEY } \"","title":"For Kubernetes"},{"location":"get-started/getStarted/","text":"Get Started \u00b6 You can quickly deploy Docker images of Ping Identity products. We use Docker, Docker Compose, and Kubernetes to deploy our Docker images in stable, network-enabled containers. Our Docker images are preconfigured to provide working instances of our products, either as single containers or in orchestrated sets. Prerequisites \u00b6 Docker Docker Compose (included with Docker Desktop on Mac and Windows) Your terminal configuration is set to use the Bash shell. Default Shell With Apple macOS Catalina, the Z shell (zsh) is the default shell, rather than Bash. To set your default terminal shell to Bash, enter: chsh -s /bin/bash . You've installed the ping-devops utility. Product license \u00b6 You'll need a product license to run our Docker images. You can use either: An evaluation license obtained with a valid DevOps user key. See DevOps Registration for more information. Although you'll first need to complete your DevOps Registration, you can subsequently use a valid product license available with a current Ping Identity customer subscription. Set Up Your Devops Environment \u00b6 Open a terminal and create a local DevOps directory named ${HOME}/projects/devops . Parent Directory We'll use this as the parent directory for all DevOps examples referenced in our documentation. Configure your DevOps environment: ping-devops config Respond to all Docker configuration questions, accepting the defaults if you're not sure. You can accept the (empty) defaults for Kubernetes. Settings for custom variables aren't needed initially. All of your responses are stored as settings in your local ~/.pingidentity/devops file. Allow the configuration script to source this file in your shell profile (for example, ~/.bash_profile). To display your DevOps environment settings, enter: ping-devops info You can use the ping-devops utility to run a quick demonstration of any of our products in your Docker environment. a. To display information about the containers or stacks available using the ping-devops utility, enter: ping-devops docker info b. To display information about one of the listed containers or stacks, enter: ping-devops docker info <name> Where <name> is one of the listed container or stack names. To start one of the containers or stacks, enter: ping-devops docker start <name> Where <name> is one of the listed container or stack names. The initial run will ensure dependencies are met (such as, Docker or Docker Compose). When you're done: To stop the container or stack, enter: ping-devops docker stop <name> To remove the container or stack and all associated data, enter ping-devops docker rm <name>","title":"Introduction"},{"location":"get-started/getStarted/#get-started","text":"You can quickly deploy Docker images of Ping Identity products. We use Docker, Docker Compose, and Kubernetes to deploy our Docker images in stable, network-enabled containers. Our Docker images are preconfigured to provide working instances of our products, either as single containers or in orchestrated sets.","title":"Get Started"},{"location":"get-started/getStarted/#prerequisites","text":"Docker Docker Compose (included with Docker Desktop on Mac and Windows) Your terminal configuration is set to use the Bash shell. Default Shell With Apple macOS Catalina, the Z shell (zsh) is the default shell, rather than Bash. To set your default terminal shell to Bash, enter: chsh -s /bin/bash . You've installed the ping-devops utility.","title":"Prerequisites"},{"location":"get-started/getStarted/#product-license","text":"You'll need a product license to run our Docker images. You can use either: An evaluation license obtained with a valid DevOps user key. See DevOps Registration for more information. Although you'll first need to complete your DevOps Registration, you can subsequently use a valid product license available with a current Ping Identity customer subscription.","title":"Product license"},{"location":"get-started/getStarted/#set-up-your-devops-environment","text":"Open a terminal and create a local DevOps directory named ${HOME}/projects/devops . Parent Directory We'll use this as the parent directory for all DevOps examples referenced in our documentation. Configure your DevOps environment: ping-devops config Respond to all Docker configuration questions, accepting the defaults if you're not sure. You can accept the (empty) defaults for Kubernetes. Settings for custom variables aren't needed initially. All of your responses are stored as settings in your local ~/.pingidentity/devops file. Allow the configuration script to source this file in your shell profile (for example, ~/.bash_profile). To display your DevOps environment settings, enter: ping-devops info You can use the ping-devops utility to run a quick demonstration of any of our products in your Docker environment. a. To display information about the containers or stacks available using the ping-devops utility, enter: ping-devops docker info b. To display information about one of the listed containers or stacks, enter: ping-devops docker info <name> Where <name> is one of the listed container or stack names. To start one of the containers or stacks, enter: ping-devops docker start <name> Where <name> is one of the listed container or stack names. The initial run will ensure dependencies are met (such as, Docker or Docker Compose). When you're done: To stop the container or stack, enter: ping-devops docker stop <name> To remove the container or stack and all associated data, enter ping-devops docker rm <name>","title":"Set Up Your Devops Environment"},{"location":"get-started/getStartedWithGitRepo/","text":"Deploy an Example Stack \u00b6 The pingidentity-devops-getting-started repository contains all of our working Docker and Kubernetes examples. What You'll Do \u00b6 You'll use Git to clone the pingidentity-devops-getting-started repository, and Docker Compose to deploy the full stack example. Prerequisites \u00b6 You've already set up your DevOps environment. See Get Started . Installed Git Clone the getting-started Repo \u00b6 Clone the pingidentity-devops-getting-started repository to your local ${PING_IDENTITY_DEVOPS_HOME} directory: The ${PING_IDENTITY_DEVOPS_HOME} environment variable was set when you ran ping-devops config . cd ${ PING_IDENTITY_DEVOPS_HOME } git clone \\ https://github.com/pingidentity/pingidentity-devops-getting-started.git Deploy the Full Stack \u00b6 Deploy the full stack of our product containers: Initial Deployment For your initial deployment of the stack, we recommend you make no changes to the docker-compose.yaml file to ensure you have a successful first-time deployment. For subsequent deployments, see Saving Your Configuration Changes . To start the stack, go to your local pingidentity-devops-getting-started/11-docker-compose/03-full-stack directory and enter: docker-compose up -d The full set of our DevOps images is automatically pulled from our repository, if you haven't already pulled the images from Docker Hub . Use this command to display the logs as the stack starts: docker-compose logs -f Enter Ctrl+C to exit the display. Use either of these commands to display the status of the Docker containers in the stack: docker ps (enter this at intervals) watch \"docker container ls --format 'table {{.Names}}\\t{{.Status}}'\" Refer to the Docker Compose Documentation for more information. Log in to the management consoles for the products: Product Connection Details PingFederate URL: https://localhost:9999/pingfederate/app Username: administrator Password: 2FederateM0re PingDirectory URL: https://localhost:8443/console Server: pingdirectory:636 Username: administrator Password: 2FederateM0re PingAccess URL: https://localhost:9000 Username: administrator Password: 2FederateM0re PingDataGovernance URL: https://localhost:8443/console Server: pingdatagovernance:636 Username: administrator Password: 2FederateM0re PingDataSync URL: https://localhost:8443/console Server: pingdatasync:636 Username: administrator Password: 2FederateM0re PingCentral URL: https://localhost:9022 Username: administrator Password: 2Federate Apache Directory Studio for PingDirectory LDAP Port: 1636 LDAP BaseDN: dc=example,dc=com Root Username: cn=administrator Root Password: 2FederateM0re When you no longer want to run the stack, you can either stop or remove the stack. To stop the running stack (doesn't remove any of the containers or associated Docker networks or volumes), enter: docker-compose stop To stop the stack and remove all of the containers and associated Docker networks (preservers volumes), enter: docker-compose down","title":"Deploy Example Stack"},{"location":"get-started/getStartedWithGitRepo/#deploy-an-example-stack","text":"The pingidentity-devops-getting-started repository contains all of our working Docker and Kubernetes examples.","title":"Deploy an Example Stack"},{"location":"get-started/getStartedWithGitRepo/#what-youll-do","text":"You'll use Git to clone the pingidentity-devops-getting-started repository, and Docker Compose to deploy the full stack example.","title":"What You'll Do"},{"location":"get-started/getStartedWithGitRepo/#prerequisites","text":"You've already set up your DevOps environment. See Get Started . Installed Git","title":"Prerequisites"},{"location":"get-started/getStartedWithGitRepo/#clone-the-getting-started-repo","text":"Clone the pingidentity-devops-getting-started repository to your local ${PING_IDENTITY_DEVOPS_HOME} directory: The ${PING_IDENTITY_DEVOPS_HOME} environment variable was set when you ran ping-devops config . cd ${ PING_IDENTITY_DEVOPS_HOME } git clone \\ https://github.com/pingidentity/pingidentity-devops-getting-started.git","title":"Clone the getting-started Repo"},{"location":"get-started/getStartedWithGitRepo/#deploy-the-full-stack","text":"Deploy the full stack of our product containers: Initial Deployment For your initial deployment of the stack, we recommend you make no changes to the docker-compose.yaml file to ensure you have a successful first-time deployment. For subsequent deployments, see Saving Your Configuration Changes . To start the stack, go to your local pingidentity-devops-getting-started/11-docker-compose/03-full-stack directory and enter: docker-compose up -d The full set of our DevOps images is automatically pulled from our repository, if you haven't already pulled the images from Docker Hub . Use this command to display the logs as the stack starts: docker-compose logs -f Enter Ctrl+C to exit the display. Use either of these commands to display the status of the Docker containers in the stack: docker ps (enter this at intervals) watch \"docker container ls --format 'table {{.Names}}\\t{{.Status}}'\" Refer to the Docker Compose Documentation for more information. Log in to the management consoles for the products: Product Connection Details PingFederate URL: https://localhost:9999/pingfederate/app Username: administrator Password: 2FederateM0re PingDirectory URL: https://localhost:8443/console Server: pingdirectory:636 Username: administrator Password: 2FederateM0re PingAccess URL: https://localhost:9000 Username: administrator Password: 2FederateM0re PingDataGovernance URL: https://localhost:8443/console Server: pingdatagovernance:636 Username: administrator Password: 2FederateM0re PingDataSync URL: https://localhost:8443/console Server: pingdatasync:636 Username: administrator Password: 2FederateM0re PingCentral URL: https://localhost:9022 Username: administrator Password: 2Federate Apache Directory Studio for PingDirectory LDAP Port: 1636 LDAP BaseDN: dc=example,dc=com Root Username: cn=administrator Root Password: 2FederateM0re When you no longer want to run the stack, you can either stop or remove the stack. To stop the running stack (doesn't remove any of the containers or associated Docker networks or volumes), enter: docker-compose stop To stop the stack and remove all of the containers and associated Docker networks (preservers volumes), enter: docker-compose down","title":"Deploy the Full Stack"},{"location":"get-started/pingDevopsUtil/","text":"The ping-devops Utility \u00b6 ping-devops is our general DevOps command line utility. Dependent Utilities \u00b6 To perform all of its operations, ping-devops has a dependency on these utilities: openssl base64 kustomize kubectl envsubst jq Installation and Upgrades \u00b6 Use Homebrew, to install ping-devops on Apple or Linux: To install, enter: brew tap pingidentity/devops brew install ping-devops To upgrade, enter: brew upgrade ping-devops Check for upgrades regularly. Run the following command to see if there's an upgrade available: ping-devops version The dependent utilities for ping-devops will also be installed or upgraded during this process. On Linux systems, install or upgrade the ping-devops utility and bash_profile aliases to your current directory by entering: curl -sL https://bit.ly/ping-devops-install | bash Follow instructions to copy to the preferred location. Ensure you have the dependent utilities for ping-devops installed as well. ping-devops Usage \u00b6 Enter ping-devops in a terminal to display the commands listing. The display will be similar to this: ##################################################################### # Ping Identity DevOps (version 0.7.2) # # Documentation: https://devops.pingidentity.com # GitHub Repos: https://github.com/topics/ping-devops ##################################################################### General Usage: ping-devops config ping-devops info [ -v ] ping-devops version ping-devops clean ping-devops topic [ { topic-name } ] Generate Kubernetes/Kustomize/License Resources: ping-devops generate devops-secret ping-devops generate tls-secret { domain } ping-devops generate ssh-id-secret { ssh id_rsa file } ping-devops generate license { product } { ver } ping-devops generate license-secret { license file } ping-devops generate license-secret { product } { ver } ping-devops generate kustomization.yaml Running Docker/Kubernetes Evironments: ping-devops docker [ info | start | stop | rm | clean ] ping-devops kubernetes [ info | start | rm | clean ] Hashicorp Vault: ping-devops vault get-token ping-devops vault create-annotations { secret } Further help: https://github.com/pingidentity/ping-devops","title":"ping-devops Utility"},{"location":"get-started/pingDevopsUtil/#the-ping-devops-utility","text":"ping-devops is our general DevOps command line utility.","title":"The ping-devops Utility"},{"location":"get-started/pingDevopsUtil/#dependent-utilities","text":"To perform all of its operations, ping-devops has a dependency on these utilities: openssl base64 kustomize kubectl envsubst jq","title":"Dependent Utilities"},{"location":"get-started/pingDevopsUtil/#installation-and-upgrades","text":"Use Homebrew, to install ping-devops on Apple or Linux: To install, enter: brew tap pingidentity/devops brew install ping-devops To upgrade, enter: brew upgrade ping-devops Check for upgrades regularly. Run the following command to see if there's an upgrade available: ping-devops version The dependent utilities for ping-devops will also be installed or upgraded during this process. On Linux systems, install or upgrade the ping-devops utility and bash_profile aliases to your current directory by entering: curl -sL https://bit.ly/ping-devops-install | bash Follow instructions to copy to the preferred location. Ensure you have the dependent utilities for ping-devops installed as well.","title":"Installation and Upgrades"},{"location":"get-started/pingDevopsUtil/#ping-devops-usage","text":"Enter ping-devops in a terminal to display the commands listing. The display will be similar to this: ##################################################################### # Ping Identity DevOps (version 0.7.2) # # Documentation: https://devops.pingidentity.com # GitHub Repos: https://github.com/topics/ping-devops ##################################################################### General Usage: ping-devops config ping-devops info [ -v ] ping-devops version ping-devops clean ping-devops topic [ { topic-name } ] Generate Kubernetes/Kustomize/License Resources: ping-devops generate devops-secret ping-devops generate tls-secret { domain } ping-devops generate ssh-id-secret { ssh id_rsa file } ping-devops generate license { product } { ver } ping-devops generate license-secret { license file } ping-devops generate license-secret { product } { ver } ping-devops generate kustomization.yaml Running Docker/Kubernetes Evironments: ping-devops docker [ info | start | stop | rm | clean ] ping-devops kubernetes [ info | start | rm | clean ] Hashicorp Vault: ping-devops vault get-token ping-devops vault create-annotations { secret } Further help: https://github.com/pingidentity/ping-devops","title":"ping-devops Usage"},{"location":"get-started/prodLicense/","text":"DevOps Product Licenses \u00b6 In order to run the Ping Identity DevOps images, a valid product license is required. There are several ways to obtain a product license to run the images: Evaluation License \u00b6 By registering for Ping Identity's DevOps program, you'll be issued credentials that will automate the process of retrieving evaluation product license. Evaluation License Please note that evaluation licenses are short lived (30 days) and must not be used in production deployments. Evaluation licenses can only be used with images published in the last 90 days. If you wish to continue to use an image that was published more than 90 days ago, you must obtain a product license. Once you have product license for the product and version of the more-than-90-days-old image, follow the instructions to mount the product license . Using your DevOps User/Key Existing License \u00b6 Mount Existing Product License Using Your DevOps User and Key \u00b6 When starting an image, you can provide your devops property file ~/.pingidentity/devops or using the individual environment variables. The examples provided for docker-compose are set up to use this property file by default. For more detail, run the ping-devops info to get your DevOps environment information. Example Docker Run Command \u00b6 An example of running a docker image using the docker run command would look like the following example (See the 2 environment variables starting with PING_IDENTITY_DEVOPS ): docker run \\ --name pingdirectory \\ --publish 1389 :389 \\ --publish 8443 :443 \\ --detach \\ --env SERVER_PROFILE_URL = https://github.com/pingidentity/pingidentity-server-profiles.git \\ --env SERVER_PROFILE_PATH = getting-started/pingdirectory \\ --env-file ~/.pingidentity/devops \\ pingidentity/pingdirectory Example YAML file \u00b6 An example of running a docker image using any docker .yaml file would look like the following example (See the 2 environment variables starting with PING_IDENTITY_DEVOPS ): ... pingdirectory : image : pingidentity/pingdirectory env_file : - ${HOME}/.pingidentity/devops environment : - SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_PATH=getting-started/pingdirectory ... Example Inline Env Variables \u00b6 An example of running a docker image using any docker .yaml file would look like the following example (See the 2 environment variables starting with PING_IDENTITY_DEVOPS ): ... pingdirectory : image : pingidentity/pingdirectory environment : - SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_PATH=getting-started/pingdirectory - PING_IDENTITY_DEVOPS_USER=jsmith@example.com - PING_IDENTITY_DEVOPS_KEY=e9bd26ac-17e9-4133-a981-d7a7509314b2 ... Mount Existing Product License \u00b6 You can pass the license file to a container via mounting to the container's /opt/in directory. Note: You do not need to do this if you are using your DevOps User/Key. If you have provided license files via the volume mount and a DevOps User/Key, it will ignore the DevOps User/Key. The /opt/in directory overlays files onto the products runtime filesystem, the license needs to be named correctly and mounted in the exact location the product checks for valid licenses. Example Mounts \u00b6 Product File Name Mount Path PingFederate pingfederate.lic /opt/in/instance/server/default/conf/pingfederate.lic PingAccess pingaccess.lic /opt/in/instance/conf/pingaccess.lic PingDirectory PingDirectory.lic /opt/in/instance/PingDirectory.lic PingDataSync PingDirectory.lic /opt/in/instance/PingDirectory.lic PingDataGovernance PingDataGovernance.lic /opt/in/instance/PingDataGovernance.lic PingCentral pingcentral.lic /opt/in/instance/conf/pingcentral.lic Volume Mount Syntax \u00b6 Docker \u00b6 Sample docker run command with mounted license: docker run \\ --name pingfederate \\ --volume <local/path/to/pingfederate.lic>:/opt/in/instance/server/default/conf/pingfederate.lic pingidentity/pingfederate:edge Sample docker-compose.yaml with mounted license: version: \"2.4\" services: pingfederate: image: pingidentity/pingfederate:edge volumes: - path/to/pingfederate.lic:/opt/in/instance/server/default/conf/pingfederate.lic Kubernetes \u00b6 Create a Kubernetes secret from the license file kubectl create secret generic pingfederate-license \\ --from-file = ./pingfederate.lic Then mount it to the pod spec: containers: - name: pingfederate image: pingidentity/pingfederate volumeMounts: - name: pingfederate-license-volume mountPath: \"/opt/in/instance/server/default/conf/pingfederate.lic\" subPath: pingfederate.lic volumes: - name: pingfederate-license-volume secret: secretName: pingfederate-license Helm \u00b6 Create a Kubernetes secret from the license file kubectl create secret generic pingfederate-license \\ --from-file = ./pingfederate.lic Add the secretVolumes within your values.yaml deployment file pingfederate-admin: ... secretVolumes: pingfederate-license: items: pingfederate.lic: /opt/in/instance/server/default/conf/pingfederate.lic","title":"DevOps Product Licenses"},{"location":"get-started/prodLicense/#devops-product-licenses","text":"In order to run the Ping Identity DevOps images, a valid product license is required. There are several ways to obtain a product license to run the images:","title":"DevOps Product Licenses"},{"location":"get-started/prodLicense/#evaluation-license","text":"By registering for Ping Identity's DevOps program, you'll be issued credentials that will automate the process of retrieving evaluation product license. Evaluation License Please note that evaluation licenses are short lived (30 days) and must not be used in production deployments. Evaluation licenses can only be used with images published in the last 90 days. If you wish to continue to use an image that was published more than 90 days ago, you must obtain a product license. Once you have product license for the product and version of the more-than-90-days-old image, follow the instructions to mount the product license . Using your DevOps User/Key","title":"Evaluation License"},{"location":"get-started/prodLicense/#existing-license","text":"Mount Existing Product License","title":"Existing License"},{"location":"get-started/prodLicense/#using-your-devops-user-and-key","text":"When starting an image, you can provide your devops property file ~/.pingidentity/devops or using the individual environment variables. The examples provided for docker-compose are set up to use this property file by default. For more detail, run the ping-devops info to get your DevOps environment information.","title":"Using Your DevOps User and Key"},{"location":"get-started/prodLicense/#example-docker-run-command","text":"An example of running a docker image using the docker run command would look like the following example (See the 2 environment variables starting with PING_IDENTITY_DEVOPS ): docker run \\ --name pingdirectory \\ --publish 1389 :389 \\ --publish 8443 :443 \\ --detach \\ --env SERVER_PROFILE_URL = https://github.com/pingidentity/pingidentity-server-profiles.git \\ --env SERVER_PROFILE_PATH = getting-started/pingdirectory \\ --env-file ~/.pingidentity/devops \\ pingidentity/pingdirectory","title":"Example Docker Run Command"},{"location":"get-started/prodLicense/#example-yaml-file","text":"An example of running a docker image using any docker .yaml file would look like the following example (See the 2 environment variables starting with PING_IDENTITY_DEVOPS ): ... pingdirectory : image : pingidentity/pingdirectory env_file : - ${HOME}/.pingidentity/devops environment : - SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_PATH=getting-started/pingdirectory ...","title":"Example YAML file"},{"location":"get-started/prodLicense/#example-inline-env-variables","text":"An example of running a docker image using any docker .yaml file would look like the following example (See the 2 environment variables starting with PING_IDENTITY_DEVOPS ): ... pingdirectory : image : pingidentity/pingdirectory environment : - SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_PATH=getting-started/pingdirectory - PING_IDENTITY_DEVOPS_USER=jsmith@example.com - PING_IDENTITY_DEVOPS_KEY=e9bd26ac-17e9-4133-a981-d7a7509314b2 ...","title":"Example Inline Env Variables"},{"location":"get-started/prodLicense/#mount-existing-product-license","text":"You can pass the license file to a container via mounting to the container's /opt/in directory. Note: You do not need to do this if you are using your DevOps User/Key. If you have provided license files via the volume mount and a DevOps User/Key, it will ignore the DevOps User/Key. The /opt/in directory overlays files onto the products runtime filesystem, the license needs to be named correctly and mounted in the exact location the product checks for valid licenses.","title":"Mount Existing Product License"},{"location":"get-started/prodLicense/#example-mounts","text":"Product File Name Mount Path PingFederate pingfederate.lic /opt/in/instance/server/default/conf/pingfederate.lic PingAccess pingaccess.lic /opt/in/instance/conf/pingaccess.lic PingDirectory PingDirectory.lic /opt/in/instance/PingDirectory.lic PingDataSync PingDirectory.lic /opt/in/instance/PingDirectory.lic PingDataGovernance PingDataGovernance.lic /opt/in/instance/PingDataGovernance.lic PingCentral pingcentral.lic /opt/in/instance/conf/pingcentral.lic","title":"Example Mounts"},{"location":"get-started/prodLicense/#volume-mount-syntax","text":"","title":"Volume Mount Syntax"},{"location":"get-started/prodLicense/#docker","text":"Sample docker run command with mounted license: docker run \\ --name pingfederate \\ --volume <local/path/to/pingfederate.lic>:/opt/in/instance/server/default/conf/pingfederate.lic pingidentity/pingfederate:edge Sample docker-compose.yaml with mounted license: version: \"2.4\" services: pingfederate: image: pingidentity/pingfederate:edge volumes: - path/to/pingfederate.lic:/opt/in/instance/server/default/conf/pingfederate.lic","title":"Docker"},{"location":"get-started/prodLicense/#kubernetes","text":"Create a Kubernetes secret from the license file kubectl create secret generic pingfederate-license \\ --from-file = ./pingfederate.lic Then mount it to the pod spec: containers: - name: pingfederate image: pingidentity/pingfederate volumeMounts: - name: pingfederate-license-volume mountPath: \"/opt/in/instance/server/default/conf/pingfederate.lic\" subPath: pingfederate.lic volumes: - name: pingfederate-license-volume secret: secretName: pingfederate-license","title":"Kubernetes"},{"location":"get-started/prodLicense/#helm","text":"Create a Kubernetes secret from the license file kubectl create secret generic pingfederate-license \\ --from-file = ./pingfederate.lic Add the secretVolumes within your values.yaml deployment file pingfederate-admin: ... secretVolumes: pingfederate-license: items: pingfederate.lic: /opt/in/instance/server/default/conf/pingfederate.lic","title":"Helm"},{"location":"how-to/buildPingFederateProfile/","text":"Build Profile From Current Deployment \u00b6 The term \"profile\" can vary in many instances. Here we will focus on two types of profiles for PingFederate: configuration archive, and bulk export. We will discuss the similarities and differences between two as well as how to build either from a running PingFederate environment. Prerequisite \u00b6 You've already been through Get Started to set up your DevOps environment and run a test deployment of the products. You understand our Product Container Anatomy . Recommended: You've read through Customizing Server Profiles Similarities \u00b6 The two profile methods we are covering are: Bulk API Export - the resulting .json from the admin api at /bulk/export. typically saved as data.json Configuration Archive Pulled either from the admin UI - Server > Configuration Archive or from the admin API at /configArchive . We'll call the result of this output data.zip or the /data folder. Both of these methods are considered file-based profiles. This means a \"complete profile\" looks like a subset of files that you would typically find in a running PingFederate filesystem. Again, this is a subset of files. Specifically, the minimal number of files needed to achieve your PingFederate configuration. All additional files that are not specific to your configuration should be left out as they will be filled in by the PingFederate docker image. (Refer to Container Anatomy for additional details). Considering the above, familiarity with the PingFederate filesystem will help you achieve the optimal profile. Some key information can be found in profile structures . But, to put it simply, you want to at least save every file outside of pingfederate/server/default/data that you've edited Additionally, all files that are included in the profile should also be environment agnostic. This typically means turning hostnames and secrets into variables that can be delivered from the Orchestration Layer . Bulk API Export Profile Method \u00b6 What You'll Do \u00b6 A summary of the resulting process: Export a data.json from /bulk/export Configure and run bulkconfig tool Export Key Pairs base64 encode exported key pairs add data.json.subst to your profile at instance/bulk-config/data.json.subst Rather than just following the above steps, we will look at this comprehensively to understand purpose. Use the steps for reference as needed A PingFederate Admin Console will import a data.json on startup if it finds it in instance/bulk-config/data.json . The PF admin api /bulk/export endpoint will output a large json blob that is representative of the entire pingfederate/server/default/data folder, PingFederate 'core config', or a representation of anything you would configure from the PingFederate UI. You could consider it \"the configuration archive in json format\". So, you could just: Go to a running PingFederate, run: curl \\ --location \\ --request GET 'https://pingfederate-admin.ping-devops.com/pf-admin-api/v1/bulk/export' \\ --header 'X-XSRF-Header: PingFederate' \\ --user \"administrator: ${ passsword } \" > data.json Save data.json into a profile at instance/bulk-config/data.json Delete everything except pf.jwk in instance/server/default/data And you will have a bulk api export \"profile\". This is handy because the entire config is on a single file and if you store it in source control you then only have to compare differences on one file. However, there's much more hidden value beyond being on one file. Make the Bulk API Export \"Profile Worthy\" \u00b6 By default, the resulting data.json from the export contains encrypted values, and to import this file, your PingFederate needs to have the corresponding master key ( pf.jwk ) in pingfederate/server/default/data . Note, in the devops world, we call this folder instance/server/default/data . However, each of the encrypted values also have the option to be replaced with an unencrypted form and, when required, a corresponding password. For example the SSL Server Certificate from the PingFederate Baseline Profile when exported to data.json looks like: { \"resourceType\" : \"/keyPairs/sslServer\" , \"operationType\" : \"SAVE\" , \"items\" : [ { \"id\" : \"sslservercert\" , \"fileData\" : \"MIIRBwIBAzCCEMAGCSqGSIb3DQEHAaCCELEEghCtMIIQqTCCCeUGCSqGSIb3DQEHAaCCCdYEggnSMIIJzjCCCcoGCyqGSIb3DQEMCgECoIIJezCCCXcwKQYKKoZIhvcNAQwBAzAbBBQu6vDERQZX3uujWa7v_q3sYN4Q0gIDAMNQBIIJSFtdWbvLhzYrTqeKKiJqiqROgE0E4mkVvmEC6NwhhPbcH37IDNvVLu0umm--CDZnEmlyPpUucO345-U-6z-cskw4TbsjYIzM10MwS6JdsyYFTC3GwqioqndVgBUzDh8xGnfzx52zEehX8d-ig1F6xYsbEc01gTbh4lF5MA7E7VfoTa4hWqtceV8PQeqzJNarlZyDSaS5BLn1J6G9BYUze-M1xGhATz7F2l-aAt6foi0mwIBlc2fwsdEPuAALZgdG-q_V4gOJW2K0ONnmWhMgMLpCL42cmSb ... more encrypted text ... Yxpzp_srpy4LHNdgHqhVBhqtDrjeKJDRfc1yk21P5PpfEBxn5MD4wITAJBgUrDgMCGgUABBQLBpq8y79Pq1TzG1Xf6OAjZzBZaQQUC4kD4CkcrH-WTQhJHud850ddn08CAwGGoA==\" , \"encryptedPassword\" : \"eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2Iiwia2lkIjoiRW1JY1UxOVdueSIsInZlcnNpb24iOiIxMC4xLjEuMCJ9..l6PJ55nSSvKHl0vSWTpkOA.i7hpnnu2yIByhyq_aGBCdaqS3u050yG8eMRGnLRx2Yk.Mo4WSkbbJyLISHq6i4nlVA\" } ] } But this master key dependent form can be converted to: { \"operationType\" : \"SAVE\" , \"items\" : [{ \"password\" : \"2FederateM0re\" , \"fileData\" : \"MIIRCQIBAzCCEM8GCSqGSIb3DQEHAaCCEMAEghC8MIIQuDCCC28GCSqGSIb3DQEHBqCCC2AwggtcAgEAMIILVQYJKoZIhvcNAQcBMBwGCiqGSIb3DQEMAQYwDgQIjXWLRGuGNIQCAggAgIILKOgCQ9onDqBPQsshsaS50OjWtj\\/7s47BUYal1YhO70fBup1a82WGHGhAvb\\/SY1yOhqQR+TloEBOPI5cExoGN\\/Gvw2Mw5\\/wkQZZMSHqxjz68KhN4B0hrsOf4rqShB7jsz9ebSml3r2w0sUZWR73GBtBt1Y3wIlXLS2WtqdtHra9VnUqp1eOk+xenjuWM+u2ndDD43GgKB3n8mNBSSVBqx6ne7aSRJRuAUd+HAzLvSeXjTPMObI1Jod2F+7 ... more base64 encoded exported .p12 ... 5QJ15OJp2iEoVBWxogKf64s2F0iIYPoo6yjNvlidZCevP564FwknWrHoD7R8cIBrhlCJQbEOpOhPg66r4MK1CeJ2poaKRlMS8HGcMRaTpaqD+pIlgmUS6xFw49vr9Kwfb7KteRsTkNR+I8A7HjUpuCMSUwIwYJKoZIhvcNAQkVMRYEFOb7g1xwDka5fJ4sqngEvzTyuWnpMDEwITAJBgUrDgMCGgUABBRlJ+D+FR\\/vQbaTGbKDFiBK\\/xDbqQQIAjLc+GgRg44CAggA\" , \"id\" : \"sslservercert\" }], \"resourceType\" : \"/keyPairs/sslServer\" } What happened: Exported the private key+cert of the server cert with alias sslservercert . Upon export, a password is requested and 2FederateM0re was used. This results in download of a password protected .p12 file. On data.json key name encryptedPassword converted to just password The value for fileData is replaced with a base64 encoded version of the exported .p12 file. This is a process that can be used for all encrypted items and environment specific items: Key Pairs (.p12) Trusted Certs (x509) Admin Password Data Store Passwords Integration Kit Properties Hostnames Now, if you follow this through the entire data.json, it would take a while, and you would be left with a file that is unacceptable for source control (since it's completely unencrypted). So, the next logical step is to abstract the unencrypted values and replace with variables. Then the values can be stored in a secrets management and the variablized file can be in source control. Doing all this would manually would take a long time, fortunately, there's the ping-bulkconfig-tool . Detailed steps for using the tool are documented next to where it is stored. The general concept is to point the tool at the data.json and a config file. After running you will be left with a data.json.subst and a list of environment variables waiting to be filled. The data.json.subst form of our example above will look like: { \"operationType\" : \"SAVE\" , \"items\" : [{ \"password\" : \"${keyPairs_sslServer_items_sslservercert_sslservercert_password}\" , \"fileData\" : \"${keyPairs_sslServer_items_sslservercert_sslservercert_fileData}\" , \"id\" : \"sslservercert\" }], \"resourceType\" : \"/keyPairs/sslServer\" } The variablized data.json.subst is now a good candidate to for committing to source control. The resulting env_vars file can be used as a guideline for secrets that should be managed externally and only delivered to the container/image as needed for it's specific environment. Additional Notes \u00b6 The bulk api export is intended to be used as a bulk import. The /bulk/import endpoint is destructive and overwrites the entire current admin config. If you are in a clustered environment, the PingFederate image will import the data.json and also replicate the configuration to engines in the cluster. Config Archive Profiles \u00b6 Comparing Profile Methods \u00b6 Configuration Archive based profiles have some pros/cons to weigh when compared to bulk api export profiles. You will find bulk api export profiles to be more advantageous in most scenarios besides devops principle purists. Pros: The /data folder, opposed to a data.json file, is better for profile layering Configuration is available on engines at startup. This: lowers dependency on the admin at initial cluster startup enables mixed configurations in a single cluster. Canary-like \"roll-out\" instead of config pushed to all engines at once. Cons: The /data folder contains key pairs in a .jks so externally managing keys is very difficult. Encrypted data is scattered throughout the folder creating dependency on the master encryption key. What You'll Do \u00b6 A summary of the resulting process: Export a data.zip Optionally, variablize Replace data folder","title":"Build Your Own ..."},{"location":"how-to/buildPingFederateProfile/#build-profile-from-current-deployment","text":"The term \"profile\" can vary in many instances. Here we will focus on two types of profiles for PingFederate: configuration archive, and bulk export. We will discuss the similarities and differences between two as well as how to build either from a running PingFederate environment.","title":"Build Profile From Current Deployment"},{"location":"how-to/buildPingFederateProfile/#prerequisite","text":"You've already been through Get Started to set up your DevOps environment and run a test deployment of the products. You understand our Product Container Anatomy . Recommended: You've read through Customizing Server Profiles","title":"Prerequisite"},{"location":"how-to/buildPingFederateProfile/#similarities","text":"The two profile methods we are covering are: Bulk API Export - the resulting .json from the admin api at /bulk/export. typically saved as data.json Configuration Archive Pulled either from the admin UI - Server > Configuration Archive or from the admin API at /configArchive . We'll call the result of this output data.zip or the /data folder. Both of these methods are considered file-based profiles. This means a \"complete profile\" looks like a subset of files that you would typically find in a running PingFederate filesystem. Again, this is a subset of files. Specifically, the minimal number of files needed to achieve your PingFederate configuration. All additional files that are not specific to your configuration should be left out as they will be filled in by the PingFederate docker image. (Refer to Container Anatomy for additional details). Considering the above, familiarity with the PingFederate filesystem will help you achieve the optimal profile. Some key information can be found in profile structures . But, to put it simply, you want to at least save every file outside of pingfederate/server/default/data that you've edited Additionally, all files that are included in the profile should also be environment agnostic. This typically means turning hostnames and secrets into variables that can be delivered from the Orchestration Layer .","title":"Similarities"},{"location":"how-to/buildPingFederateProfile/#bulk-api-export-profile-method","text":"","title":"Bulk API Export Profile Method"},{"location":"how-to/buildPingFederateProfile/#what-youll-do","text":"A summary of the resulting process: Export a data.json from /bulk/export Configure and run bulkconfig tool Export Key Pairs base64 encode exported key pairs add data.json.subst to your profile at instance/bulk-config/data.json.subst Rather than just following the above steps, we will look at this comprehensively to understand purpose. Use the steps for reference as needed A PingFederate Admin Console will import a data.json on startup if it finds it in instance/bulk-config/data.json . The PF admin api /bulk/export endpoint will output a large json blob that is representative of the entire pingfederate/server/default/data folder, PingFederate 'core config', or a representation of anything you would configure from the PingFederate UI. You could consider it \"the configuration archive in json format\". So, you could just: Go to a running PingFederate, run: curl \\ --location \\ --request GET 'https://pingfederate-admin.ping-devops.com/pf-admin-api/v1/bulk/export' \\ --header 'X-XSRF-Header: PingFederate' \\ --user \"administrator: ${ passsword } \" > data.json Save data.json into a profile at instance/bulk-config/data.json Delete everything except pf.jwk in instance/server/default/data And you will have a bulk api export \"profile\". This is handy because the entire config is on a single file and if you store it in source control you then only have to compare differences on one file. However, there's much more hidden value beyond being on one file.","title":"What You'll Do"},{"location":"how-to/buildPingFederateProfile/#make-the-bulk-api-export-profile-worthy","text":"By default, the resulting data.json from the export contains encrypted values, and to import this file, your PingFederate needs to have the corresponding master key ( pf.jwk ) in pingfederate/server/default/data . Note, in the devops world, we call this folder instance/server/default/data . However, each of the encrypted values also have the option to be replaced with an unencrypted form and, when required, a corresponding password. For example the SSL Server Certificate from the PingFederate Baseline Profile when exported to data.json looks like: { \"resourceType\" : \"/keyPairs/sslServer\" , \"operationType\" : \"SAVE\" , \"items\" : [ { \"id\" : \"sslservercert\" , \"fileData\" : \"MIIRBwIBAzCCEMAGCSqGSIb3DQEHAaCCELEEghCtMIIQqTCCCeUGCSqGSIb3DQEHAaCCCdYEggnSMIIJzjCCCcoGCyqGSIb3DQEMCgECoIIJezCCCXcwKQYKKoZIhvcNAQwBAzAbBBQu6vDERQZX3uujWa7v_q3sYN4Q0gIDAMNQBIIJSFtdWbvLhzYrTqeKKiJqiqROgE0E4mkVvmEC6NwhhPbcH37IDNvVLu0umm--CDZnEmlyPpUucO345-U-6z-cskw4TbsjYIzM10MwS6JdsyYFTC3GwqioqndVgBUzDh8xGnfzx52zEehX8d-ig1F6xYsbEc01gTbh4lF5MA7E7VfoTa4hWqtceV8PQeqzJNarlZyDSaS5BLn1J6G9BYUze-M1xGhATz7F2l-aAt6foi0mwIBlc2fwsdEPuAALZgdG-q_V4gOJW2K0ONnmWhMgMLpCL42cmSb ... more encrypted text ... Yxpzp_srpy4LHNdgHqhVBhqtDrjeKJDRfc1yk21P5PpfEBxn5MD4wITAJBgUrDgMCGgUABBQLBpq8y79Pq1TzG1Xf6OAjZzBZaQQUC4kD4CkcrH-WTQhJHud850ddn08CAwGGoA==\" , \"encryptedPassword\" : \"eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2Iiwia2lkIjoiRW1JY1UxOVdueSIsInZlcnNpb24iOiIxMC4xLjEuMCJ9..l6PJ55nSSvKHl0vSWTpkOA.i7hpnnu2yIByhyq_aGBCdaqS3u050yG8eMRGnLRx2Yk.Mo4WSkbbJyLISHq6i4nlVA\" } ] } But this master key dependent form can be converted to: { \"operationType\" : \"SAVE\" , \"items\" : [{ \"password\" : \"2FederateM0re\" , \"fileData\" : \"MIIRCQIBAzCCEM8GCSqGSIb3DQEHAaCCEMAEghC8MIIQuDCCC28GCSqGSIb3DQEHBqCCC2AwggtcAgEAMIILVQYJKoZIhvcNAQcBMBwGCiqGSIb3DQEMAQYwDgQIjXWLRGuGNIQCAggAgIILKOgCQ9onDqBPQsshsaS50OjWtj\\/7s47BUYal1YhO70fBup1a82WGHGhAvb\\/SY1yOhqQR+TloEBOPI5cExoGN\\/Gvw2Mw5\\/wkQZZMSHqxjz68KhN4B0hrsOf4rqShB7jsz9ebSml3r2w0sUZWR73GBtBt1Y3wIlXLS2WtqdtHra9VnUqp1eOk+xenjuWM+u2ndDD43GgKB3n8mNBSSVBqx6ne7aSRJRuAUd+HAzLvSeXjTPMObI1Jod2F+7 ... more base64 encoded exported .p12 ... 5QJ15OJp2iEoVBWxogKf64s2F0iIYPoo6yjNvlidZCevP564FwknWrHoD7R8cIBrhlCJQbEOpOhPg66r4MK1CeJ2poaKRlMS8HGcMRaTpaqD+pIlgmUS6xFw49vr9Kwfb7KteRsTkNR+I8A7HjUpuCMSUwIwYJKoZIhvcNAQkVMRYEFOb7g1xwDka5fJ4sqngEvzTyuWnpMDEwITAJBgUrDgMCGgUABBRlJ+D+FR\\/vQbaTGbKDFiBK\\/xDbqQQIAjLc+GgRg44CAggA\" , \"id\" : \"sslservercert\" }], \"resourceType\" : \"/keyPairs/sslServer\" } What happened: Exported the private key+cert of the server cert with alias sslservercert . Upon export, a password is requested and 2FederateM0re was used. This results in download of a password protected .p12 file. On data.json key name encryptedPassword converted to just password The value for fileData is replaced with a base64 encoded version of the exported .p12 file. This is a process that can be used for all encrypted items and environment specific items: Key Pairs (.p12) Trusted Certs (x509) Admin Password Data Store Passwords Integration Kit Properties Hostnames Now, if you follow this through the entire data.json, it would take a while, and you would be left with a file that is unacceptable for source control (since it's completely unencrypted). So, the next logical step is to abstract the unencrypted values and replace with variables. Then the values can be stored in a secrets management and the variablized file can be in source control. Doing all this would manually would take a long time, fortunately, there's the ping-bulkconfig-tool . Detailed steps for using the tool are documented next to where it is stored. The general concept is to point the tool at the data.json and a config file. After running you will be left with a data.json.subst and a list of environment variables waiting to be filled. The data.json.subst form of our example above will look like: { \"operationType\" : \"SAVE\" , \"items\" : [{ \"password\" : \"${keyPairs_sslServer_items_sslservercert_sslservercert_password}\" , \"fileData\" : \"${keyPairs_sslServer_items_sslservercert_sslservercert_fileData}\" , \"id\" : \"sslservercert\" }], \"resourceType\" : \"/keyPairs/sslServer\" } The variablized data.json.subst is now a good candidate to for committing to source control. The resulting env_vars file can be used as a guideline for secrets that should be managed externally and only delivered to the container/image as needed for it's specific environment.","title":"Make the Bulk API Export \"Profile Worthy\""},{"location":"how-to/buildPingFederateProfile/#additional-notes","text":"The bulk api export is intended to be used as a bulk import. The /bulk/import endpoint is destructive and overwrites the entire current admin config. If you are in a clustered environment, the PingFederate image will import the data.json and also replicate the configuration to engines in the cluster.","title":"Additional Notes"},{"location":"how-to/buildPingFederateProfile/#config-archive-profiles","text":"","title":"Config Archive Profiles"},{"location":"how-to/buildPingFederateProfile/#comparing-profile-methods","text":"Configuration Archive based profiles have some pros/cons to weigh when compared to bulk api export profiles. You will find bulk api export profiles to be more advantageous in most scenarios besides devops principle purists. Pros: The /data folder, opposed to a data.json file, is better for profile layering Configuration is available on engines at startup. This: lowers dependency on the admin at initial cluster startup enables mixed configurations in a single cluster. Canary-like \"roll-out\" instead of config pushed to all engines at once. Cons: The /data folder contains key pairs in a .jks so externally managing keys is very difficult. Encrypted data is scattered throughout the folder creating dependency on the master encryption key.","title":"Comparing Profile Methods"},{"location":"how-to/buildPingFederateProfile/#what-youll-do_1","text":"A summary of the resulting process: Export a data.zip Optionally, variablize Replace data folder","title":"What You'll Do"},{"location":"how-to/containerAnatomy/","text":"Deployment \u00b6 Any configuration that is deployed with one of our product containers can be considered a \"server profile\". A profile typically looks like a set of files. Profiles can be used in these ways: Pulled at startup Build into the image Mounted as a container volume Pulled at Startup \u00b6 Pass a Github-based URL and path as environment variables that point to a server profile. If the container sees these variables: SERVER_PROFILE_URL - The git URL with the server profile. SERVER_PROFILE_PATH - The location from the base of the URL with the specific server profile. This allows for several products server profile to be housed in the same git repo. SERVER_PROFILE_BRANCH (optional) - If other than the default branch (usually master or main), allows for specifying a different branch. Example might be a user's development branch before merging into master. The image will use these items to clone the repo at startup and pull the profile into the container. There is additional customizable functionality. This is the most common way that profiles are provided to containers, as it makes it very easy to provide a known starting state as well as tracking changes over time. See Private Github Repos for more information. Pros: Easily sharable, inherently source-controlled. Cons: Adds download time at container startup. Built Into The Image \u00b6 Build your own image from one of our Docker images and copy the profile files in. This is useful when you have no access to the Github repository, or if you're often spinning containers up and down. For example, if you made a Dockerfile at this location: https://github.com/pingidentity/pingidentity-server-profiles/tree/master/baseline, the relevant entries might look similar to this: FROM: pingidentity/pingfederate:edge COPY pingfederate/. /opt/in/. Pros: No download at startup, and no egress required. Cons: Tedious to build images when making iterative changes. Mounted as a Docker Volume \u00b6 Using docker-compose you can bind-mount a host file system location to a location in the container. This is useful when you're developing a server profile, and you want to be able to quickly make changes to the profile and spin up a container against it. For example, if you have a profile in same directory as your docker-compose.yaml file, you can add a bind-mount volume to /opt/in like this: volumes: - ./pingfederate:/opt/in Pros: Most iterative. There's no download time, and you can see the file system while you are working in the container. Cons: There's no great way to do this in Kubernetes or other platform orchestration tools.","title":"Deployment"},{"location":"how-to/containerAnatomy/#deployment","text":"Any configuration that is deployed with one of our product containers can be considered a \"server profile\". A profile typically looks like a set of files. Profiles can be used in these ways: Pulled at startup Build into the image Mounted as a container volume","title":"Deployment"},{"location":"how-to/containerAnatomy/#pulled-at-startup","text":"Pass a Github-based URL and path as environment variables that point to a server profile. If the container sees these variables: SERVER_PROFILE_URL - The git URL with the server profile. SERVER_PROFILE_PATH - The location from the base of the URL with the specific server profile. This allows for several products server profile to be housed in the same git repo. SERVER_PROFILE_BRANCH (optional) - If other than the default branch (usually master or main), allows for specifying a different branch. Example might be a user's development branch before merging into master. The image will use these items to clone the repo at startup and pull the profile into the container. There is additional customizable functionality. This is the most common way that profiles are provided to containers, as it makes it very easy to provide a known starting state as well as tracking changes over time. See Private Github Repos for more information. Pros: Easily sharable, inherently source-controlled. Cons: Adds download time at container startup.","title":"Pulled at Startup"},{"location":"how-to/containerAnatomy/#built-into-the-image","text":"Build your own image from one of our Docker images and copy the profile files in. This is useful when you have no access to the Github repository, or if you're often spinning containers up and down. For example, if you made a Dockerfile at this location: https://github.com/pingidentity/pingidentity-server-profiles/tree/master/baseline, the relevant entries might look similar to this: FROM: pingidentity/pingfederate:edge COPY pingfederate/. /opt/in/. Pros: No download at startup, and no egress required. Cons: Tedious to build images when making iterative changes.","title":"Built Into The Image"},{"location":"how-to/containerAnatomy/#mounted-as-a-docker-volume","text":"Using docker-compose you can bind-mount a host file system location to a location in the container. This is useful when you're developing a server profile, and you want to be able to quickly make changes to the profile and spin up a container against it. For example, if you have a profile in same directory as your docker-compose.yaml file, you can add a bind-mount volume to /opt/in like this: volumes: - ./pingfederate:/opt/in Pros: Most iterative. There's no download time, and you can see the file system while you are working in the container. Cons: There's no great way to do this in Kubernetes or other platform orchestration tools.","title":"Mounted as a Docker Volume"},{"location":"how-to/existingLicense/","text":"Using Existing Product License \u00b6 If you have an existing, valid product license for the product or products you'll be running, you can use this instead of the DevOps evaluation license. Prerequisites \u00b6 You've already been through Get Started to set up your DevOps environment and run a test deployment of the products. You've deployed an example stack. See Deploy an Example Stack . What You'll Do \u00b6 Use the instructions in any of these subtopics: License Declarations For Stacks to persist the license information in the local Docker volume that can be used for runtime startup information. See Save Your Configuration Changes for instructions in using local Docker volumes. License Declarations For Standalone Containers when bringing up standalone containers. Passing a License as a Kubernetes Secret to use an existing license with Kubernetes. License Declarations For Standalone Containers \u00b6 For a standalone container, use this syntax to make the license file available to the deployment: docker run \\ --name pingfederate \\ --volume <path>/pingfederate.lic>:/opt/in/instance/server/default/conf/pingfederate.lic \\ pingidentity/pingfederate:edge Where <path> and the /opt/in mount path are as specified for our Docker stacks above. License Declarations For Stacks \u00b6 For our Docker stacks, copy each license file to the /opt/in volume that you've mounted. The /opt/in directory overlays files onto the products runtime file system. The license needs to be named correctly and mounted in the exact location where the product checks for valid licenses. Add a volumes section to the container entry for each product for which you have a license file in the docker-compose.yaml file you're using for the stack. Under the volumes section, add a location to mount opt/in . An example using PingFederate: pingfederate : ... volumes : - <path>/pingfederate.lic:/opt/in/instance/server/default/conf/pingfederate.lic Where <path> is the location of your existing PingFederate license file. When the container starts, this will mount <path>/pingfederate.lic to this location in the container /opt/in/instance/server/default/conf/pingfederate.lic . The mount paths must match the expected license path for the product. Product File Name Mount Path PingFederate pingfederate.lic /opt/in/instance/server/default/conf/pingfederate.lic PingAccess pingaccess.lic /opt/in/instance/conf/pingaccess.lic PingDirectory PingDirectory.lic /opt/in/instance/PingDirectory.lic PingDataSync PingDirectory.lic /opt/in/instance/PingDirectory.lic PingDataGovernance PingDataGovernance.lic /opt/in/instance/PingDataGovernance.lic PingCentral pingcentral.lic /opt/in/instance/conf/pingcentral.lic Repeat this process for the remaining container entries for which you have an existing license. Passing a License as a Kubernetes Secret \u00b6 We'll use PingFederate as an example. You'll need to supply your PingFederate license file. The kustomize tool provides built-in generators for creating secrets. In this example, the secret will be generated using the pingfederate.lic file. You'll find the YAML files for this example in your local pingidentity-devops-getting-started/20-kubernetes/07-license-as-secret directory. Prerequisites \u00b6 kustomize kubectl Procedure \u00b6 Copy your PingFederate license file to your working directory. Rename the file to pingfederate.lic . Copy the YAML files from your local your local pingidentity-devops-getting-started/20-kubernetes/07-license-as-secret directory to your working directory. In the pingfederate.yaml file, declare the volume to use for the license: volumes : - name : <product-license-volume> secret : secretName : <pingfederate-license> Where <product-license-volume> is the volume where it will be referenced from the container, and <pingfederate-license> is your license information. Add the following values in the volumeMounts section: volumeMounts : - name : <product-license-volume> mountPath : \"/opt/in/instance/server/default/conf/pingfederate.lic\" subPath : pingfederate.lic readOnly : true Where: name matches the name value you specified in the volumes section. mountPath is the Docker bind-mount path used for the PingFederate license. subPath is the name of the license file to be created. readOnly is an optional attribute. In the kustomization.yaml file, add your license information to the secretGenerator section: secretGenerator : - files : - pingfederate.lic name : <pingfederate-license> type : Opaque The <pingfederate-license> value must match the <pingfederate-license> secretName value you specified in pingfederate.yaml . Deploy your license. In your working directory, enter: kustomize build . | kubectl apply -f - To clean up when you're finished, enter: kustomize build . | kubectl delete -f -","title":"Use Existing Licenses"},{"location":"how-to/existingLicense/#using-existing-product-license","text":"If you have an existing, valid product license for the product or products you'll be running, you can use this instead of the DevOps evaluation license.","title":"Using Existing Product License"},{"location":"how-to/existingLicense/#prerequisites","text":"You've already been through Get Started to set up your DevOps environment and run a test deployment of the products. You've deployed an example stack. See Deploy an Example Stack .","title":"Prerequisites"},{"location":"how-to/existingLicense/#what-youll-do","text":"Use the instructions in any of these subtopics: License Declarations For Stacks to persist the license information in the local Docker volume that can be used for runtime startup information. See Save Your Configuration Changes for instructions in using local Docker volumes. License Declarations For Standalone Containers when bringing up standalone containers. Passing a License as a Kubernetes Secret to use an existing license with Kubernetes.","title":"What You'll Do"},{"location":"how-to/existingLicense/#license-declarations-for-standalone-containers","text":"For a standalone container, use this syntax to make the license file available to the deployment: docker run \\ --name pingfederate \\ --volume <path>/pingfederate.lic>:/opt/in/instance/server/default/conf/pingfederate.lic \\ pingidentity/pingfederate:edge Where <path> and the /opt/in mount path are as specified for our Docker stacks above.","title":"License Declarations For Standalone Containers"},{"location":"how-to/existingLicense/#license-declarations-for-stacks","text":"For our Docker stacks, copy each license file to the /opt/in volume that you've mounted. The /opt/in directory overlays files onto the products runtime file system. The license needs to be named correctly and mounted in the exact location where the product checks for valid licenses. Add a volumes section to the container entry for each product for which you have a license file in the docker-compose.yaml file you're using for the stack. Under the volumes section, add a location to mount opt/in . An example using PingFederate: pingfederate : ... volumes : - <path>/pingfederate.lic:/opt/in/instance/server/default/conf/pingfederate.lic Where <path> is the location of your existing PingFederate license file. When the container starts, this will mount <path>/pingfederate.lic to this location in the container /opt/in/instance/server/default/conf/pingfederate.lic . The mount paths must match the expected license path for the product. Product File Name Mount Path PingFederate pingfederate.lic /opt/in/instance/server/default/conf/pingfederate.lic PingAccess pingaccess.lic /opt/in/instance/conf/pingaccess.lic PingDirectory PingDirectory.lic /opt/in/instance/PingDirectory.lic PingDataSync PingDirectory.lic /opt/in/instance/PingDirectory.lic PingDataGovernance PingDataGovernance.lic /opt/in/instance/PingDataGovernance.lic PingCentral pingcentral.lic /opt/in/instance/conf/pingcentral.lic Repeat this process for the remaining container entries for which you have an existing license.","title":"License Declarations For Stacks"},{"location":"how-to/existingLicense/#passing-a-license-as-a-kubernetes-secret","text":"We'll use PingFederate as an example. You'll need to supply your PingFederate license file. The kustomize tool provides built-in generators for creating secrets. In this example, the secret will be generated using the pingfederate.lic file. You'll find the YAML files for this example in your local pingidentity-devops-getting-started/20-kubernetes/07-license-as-secret directory.","title":"Passing a License as a Kubernetes Secret"},{"location":"how-to/existingLicense/#prerequisites_1","text":"kustomize kubectl","title":"Prerequisites"},{"location":"how-to/existingLicense/#procedure","text":"Copy your PingFederate license file to your working directory. Rename the file to pingfederate.lic . Copy the YAML files from your local your local pingidentity-devops-getting-started/20-kubernetes/07-license-as-secret directory to your working directory. In the pingfederate.yaml file, declare the volume to use for the license: volumes : - name : <product-license-volume> secret : secretName : <pingfederate-license> Where <product-license-volume> is the volume where it will be referenced from the container, and <pingfederate-license> is your license information. Add the following values in the volumeMounts section: volumeMounts : - name : <product-license-volume> mountPath : \"/opt/in/instance/server/default/conf/pingfederate.lic\" subPath : pingfederate.lic readOnly : true Where: name matches the name value you specified in the volumes section. mountPath is the Docker bind-mount path used for the PingFederate license. subPath is the name of the license file to be created. readOnly is an optional attribute. In the kustomization.yaml file, add your license information to the secretGenerator section: secretGenerator : - files : - pingfederate.lic name : <pingfederate-license> type : Opaque The <pingfederate-license> value must match the <pingfederate-license> secretName value you specified in pingfederate.yaml . Deploy your license. In your working directory, enter: kustomize build . | kubectl apply -f - To clean up when you're finished, enter: kustomize build . | kubectl delete -f -","title":"Procedure"},{"location":"how-to/manage/","text":"Managing Deployments \u00b6 In addition to Customizing Deployments , you'll also need to maintain your deployments over time, as new versions of our products are released, and as you tune your deployments to better reflect your changing needs.","title":"Introduction"},{"location":"how-to/manage/#managing-deployments","text":"In addition to Customizing Deployments , you'll also need to maintain your deployments over time, as new versions of our products are released, and as you tune your deployments to better reflect your changing needs.","title":"Managing Deployments"},{"location":"how-to/privateRepos/","text":"Using Private Github Repositories \u00b6 Generally, you'll not want your server profiles to be public, and will instead want to persist your server profiles in private GitHub repositories. To use server profiles with private repositories, you'll need to generate an access token in GitHub, then specify the access token in the URL you assign to the SERVER_PROFILE_URL environment variable in your YAML files. Create a Github Access Token \u00b6 In Github, go to Settings --> Developer Settings --> Personal access tokens . Click Generate new token and assign the token a name. Grant the token privilege to the repo group. Copy the token to a secure location. You'll won't be able to view the token again. Scroll to the bottom of the page and click Generate Token . Using The Token In YAML \u00b6 To use the token in your YAML file, include it in the SERVER_PROFILE_URL environment variable using this format: https:// < github-username > : < github-token > @github.com/ < your-repository > .git For example: SERVER_PROFILE_URL=https://github_user:zqb4famrbadjv39jdi6shvl1xvozut7tamd5v6eva@github.com/pingidentity/server_profile.git Using Git Credentials in Profile URL \u00b6 Typically, variables in a SERVER_PROFILE_URL string will not be replaced. However, certain Git user and password variables, can be replaced: Include either or both ${SERVER_PROFILE_GIT_USER} and ${SERVER_PROFILE_GIT_PASSWORD} in your server profile URL to substitute for those variables using values defined in your YAML files. For example: SERVER_PROFILE_URL = https:// ${ SERVER_PROFILE_GIT_USER } : ${ SERVER_PROFILE_GIT_PASSWORD } @github.com/pingidentity/server_profile.git When using layered server profiles, each layer can use the base user and password variables, or can define values specific to that layer. For example, for a license server profile layer, you can use the SERVER_PROFILE_LICENSE_GIT_USER and SERVER_PROFILE_LICENSE_GIT_PASSWORD variables, and substitute for those variables using values defined in your YAML files.","title":"Private Github Repos"},{"location":"how-to/privateRepos/#using-private-github-repositories","text":"Generally, you'll not want your server profiles to be public, and will instead want to persist your server profiles in private GitHub repositories. To use server profiles with private repositories, you'll need to generate an access token in GitHub, then specify the access token in the URL you assign to the SERVER_PROFILE_URL environment variable in your YAML files.","title":"Using Private Github Repositories"},{"location":"how-to/privateRepos/#create-a-github-access-token","text":"In Github, go to Settings --> Developer Settings --> Personal access tokens . Click Generate new token and assign the token a name. Grant the token privilege to the repo group. Copy the token to a secure location. You'll won't be able to view the token again. Scroll to the bottom of the page and click Generate Token .","title":"Create a Github Access Token"},{"location":"how-to/privateRepos/#using-the-token-in-yaml","text":"To use the token in your YAML file, include it in the SERVER_PROFILE_URL environment variable using this format: https:// < github-username > : < github-token > @github.com/ < your-repository > .git For example: SERVER_PROFILE_URL=https://github_user:zqb4famrbadjv39jdi6shvl1xvozut7tamd5v6eva@github.com/pingidentity/server_profile.git","title":"Using The Token In YAML"},{"location":"how-to/privateRepos/#using-git-credentials-in-profile-url","text":"Typically, variables in a SERVER_PROFILE_URL string will not be replaced. However, certain Git user and password variables, can be replaced: Include either or both ${SERVER_PROFILE_GIT_USER} and ${SERVER_PROFILE_GIT_PASSWORD} in your server profile URL to substitute for those variables using values defined in your YAML files. For example: SERVER_PROFILE_URL = https:// ${ SERVER_PROFILE_GIT_USER } : ${ SERVER_PROFILE_GIT_PASSWORD } @github.com/pingidentity/server_profile.git When using layered server profiles, each layer can use the base user and password variables, or can define values specific to that layer. For example, for a license server profile layer, you can use the SERVER_PROFILE_LICENSE_GIT_USER and SERVER_PROFILE_LICENSE_GIT_PASSWORD variables, and substitute for those variables using values defined in your YAML files.","title":"Using Git Credentials in Profile URL"},{"location":"how-to/profiles/","text":"Customizing Server Profiles \u00b6 When you deployed the full stack of product containers in Getting Started , you were employing the server profiles associated with each of our products. In the YAML files, you'll see entries such as this for each product instance: environment : - SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_PATH=baseline/pingaccess Our pingidentity-server-profiles repository indicated by the SERVER_PROFILE_URL environment variable, contains the server profiles we use for our DevOps deployment examples. The SERVER_PROFILE_PATH environment variable indicates the location of the product profile data to use. In the example above, the PingAccess profile data is located in the baseline/pingaccess directory. We use environment variables for certain startup and runtime configuration settings of both standalone and orchestrated deployments. There are environment variables that are common to all product images. You'll find these in the PingBase Image Directory . There are also product-specific environment variables. You'll find these in the Docker Image Reference for each available product. Prerequisite \u00b6 You've already been through Get Started to set up your DevOps environment and run a test deployment of the products. You understand the Anatomy of the Product Containers . What You'll Do \u00b6 Add or change the environment variables used for any of our server profiles to better fit your purposes. These environment variables are located in the Server Profiles Repository for each product. For example, the location for the env_vars file for PingAccess is located in the baseline/pingaccess server profile . Modify one of our server profiles to reflect an existing Ping Identity product installation in your organization. You can do this by forking our server profiles repository ( https://github.com/pingidentity/pingidentity-server-profiles ) to your Github repository, or by using local directories. Add or Change Environment Variables \u00b6 Select any environment variables to add from either the Docker Images in the Docker Images Reference for the product-specific environment variables, or the PingBase Image Directory for the environment variables common to all of our products. Select the product whose profile you want to modify from the baseline , getting-started , or simple-sync directories in the Server Profiles Repository . Open the env_vars file associated with the product and add any of the environment variables you've selected, or change the existing environment variables to fit your purpose. Modify a Server Profile \u00b6 You can modify one of our server profiles based on data from your existing Ping Identity product installation. Modify a server profile in either of these ways: Using your Github repository Using local directories Using Your Github Repository \u00b6 We'll use a PingFederate installation as an example. This method uses a server profile provided through a Github URL and assigned to the SERVER_PROFILE_PATH environment variable (such as, --env SERVER_PROFILE_PATH=getting-started/pingfederate ). Export a configuration archive as a *.zip file from a PingFederate installation to a local directory. Make sure this is exported as a .zip rather than compressing it yourself. Log in to Github and fork https://github.com/pingidentity/pingidentity-server-profiles into your own GitHub repository. Open a terminal, create a new directory, and clone your Github repository to a local directory. For example: mkdir /tmp/pf_to_docker cd /tmp/pf_to_docker git clone https://github.com/<github-username>/pingidentity-server-profiles.git Where <github-username> is the name you used to log in to the Github account. Go to the location where you cloned your fork of our pingidentity-server-profiles repository, and replace the /data directory in getting-started/pingfederate/instance/server/default with the data directory you exported from your existing PingFederate installation. For example: cd pingidentity-server-profiles/getting-started/pingfederate/instance/server/default rm -rf data unzip -qd data <path_to_your_configuration_archive>/data.zip Where <path_to_your_configuration_archive> is the location for your exported PingFederate configuration archive. You now have a local server profile based on your existing PingFederate installation. Pushing to Github We recommend you push to Github only what is necessary for your customizations. Our Docker images create the /opt/out directory using a product's base install and layering a profile (set of files) on top. Push your changes (your local server profile) to the Github repository where you forked our server profile repository. You now have a server profile available through a Github URL. Deploy the PingFederate container. The environment variables SERVER_PROFILE_URL and SERVER_PROFILE_PATH direct Docker to use the server profile you've modified and pushed to Github. Saving Changes To save any changes you make after the container is running, add the entry --volume <local-path>:/opt/out to the docker run command, where <local-path> is a directory you've not already created. See Saving Your Changes for more information. For example: docker run \\ --name pingfederate \\ --publish 9999 :9999 \\ --publish 9031 :9031 \\ --detach \\ --env SERVER_PROFILE_URL = https://github.com/<your_username>/pingidentity-server-profiles.git \\ --env SERVER_PROFILE_PATH = getting-started/pingfederate \\ --env-file ~/.pingidentity/devops \\ pingidentity/pingfederate:edge Private Repo If your GitHub server-profile repo is private, use the username:token format so the container can access the repository. For example, https://github.com/<your_username>:<your_access_token>/pingidentity-server-profiles.git . See Using Private Github Repositories for more information. To display the logs as the container starts up, enter: docker container logs -f pingfederate In a browser, go to https://localhost:9999/pingfederate/app to display the PingFederate console. Using Local Directories \u00b6 This method is particularly helpful when developing locally and the configuration is not ready to be distributed (using Github, for example). We'll use PingFederate as an example. The local directories used by our containers to persist state and data, /opt/in and /opt/out , will be bound to another local directory and mounted as Docker volumes. This is our infrastructure for modifying the server profile. Bind Mounts in Production Docker recommends that you never use bind mounts in a production environment. This method is solely for developing server profiles. See the Docker Documentation for more information. The /opt/out directory All configurations and changes during our container runtimes (persisted data) are captured here. For example, the PingFederate image /opt/out/instance will contain much of the typical PingFederate root directory: . \u251c\u2500\u2500 README.md \u251c\u2500\u2500 SNMP \u251c\u2500\u2500 bin \u251c\u2500\u2500 connection_export_examples \u251c\u2500\u2500 etc \u251c\u2500\u2500 legal \u251c\u2500\u2500 lib \u251c\u2500\u2500 log \u251c\u2500\u2500 modules \u251c\u2500\u2500 sbin \u251c\u2500\u2500 sdk \u251c\u2500\u2500 server \u251c\u2500\u2500 tools \u2514\u2500\u2500 work The /opt/in directory If a mounted opt/in directory exists, our containers will reference this directory at startup for any server profile structures or other relevant files. This method is in contrast to a server profile provided using a Github URL assigned to the SERVER_PROFILE_PATH environment variable (such as, --env SERVER_PROFILE_PATH=getting-started/pingfederate ). See Server profile structures for the data each product writes to a mounted /opt/in directory. These directories are useful for building and working with local server-profiles. The /opt/in directory is particularly valuable if you do not want your containers to access Github for data (the default for our server profiles). Here's an example, again using PingFederate: Deploy PingFederate using our sample getting-started Server Profile , and mount /opt/out to a local directory. For example: docker run \\ --name pingfederate \\ --publish 9999 :9999 \\ --detach \\ --env SERVER_PROFILE_URL = https://github.com/pingidentity/pingidentity-server-profiles.git \\ --env SERVER_PROFILE_PATH = getting-started/pingfederate \\ --env-file ~/.pingidentity/devops \\ --volume /tmp/docker/pf:/opt/out \\ pingidentity/pingfederate:edge Make sure the local directory (in this case, /tmp/docker/pf ) is not already created. Docker needs to create this directory for the mount to /opt/out . Go to the mounted local directory (in this case, /tmp/docker/pf ), then make and save some configuration changes to PingFederate using the management console. As you save the changes, you'll be able to see the files in the mounted directory change. For PingFederate, an instance directory is created. This is a PingFederate server profile. Stop & remove the container and start a new container, adding another /tmp/docker/pf bind mounted volume, this time to /opt/in . For example: docker container rm pingfederate docker run \\ --name pingfederate-local \\ --publish 9999 :9999 \\ --detach \\ --volume /tmp/docker/pf:/opt/out \\ --volume /tmp/docker/pf:/opt/in \\ pingidentity/pingfederate:edge The new container will now use the changes you made using the PingFederate console. In the logs you can see where /opt/in is used: sh docker logs pingfederate-local Finally, stop and remove the new container. Remember your /tmp/docker/pf directory will stay until you remove it (or your machine is rebooted, as this is in /tmp): docker container rm pingfederate-local If you also want to remove your work, enter: rm -rf /tmp/docker/pf","title":"Customization"},{"location":"how-to/profiles/#customizing-server-profiles","text":"When you deployed the full stack of product containers in Getting Started , you were employing the server profiles associated with each of our products. In the YAML files, you'll see entries such as this for each product instance: environment : - SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_PATH=baseline/pingaccess Our pingidentity-server-profiles repository indicated by the SERVER_PROFILE_URL environment variable, contains the server profiles we use for our DevOps deployment examples. The SERVER_PROFILE_PATH environment variable indicates the location of the product profile data to use. In the example above, the PingAccess profile data is located in the baseline/pingaccess directory. We use environment variables for certain startup and runtime configuration settings of both standalone and orchestrated deployments. There are environment variables that are common to all product images. You'll find these in the PingBase Image Directory . There are also product-specific environment variables. You'll find these in the Docker Image Reference for each available product.","title":"Customizing Server Profiles"},{"location":"how-to/profiles/#prerequisite","text":"You've already been through Get Started to set up your DevOps environment and run a test deployment of the products. You understand the Anatomy of the Product Containers .","title":"Prerequisite"},{"location":"how-to/profiles/#what-youll-do","text":"Add or change the environment variables used for any of our server profiles to better fit your purposes. These environment variables are located in the Server Profiles Repository for each product. For example, the location for the env_vars file for PingAccess is located in the baseline/pingaccess server profile . Modify one of our server profiles to reflect an existing Ping Identity product installation in your organization. You can do this by forking our server profiles repository ( https://github.com/pingidentity/pingidentity-server-profiles ) to your Github repository, or by using local directories.","title":"What You'll Do"},{"location":"how-to/profiles/#add-or-change-environment-variables","text":"Select any environment variables to add from either the Docker Images in the Docker Images Reference for the product-specific environment variables, or the PingBase Image Directory for the environment variables common to all of our products. Select the product whose profile you want to modify from the baseline , getting-started , or simple-sync directories in the Server Profiles Repository . Open the env_vars file associated with the product and add any of the environment variables you've selected, or change the existing environment variables to fit your purpose.","title":"Add or Change Environment Variables"},{"location":"how-to/profiles/#modify-a-server-profile","text":"You can modify one of our server profiles based on data from your existing Ping Identity product installation. Modify a server profile in either of these ways: Using your Github repository Using local directories","title":"Modify a Server Profile"},{"location":"how-to/profiles/#using-your-github-repository","text":"We'll use a PingFederate installation as an example. This method uses a server profile provided through a Github URL and assigned to the SERVER_PROFILE_PATH environment variable (such as, --env SERVER_PROFILE_PATH=getting-started/pingfederate ). Export a configuration archive as a *.zip file from a PingFederate installation to a local directory. Make sure this is exported as a .zip rather than compressing it yourself. Log in to Github and fork https://github.com/pingidentity/pingidentity-server-profiles into your own GitHub repository. Open a terminal, create a new directory, and clone your Github repository to a local directory. For example: mkdir /tmp/pf_to_docker cd /tmp/pf_to_docker git clone https://github.com/<github-username>/pingidentity-server-profiles.git Where <github-username> is the name you used to log in to the Github account. Go to the location where you cloned your fork of our pingidentity-server-profiles repository, and replace the /data directory in getting-started/pingfederate/instance/server/default with the data directory you exported from your existing PingFederate installation. For example: cd pingidentity-server-profiles/getting-started/pingfederate/instance/server/default rm -rf data unzip -qd data <path_to_your_configuration_archive>/data.zip Where <path_to_your_configuration_archive> is the location for your exported PingFederate configuration archive. You now have a local server profile based on your existing PingFederate installation. Pushing to Github We recommend you push to Github only what is necessary for your customizations. Our Docker images create the /opt/out directory using a product's base install and layering a profile (set of files) on top. Push your changes (your local server profile) to the Github repository where you forked our server profile repository. You now have a server profile available through a Github URL. Deploy the PingFederate container. The environment variables SERVER_PROFILE_URL and SERVER_PROFILE_PATH direct Docker to use the server profile you've modified and pushed to Github. Saving Changes To save any changes you make after the container is running, add the entry --volume <local-path>:/opt/out to the docker run command, where <local-path> is a directory you've not already created. See Saving Your Changes for more information. For example: docker run \\ --name pingfederate \\ --publish 9999 :9999 \\ --publish 9031 :9031 \\ --detach \\ --env SERVER_PROFILE_URL = https://github.com/<your_username>/pingidentity-server-profiles.git \\ --env SERVER_PROFILE_PATH = getting-started/pingfederate \\ --env-file ~/.pingidentity/devops \\ pingidentity/pingfederate:edge Private Repo If your GitHub server-profile repo is private, use the username:token format so the container can access the repository. For example, https://github.com/<your_username>:<your_access_token>/pingidentity-server-profiles.git . See Using Private Github Repositories for more information. To display the logs as the container starts up, enter: docker container logs -f pingfederate In a browser, go to https://localhost:9999/pingfederate/app to display the PingFederate console.","title":"Using Your Github Repository"},{"location":"how-to/profiles/#using-local-directories","text":"This method is particularly helpful when developing locally and the configuration is not ready to be distributed (using Github, for example). We'll use PingFederate as an example. The local directories used by our containers to persist state and data, /opt/in and /opt/out , will be bound to another local directory and mounted as Docker volumes. This is our infrastructure for modifying the server profile. Bind Mounts in Production Docker recommends that you never use bind mounts in a production environment. This method is solely for developing server profiles. See the Docker Documentation for more information. The /opt/out directory All configurations and changes during our container runtimes (persisted data) are captured here. For example, the PingFederate image /opt/out/instance will contain much of the typical PingFederate root directory: . \u251c\u2500\u2500 README.md \u251c\u2500\u2500 SNMP \u251c\u2500\u2500 bin \u251c\u2500\u2500 connection_export_examples \u251c\u2500\u2500 etc \u251c\u2500\u2500 legal \u251c\u2500\u2500 lib \u251c\u2500\u2500 log \u251c\u2500\u2500 modules \u251c\u2500\u2500 sbin \u251c\u2500\u2500 sdk \u251c\u2500\u2500 server \u251c\u2500\u2500 tools \u2514\u2500\u2500 work The /opt/in directory If a mounted opt/in directory exists, our containers will reference this directory at startup for any server profile structures or other relevant files. This method is in contrast to a server profile provided using a Github URL assigned to the SERVER_PROFILE_PATH environment variable (such as, --env SERVER_PROFILE_PATH=getting-started/pingfederate ). See Server profile structures for the data each product writes to a mounted /opt/in directory. These directories are useful for building and working with local server-profiles. The /opt/in directory is particularly valuable if you do not want your containers to access Github for data (the default for our server profiles). Here's an example, again using PingFederate: Deploy PingFederate using our sample getting-started Server Profile , and mount /opt/out to a local directory. For example: docker run \\ --name pingfederate \\ --publish 9999 :9999 \\ --detach \\ --env SERVER_PROFILE_URL = https://github.com/pingidentity/pingidentity-server-profiles.git \\ --env SERVER_PROFILE_PATH = getting-started/pingfederate \\ --env-file ~/.pingidentity/devops \\ --volume /tmp/docker/pf:/opt/out \\ pingidentity/pingfederate:edge Make sure the local directory (in this case, /tmp/docker/pf ) is not already created. Docker needs to create this directory for the mount to /opt/out . Go to the mounted local directory (in this case, /tmp/docker/pf ), then make and save some configuration changes to PingFederate using the management console. As you save the changes, you'll be able to see the files in the mounted directory change. For PingFederate, an instance directory is created. This is a PingFederate server profile. Stop & remove the container and start a new container, adding another /tmp/docker/pf bind mounted volume, this time to /opt/in . For example: docker container rm pingfederate docker run \\ --name pingfederate-local \\ --publish 9999 :9999 \\ --detach \\ --volume /tmp/docker/pf:/opt/out \\ --volume /tmp/docker/pf:/opt/in \\ pingidentity/pingfederate:edge The new container will now use the changes you made using the PingFederate console. In the logs you can see where /opt/in is used: sh docker logs pingfederate-local Finally, stop and remove the new container. Remember your /tmp/docker/pf directory will stay until you remove it (or your machine is rebooted, as this is in /tmp): docker container rm pingfederate-local If you also want to remove your work, enter: rm -rf /tmp/docker/pf","title":"Using Local Directories"},{"location":"how-to/profilesLayered/","text":"Layering Server Profiles \u00b6 One of the benefits of our Docker images is the ability to layer product configuration. By using small discrete portions of your configuration, you can build and assemble a server profile based on multiple installations of a product. A typical organization can have multiple installations of our products, each using different configurations. By layering the server profiles, you can reuse the configurations that are common across environments, leading to fewer configurations to manage. You can have as many layers as needed. Each layer of the configuration is copied on top of the container's filesystem (not merged). Layer Precedence The profile layers are applied starting at the top layer and ending at the base layer. This may not be apparent. In our examples, the base profile layer appears first in the docker-compose.yaml file. In these cases, it's a child-before-parent order of application. Prerequisite \u00b6 You've already been through Get Started to set up your DevOps environment and run a test deployment of the products. What You'll Do \u00b6 Create a layered server profile. Assign the environment variables for the deployment. Deploy the layered server profile. Create a Layered Server Profile \u00b6 We'll use PingFederate and our server profile located in the pingidentity-server-profiles repository. We recommend you fork this repository to your Github repository, then pull your Github repository to to a local directory. When we've finished creating the layered profile, you can then push your updates the your Github repository, and reference your Github repository as an environment variable to run the deployment. We'll create separate layers for: Product license Extensions (such as, Integration Kits and Connectors) OAuth Playground For this example, these layers will be applied on top of the PingFederate server profile. However, you can span configurations across multiple repositories if you want. The complete working, layered server profile of the PingFederate example we're building here is in the pingidentity-server-profiles/layered-profiles directory. Because PingFederate's configuration is file-based, the layering works by copying configurations on top of the PingFederate container\u2019s file system. Files Copied Files are copied, not merged. It's best practice to only layer items that won't be impacted by other configuration files. Create the Base Directories \u00b6 Create a working directory named layered_profiles and within that directory create license , extensions , and oauth directories. When completed your directory structure should be: \u2514\u2500\u2500 layered_profiles \u251c\u2500\u2500 extensions \u251c\u2500\u2500 license \u2514\u2500\u2500 oauth Construct the License Layer \u00b6 Go to the license directory and create a pingfederate subdirectory. The PingFederate license file resides in the /instance/server/default/conf/ path. Create that directory path under the pingfederate directory. For example: mkdir -p instance/server/default/conf/ Your license profile path should look like this: \u2514\u2500\u2500 license \u2514\u2500\u2500 pingfederate \u2514\u2500\u2500 instance \u2514\u2500\u2500 server \u2514\u2500\u2500 default \u2514\u2500\u2500 conf \u2514\u2500\u2500 pingfederate.lic Copy your pingfederate.lic file to license/pingfederate/instance/server/default/conf . If you're using the DevOps evaluation license, when the PingFederate container is running, the license is located in the Docker file system's /opt/out/instance/server/default/conf directory. You can copy the pingfederate.lic file from the Docker file system using the syntax: docker cp <container> <source-location> <target-location> For example: docker cp \\ pingfederate \\ /opt/in/instance/server/default/conf/pingfederate.lic \\ ${ HOME } /projects/devops/layered_profiles/license/pingfederate/instance/server/default/conf Using the ping-devops tool ping-devops generate license pingfederate > \\ ${ HOME } /projects/devops/layered_profiles/license/pingfederate/instance/server/default/conf Build Extensions Layer \u00b6 Go to the layered-profiles/extensions directory, and create a pingfederate subdirectory. The PingFederate extensions reside in the /instance/server/default/deploy path. Create that directory path under the pingfederate directory. For example: mkdir -p instance/server/default/deploy Copy to this directory ( layered-profiles/extensions/pingfederate/instance/server/default/deploy ) the extensions you want to be available to PingFederate. The extensions profile path should look similar to this (extensions will vary based on your requirements): \u2514\u2500\u2500 extensions \u2514\u2500\u2500 pingfederate \u2514\u2500\u2500 instance \u2514\u2500\u2500 server \u2514\u2500\u2500 default \u2514\u2500\u2500 deploy \u251c\u2500\u2500 pf-aws-quickconnection-2.0.jar \u251c\u2500\u2500 pf-azure-ad-pcv-1.2.jar \u2514\u2500\u2500 pf-slack-quickconnection-3.0.jar Build OAuth Layer \u00b6 Go to the layered-profiles/oauth directory, and create a pingfederate subdirectory. mkdir -p instance/server/default/pingfederate OAuth Playground for PingFederate is also located in the /instance/server/default/deploy directory, like other extensions. For this example, we're building OAuth Playground into its own layer to show that it's optional for PingFederate deployments. Copy the OAuthPlayground.war file to layered-profiles/oauth/pingfederate/instance/server/default/deploy . Your OAuth profile layer should look like this: \u2514\u2500\u2500 oauth \u2514\u2500\u2500 pingfederate \u2514\u2500\u2500 instance \u2514\u2500\u2500 server \u2514\u2500\u2500 default \u2514\u2500\u2500 deploy \u2514\u2500\u2500 OAuthPlayground.war Assign Environment Variables \u00b6 We'll assign the environment variables for use in a Docker Compose YAML file. However, you can use this technique with any Docker or Kubernetes deployment. If you're intending on using your Github repository for the deployment, in the following examples, replace SERVER_PROFILE_URL = https://github.com/pingidentity/pingidentity-server-profiles.git with SERVER_PROFILE_URL = https://github.com/<your-username>/pingidentity-server-profiles.git Private Github Repo If your GitHub server-profile repo is private, use the username:token format so the container can access the repository. For example, https://github.com/<your_username>:<your_access_token>/pingidentity-server-profiles.git . See Using Private Github Repositories for more information. Create a new docker-compose.yaml file. Add your license profile to the YAML file. For example: - SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_PATH=layered-profiles/license/pingfederate SERVER_PROFILE supports URL , PATH , BRANCH and PARENT variables. Using SERVER_PROFILE_PARENT , we can instruct the container to retrieve its parent configuration. We'll specify the extensions profile as the parent: - SERVER_PROFILE_PARENT=EXTENSIONS SERVER_PROFILE can be extended to reference additional profiles. Since we specified the license profile's parent as EXTENSIONS , we can extend SERVER_PROFILE by referencing the EXTENSIONS profile (prior to the URL and PATH variables): - SERVER_PROFILE_EXTENSIONS_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_EXTENSIONS_PATH=layered-profiles/extensions/pingfederate Set the EXTENSIONS parent to OAUTH : - SERVER_PROFILE_EXTENSIONS_PARENT=OAUTH Then set the URL and PATH for the OAUTH profile: - SERVER_PROFILE_OAUTH_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_OAUTH_PATH=layered-profiles/oauth/pingfederate Set GETTING_STARTED as the OAUTH parent and declare the URL and PATH : - SERVER_PROFILE_OAUTH_PARENT=GETTING_STARTED - SERVER_PROFILE_GETTING_STARTED_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_GETTING_STARTED_PATH=getting-started/pingfederate Because the GETTING_STARTED profile is the last profile to add, it won't have a parent. Your environment section of the docker-compose.yaml file should look similar to this: environment : # **** SERVER PROFILES BEGIN **** # Server Profile - Product License - SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_PATH=layered-profiles/license/pingfederate - SERVER_PROFILE_PARENT=EXTENSIONS # Server Profile - Extensions - SERVER_PROFILE_EXTENSIONS_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_EXTENSIONS_PATH=layered-profiles/extensions/pingfederate - SERVER_PROFILE_EXTENSIONS_PARENT=OAUTH # Server Profile - OAUTH - SERVER_PROFILE_OAUTH_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_OAUTH_PATH=layered-profiles/oauth/pingfederate - SERVER_PROFILE_OAUTH_PARENT=GETTING_STARTED # Base Server Profile - SERVER_PROFILE_GETTING_STARTED_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_GETTING_STARTED_PATH=getting-started/pingfederate # **** SERVER PROFILE END **** Deploy Layered Profile \u00b6 Push your profiles and updated docker-compose.yaml file to your GitHub repository. Deploy the stack with the layered profiles. To view this example in its entirety, including the profile layers and docker-compose.yaml file, see the pingidentity-server-profiles/layered-profiles directory.","title":"Layering"},{"location":"how-to/profilesLayered/#layering-server-profiles","text":"One of the benefits of our Docker images is the ability to layer product configuration. By using small discrete portions of your configuration, you can build and assemble a server profile based on multiple installations of a product. A typical organization can have multiple installations of our products, each using different configurations. By layering the server profiles, you can reuse the configurations that are common across environments, leading to fewer configurations to manage. You can have as many layers as needed. Each layer of the configuration is copied on top of the container's filesystem (not merged). Layer Precedence The profile layers are applied starting at the top layer and ending at the base layer. This may not be apparent. In our examples, the base profile layer appears first in the docker-compose.yaml file. In these cases, it's a child-before-parent order of application.","title":"Layering Server Profiles"},{"location":"how-to/profilesLayered/#prerequisite","text":"You've already been through Get Started to set up your DevOps environment and run a test deployment of the products.","title":"Prerequisite"},{"location":"how-to/profilesLayered/#what-youll-do","text":"Create a layered server profile. Assign the environment variables for the deployment. Deploy the layered server profile.","title":"What You'll Do"},{"location":"how-to/profilesLayered/#create-a-layered-server-profile","text":"We'll use PingFederate and our server profile located in the pingidentity-server-profiles repository. We recommend you fork this repository to your Github repository, then pull your Github repository to to a local directory. When we've finished creating the layered profile, you can then push your updates the your Github repository, and reference your Github repository as an environment variable to run the deployment. We'll create separate layers for: Product license Extensions (such as, Integration Kits and Connectors) OAuth Playground For this example, these layers will be applied on top of the PingFederate server profile. However, you can span configurations across multiple repositories if you want. The complete working, layered server profile of the PingFederate example we're building here is in the pingidentity-server-profiles/layered-profiles directory. Because PingFederate's configuration is file-based, the layering works by copying configurations on top of the PingFederate container\u2019s file system. Files Copied Files are copied, not merged. It's best practice to only layer items that won't be impacted by other configuration files.","title":"Create a Layered Server Profile"},{"location":"how-to/profilesLayered/#create-the-base-directories","text":"Create a working directory named layered_profiles and within that directory create license , extensions , and oauth directories. When completed your directory structure should be: \u2514\u2500\u2500 layered_profiles \u251c\u2500\u2500 extensions \u251c\u2500\u2500 license \u2514\u2500\u2500 oauth","title":"Create the Base Directories"},{"location":"how-to/profilesLayered/#construct-the-license-layer","text":"Go to the license directory and create a pingfederate subdirectory. The PingFederate license file resides in the /instance/server/default/conf/ path. Create that directory path under the pingfederate directory. For example: mkdir -p instance/server/default/conf/ Your license profile path should look like this: \u2514\u2500\u2500 license \u2514\u2500\u2500 pingfederate \u2514\u2500\u2500 instance \u2514\u2500\u2500 server \u2514\u2500\u2500 default \u2514\u2500\u2500 conf \u2514\u2500\u2500 pingfederate.lic Copy your pingfederate.lic file to license/pingfederate/instance/server/default/conf . If you're using the DevOps evaluation license, when the PingFederate container is running, the license is located in the Docker file system's /opt/out/instance/server/default/conf directory. You can copy the pingfederate.lic file from the Docker file system using the syntax: docker cp <container> <source-location> <target-location> For example: docker cp \\ pingfederate \\ /opt/in/instance/server/default/conf/pingfederate.lic \\ ${ HOME } /projects/devops/layered_profiles/license/pingfederate/instance/server/default/conf Using the ping-devops tool ping-devops generate license pingfederate > \\ ${ HOME } /projects/devops/layered_profiles/license/pingfederate/instance/server/default/conf","title":"Construct the License Layer"},{"location":"how-to/profilesLayered/#build-extensions-layer","text":"Go to the layered-profiles/extensions directory, and create a pingfederate subdirectory. The PingFederate extensions reside in the /instance/server/default/deploy path. Create that directory path under the pingfederate directory. For example: mkdir -p instance/server/default/deploy Copy to this directory ( layered-profiles/extensions/pingfederate/instance/server/default/deploy ) the extensions you want to be available to PingFederate. The extensions profile path should look similar to this (extensions will vary based on your requirements): \u2514\u2500\u2500 extensions \u2514\u2500\u2500 pingfederate \u2514\u2500\u2500 instance \u2514\u2500\u2500 server \u2514\u2500\u2500 default \u2514\u2500\u2500 deploy \u251c\u2500\u2500 pf-aws-quickconnection-2.0.jar \u251c\u2500\u2500 pf-azure-ad-pcv-1.2.jar \u2514\u2500\u2500 pf-slack-quickconnection-3.0.jar","title":"Build Extensions Layer"},{"location":"how-to/profilesLayered/#build-oauth-layer","text":"Go to the layered-profiles/oauth directory, and create a pingfederate subdirectory. mkdir -p instance/server/default/pingfederate OAuth Playground for PingFederate is also located in the /instance/server/default/deploy directory, like other extensions. For this example, we're building OAuth Playground into its own layer to show that it's optional for PingFederate deployments. Copy the OAuthPlayground.war file to layered-profiles/oauth/pingfederate/instance/server/default/deploy . Your OAuth profile layer should look like this: \u2514\u2500\u2500 oauth \u2514\u2500\u2500 pingfederate \u2514\u2500\u2500 instance \u2514\u2500\u2500 server \u2514\u2500\u2500 default \u2514\u2500\u2500 deploy \u2514\u2500\u2500 OAuthPlayground.war","title":"Build OAuth Layer"},{"location":"how-to/profilesLayered/#assign-environment-variables","text":"We'll assign the environment variables for use in a Docker Compose YAML file. However, you can use this technique with any Docker or Kubernetes deployment. If you're intending on using your Github repository for the deployment, in the following examples, replace SERVER_PROFILE_URL = https://github.com/pingidentity/pingidentity-server-profiles.git with SERVER_PROFILE_URL = https://github.com/<your-username>/pingidentity-server-profiles.git Private Github Repo If your GitHub server-profile repo is private, use the username:token format so the container can access the repository. For example, https://github.com/<your_username>:<your_access_token>/pingidentity-server-profiles.git . See Using Private Github Repositories for more information. Create a new docker-compose.yaml file. Add your license profile to the YAML file. For example: - SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_PATH=layered-profiles/license/pingfederate SERVER_PROFILE supports URL , PATH , BRANCH and PARENT variables. Using SERVER_PROFILE_PARENT , we can instruct the container to retrieve its parent configuration. We'll specify the extensions profile as the parent: - SERVER_PROFILE_PARENT=EXTENSIONS SERVER_PROFILE can be extended to reference additional profiles. Since we specified the license profile's parent as EXTENSIONS , we can extend SERVER_PROFILE by referencing the EXTENSIONS profile (prior to the URL and PATH variables): - SERVER_PROFILE_EXTENSIONS_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_EXTENSIONS_PATH=layered-profiles/extensions/pingfederate Set the EXTENSIONS parent to OAUTH : - SERVER_PROFILE_EXTENSIONS_PARENT=OAUTH Then set the URL and PATH for the OAUTH profile: - SERVER_PROFILE_OAUTH_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_OAUTH_PATH=layered-profiles/oauth/pingfederate Set GETTING_STARTED as the OAUTH parent and declare the URL and PATH : - SERVER_PROFILE_OAUTH_PARENT=GETTING_STARTED - SERVER_PROFILE_GETTING_STARTED_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_GETTING_STARTED_PATH=getting-started/pingfederate Because the GETTING_STARTED profile is the last profile to add, it won't have a parent. Your environment section of the docker-compose.yaml file should look similar to this: environment : # **** SERVER PROFILES BEGIN **** # Server Profile - Product License - SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_PATH=layered-profiles/license/pingfederate - SERVER_PROFILE_PARENT=EXTENSIONS # Server Profile - Extensions - SERVER_PROFILE_EXTENSIONS_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_EXTENSIONS_PATH=layered-profiles/extensions/pingfederate - SERVER_PROFILE_EXTENSIONS_PARENT=OAUTH # Server Profile - OAUTH - SERVER_PROFILE_OAUTH_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_OAUTH_PATH=layered-profiles/oauth/pingfederate - SERVER_PROFILE_OAUTH_PARENT=GETTING_STARTED # Base Server Profile - SERVER_PROFILE_GETTING_STARTED_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_GETTING_STARTED_PATH=getting-started/pingfederate # **** SERVER PROFILE END ****","title":"Assign Environment Variables"},{"location":"how-to/profilesLayered/#deploy-layered-profile","text":"Push your profiles and updated docker-compose.yaml file to your GitHub repository. Deploy the stack with the layered profiles. To view this example in its entirety, including the profile layers and docker-compose.yaml file, see the pingidentity-server-profiles/layered-profiles directory.","title":"Deploy Layered Profile"},{"location":"how-to/profilesSubstitution/","text":"Environment Substitution \u00b6 In a typical environment, a product configuration is moved from server to server. Hostnames, endpoints, DNS information, and more need a way to be easily modified. By removing literal values and replacing them with environment variables, configurations can be deployed in multiple environments with minimal change. All of our configuration files can be parameterized by adding variables using the syntax: ${filename.ext}.subst . Passing Values to Containers \u00b6 Within the environment section of your container definition, declare the variable and the value for the product instance. Values can be defined in many sources, such as inline, env_vars files, and Kubernetes ConfigMaps. How it Works \u00b6 A container startup is initiated. The configuration pulls a server profile from Git or from a bind mounted /opt/in volume. All files with a .subst extension are identified. The environment variables in the identified .subst files are replaced with the actual environment values. The .subst extension is removed from all the identified files. The product instance for the container is started.","title":"ENV Substitution"},{"location":"how-to/profilesSubstitution/#environment-substitution","text":"In a typical environment, a product configuration is moved from server to server. Hostnames, endpoints, DNS information, and more need a way to be easily modified. By removing literal values and replacing them with environment variables, configurations can be deployed in multiple environments with minimal change. All of our configuration files can be parameterized by adding variables using the syntax: ${filename.ext}.subst .","title":"Environment Substitution"},{"location":"how-to/profilesSubstitution/#passing-values-to-containers","text":"Within the environment section of your container definition, declare the variable and the value for the product instance. Values can be defined in many sources, such as inline, env_vars files, and Kubernetes ConfigMaps.","title":"Passing Values to Containers"},{"location":"how-to/profilesSubstitution/#how-it-works","text":"A container startup is initiated. The configuration pulls a server profile from Git or from a bind mounted /opt/in volume. All files with a .subst extension are identified. The environment variables in the identified .subst files are replaced with the actual environment values. The .subst extension is removed from all the identified files. The product instance for the container is started.","title":"How it Works"},{"location":"how-to/saveConfigs/","text":"Saving Your Configuration Changes \u00b6 To save any configuration changes you make when using the products in the stack, you need to set up a local Docker volume to persist state and data for the stack. If you don't do this, whenever you bring the stack down your configuration changes will be lost. You'll mount a Docker volume location to the Docker /opt/out directory for the container. The location must be to a directory you've not already created. Our Docker containers use the /opt/out directory to store application data. Mounting to /opt/out Make sure the local directory is not already created. Docker needs to create this directory for the mount to /opt/out . You can mount a Docker volume for containers in a stack or for standalone containers. Bind Mounting For a Stack \u00b6 Add a volumes section under the container entry for each product in the docker-compose.yaml file you're using for the stack. Under the volumes section, add a location to persist your data. For example: pingfederate : . . . volumes : - /tmp/compose/pingfederate_1:/opt/out In the environment section, comment out the SERVER_PROFILE_PATH setting. The container will then use your volumes entry to supply the product state and data, including your configuration changes. When the container starts, this will mount /tmp/compose/pingfederate_1 to the /opt/out directory in the container. You're also able to view the product logs and data in the /tmp/compose/pingfederate_1 directory. Repeat this process for the remaining container entries in the stack. Bind Mounting For a Standalone Container \u00b6 Add a volume entry to the docker run command: docker run \\ --name pingfederate \\ --volume <local-path>:/opt/out \\ pingidentity/pingfederate:edge Get Started - Docker Compose Mounts \u00b6 Within many of the docker-compose.yaml files in the Getting-Started repository , volume mounts to opt/out have been included to persist your configuration across container restarts. To view the list of persisted volumes run the command: docker volume list To view the contents of the /opt/out/ volume when the container is running docker container exec -it <container id> sh cd out To view the contents of the /opt/out/ volume when the container is stopped docker run --rm -i -v = <volume name>:/opt/out alpine ls To remove a volume docker volume rm <volume name> To copy files from the container to your local filesystem docker cp \\ <container id>:< source path> \\ <destination path> eg. docker cp \\ b867054293a1:/opt/out \\ ~/pingfederate/ To copy files from your local filesystem to the container docker cp \\ < source path> \\ <container id>:<destination path> eg. docker cp \\ myconnector.jar \\ bb867054293a186:/opt/out/instance/server/default/deploy/","title":"Saving Configurations"},{"location":"how-to/saveConfigs/#saving-your-configuration-changes","text":"To save any configuration changes you make when using the products in the stack, you need to set up a local Docker volume to persist state and data for the stack. If you don't do this, whenever you bring the stack down your configuration changes will be lost. You'll mount a Docker volume location to the Docker /opt/out directory for the container. The location must be to a directory you've not already created. Our Docker containers use the /opt/out directory to store application data. Mounting to /opt/out Make sure the local directory is not already created. Docker needs to create this directory for the mount to /opt/out . You can mount a Docker volume for containers in a stack or for standalone containers.","title":"Saving Your Configuration Changes"},{"location":"how-to/saveConfigs/#bind-mounting-for-a-stack","text":"Add a volumes section under the container entry for each product in the docker-compose.yaml file you're using for the stack. Under the volumes section, add a location to persist your data. For example: pingfederate : . . . volumes : - /tmp/compose/pingfederate_1:/opt/out In the environment section, comment out the SERVER_PROFILE_PATH setting. The container will then use your volumes entry to supply the product state and data, including your configuration changes. When the container starts, this will mount /tmp/compose/pingfederate_1 to the /opt/out directory in the container. You're also able to view the product logs and data in the /tmp/compose/pingfederate_1 directory. Repeat this process for the remaining container entries in the stack.","title":"Bind Mounting For a Stack"},{"location":"how-to/saveConfigs/#bind-mounting-for-a-standalone-container","text":"Add a volume entry to the docker run command: docker run \\ --name pingfederate \\ --volume <local-path>:/opt/out \\ pingidentity/pingfederate:edge","title":"Bind Mounting For a Standalone Container"},{"location":"how-to/saveConfigs/#get-started-docker-compose-mounts","text":"Within many of the docker-compose.yaml files in the Getting-Started repository , volume mounts to opt/out have been included to persist your configuration across container restarts. To view the list of persisted volumes run the command: docker volume list To view the contents of the /opt/out/ volume when the container is running docker container exec -it <container id> sh cd out To view the contents of the /opt/out/ volume when the container is stopped docker run --rm -i -v = <volume name>:/opt/out alpine ls To remove a volume docker volume rm <volume name> To copy files from the container to your local filesystem docker cp \\ <container id>:< source path> \\ <destination path> eg. docker cp \\ b867054293a1:/opt/out \\ ~/pingfederate/ To copy files from your local filesystem to the container docker cp \\ < source path> \\ <container id>:<destination path> eg. docker cp \\ myconnector.jar \\ bb867054293a186:/opt/out/instance/server/default/deploy/","title":"Get Started - Docker Compose Mounts"},{"location":"how-to/secureContainers/","text":"Securing the Containers \u00b6 By default, Ping Identity Docker Images run as root within the container. When deploying these images into your production environment, you may wish to secure them by using one of the following patterns which we describe below, in order of preference. Isolate Containers with a User Namespace \u00b6 Linux namespaces provide isolation for running processes, limiting their access to system resources without the running process being aware of the limitations. The best way to prevent privilege-escalation attacks from within a container is to configure your container\u2019s applications to run as unprivileged users. For containers whose processes must run as the root user within the container, you can re-map this user to a less-privileged user on the Docker host. Please view the Docker\u2019s in-depth Documentation on this pattern for more information. Inside-Out Pattern \u00b6 Using the inside-out pattern, the container steps down from root to run as a non-privileged user. You may pass the User ID (UID) and Group ID (GID) for the container to run as. Overview of the bootstrap process: Start as root Immediately check if need to step down (PING_CONTAINER_PRIVILEGED=false) Create the group with provided group ID or 9999 Create the user with provided user ID or 9031 Strip ownership but for user:group Step-down from root to user This pattern has the benefit of removing permissions from anything but the specified user. Use the following environment variables to set the user and group and prevent from running as root PING_CONTAINER_PRIVILEGED = false PING_CONTAINER_UID = <UID> ( Default: 9031 ) PING_CONTAINER_GID = <GID> ( Default: 9999 ) Pros: This pattern is more secure Does not require infrastructure changes User/group exist within the container Recommended to be combined with namespace-Isolated containers Cons: Implementation is vendor-specific and may require more introspection on the part of the deployer Outside-In Pattern (Kubernetes Security-Context) \u00b6 Using the outside-in pattern, you will specify the user to run as via the Docker API (Docker Run, Compose, etc). This user will need to exist on the host machine if you wish to mount a volume from the host into the container and file ownership IDs need to agree. Pros: Easy, visible (accessible with docker run --user) The container runtime is never root at any point Cons: User does not have a home directory, some tools will or may have issues running properly or as expected For this pattern to work, at build time, we need to leave permissions open to the world since the user does not exist in /etc/password and inodes cannot be tied to it at runtime Ping Identity's Docker Image Hardening Guide \u00b6 View Ping Identity's Hardening Guide which outlines best practices for securing your product Docker Image.","title":"Securing the Containers"},{"location":"how-to/secureContainers/#securing-the-containers","text":"By default, Ping Identity Docker Images run as root within the container. When deploying these images into your production environment, you may wish to secure them by using one of the following patterns which we describe below, in order of preference.","title":"Securing the Containers"},{"location":"how-to/secureContainers/#isolate-containers-with-a-user-namespace","text":"Linux namespaces provide isolation for running processes, limiting their access to system resources without the running process being aware of the limitations. The best way to prevent privilege-escalation attacks from within a container is to configure your container\u2019s applications to run as unprivileged users. For containers whose processes must run as the root user within the container, you can re-map this user to a less-privileged user on the Docker host. Please view the Docker\u2019s in-depth Documentation on this pattern for more information.","title":"Isolate Containers with a User Namespace"},{"location":"how-to/secureContainers/#inside-out-pattern","text":"Using the inside-out pattern, the container steps down from root to run as a non-privileged user. You may pass the User ID (UID) and Group ID (GID) for the container to run as. Overview of the bootstrap process: Start as root Immediately check if need to step down (PING_CONTAINER_PRIVILEGED=false) Create the group with provided group ID or 9999 Create the user with provided user ID or 9031 Strip ownership but for user:group Step-down from root to user This pattern has the benefit of removing permissions from anything but the specified user. Use the following environment variables to set the user and group and prevent from running as root PING_CONTAINER_PRIVILEGED = false PING_CONTAINER_UID = <UID> ( Default: 9031 ) PING_CONTAINER_GID = <GID> ( Default: 9999 ) Pros: This pattern is more secure Does not require infrastructure changes User/group exist within the container Recommended to be combined with namespace-Isolated containers Cons: Implementation is vendor-specific and may require more introspection on the part of the deployer","title":"Inside-Out Pattern"},{"location":"how-to/secureContainers/#outside-in-pattern-kubernetes-security-context","text":"Using the outside-in pattern, you will specify the user to run as via the Docker API (Docker Run, Compose, etc). This user will need to exist on the host machine if you wish to mount a volume from the host into the container and file ownership IDs need to agree. Pros: Easy, visible (accessible with docker run --user) The container runtime is never root at any point Cons: User does not have a home directory, some tools will or may have issues running properly or as expected For this pattern to work, at build time, we need to leave permissions open to the world since the user does not exist in /etc/password and inodes cannot be tied to it at runtime","title":"Outside-In Pattern (Kubernetes Security-Context)"},{"location":"how-to/secureContainers/#ping-identitys-docker-image-hardening-guide","text":"View Ping Identity's Hardening Guide which outlines best practices for securing your product Docker Image.","title":"Ping Identity's Docker Image Hardening Guide"},{"location":"how-to/upgradePingDirectory/","text":"Upgrading PingDirectory \u00b6 Because PingDirectory is essentially a database, in it's container form each node in a cluster has its own persisted volume. Additionally, because PingDirectory is an application that focuses on state, rolling out an upgrade isn't really the same as any other configuration update. However, the product software, and scripts in the image provide a process through which upgrades are drastically simplified. This use case focuses on a PingDirectory upgrade in a default Kubernetes environment. You'll upgrade a PingDirectory StatefulSet 8.0.0.1 to 8.1.0.0 using an incremental canary roll-out. Tips \u00b6 To ensure a successful upgrade process: Avoid combining configuration changes and version upgrades in the same rollout. This adds unnecessary complexity to debugging errors during an upgrade. Successfully complete an upgrade in a proper Dev/QA environment before trying anything in production. Upgrades will happen on one server at a time. Ensure you have enough resources on the remaining machines to prevent client impact. Follow a canary deployment pattern to ensure the changes will be successful before doing a full rollout. There is no good way to roll back a completed upgrade, so take any necessary steps to avoid this. Prerequisites \u00b6 You've already been through Get Started to set up your DevOps environment and run a test deployment of the products. You've cloned or downloaded the pingidentity-devops-getting-started/20-kubernetes/12-pingdirectory-upgrade repository to your local ${HOME}/projects/devops directory. A good understanding of how to use our DevOps server profiles. Access to a Kubernetes cluster and a default StorageClass. An understanding of StatefulSets in Kubernetes is helpful. If you're upgrading in your own environment and using mounted licenses, the license for the existing version needs to be in the /opt/out persisted volume, and a license for the new version needs to be in /opt/in . The license locations are not an needed if you're using our DevOps credentials in an evaluation context. What You'll Do \u00b6 Start with a base stack. Set up a partition to make changes on just one node. Deploy changes to one node and fix any errors. Rollout changes to other nodes. Summary \u00b6 The key functionality for PingDirectory upgrades is the relationship between the image hooks and the manage-profile command in the product. The upgrade is processed in this way: When a node starts for the first time it is in SETUP mode and runs manage-profile setup . When a node restarts (for whatever reason), it runs manage-profile replace-profile . This command compares the new profile to the old profile, and if there is a change, it tries standing up the server with the new profile. Errors will be thrown if there is a configuration in the new profile that prevents it from being applied. If manage-profile replace-profile detects a product version difference, it takes the same approach as any other restart. It will attempt to migrate PingDirectory to the new version, and the command will fail if it cannot. Because there is processing that happens automatically in the container during the upgrade, you want to roll the change out to a small partition first and test it thoroughly before rolling it out to all. This partition also gives us room to revert back without impacting traffic in case something is not as expected. This process in standard Kubernetes is called a Canary Rollout and is derived from Kubernetes documentation on StatefulSet update strategies . \"Canary Rollout\" in this scenario focuses only on an incremental rollout of containers, not actually separating traffic. That could be done with additional tools like Istio, or standing up another service and pairing separate labels and selectors. Setup \u00b6 The YAML configuration files for this use case are in your local Use the 1-initial.yaml file in your local pingidentity-devops-getting-started/20-kubernetes/12-pingdirectory-upgrade directory to start with a PingDirectory StatefulSet using persistent volumes. Enter: kubectl apply -f 1 -initial.yaml All kubectl commands for this use case need to be run from the pingidentity-devops-getting-started/20-kubernetes/12-pingdirectory-upgrade directory. This will stand up a two directory topology, each with its own Persistent Volume Claim using the default storage class. Wait for both nodes to be healthy before continuing. Enter: kubectl get pods pingdirectory-0 and pingdirectory-1 should show 1/1 in the READY column. Partition \u00b6 Use the 2-partition.yaml file in your local pingidentity-devops-getting-started/20-kubernetes/12-pingdirectory-upgrade directory to add a partition to StatefulSet for updateStrategy . Enter: kubectl apply -f 2 -partition.yaml This partition configuration signifies that any changes to spec.template will only be applied to nodes with a cardinal value higher than the partition definition. Stage \u00b6 Use the 3-staging.yaml file in your local pingidentity-devops-getting-started/20-kubernetes/12-pingdirectory-upgrade directory to stage the change. Enter: kubectl apply -f 3 -staging.yaml The only actual change is to the image tag. When this change is applied: The pingdirectory-1 pod will be terminated and a new one with the new image will started. The new PingDirectory container is based on a specific version of PingDirectory, found in the /opt/server volume. The manage-profile replace-profile command will eventually be triggered when the container detects PingDirectory is in a RESTART state. This command identifies the difference in the database version running, based on the persisted volume attached to /opt/out , and then attempts to upgrade. Information similar to the following will be displayed: ... pingdirectory-1 Validating source and existing servers ..... Done pingdirectory-1 Updating the server version from 8.0.0.1 to 8.1.0.0. Local database backends pingdirectory-1 will be exported before the update in case a revert is necessary pingdirectory-1 Exporting backend with backendID userRoot. This may take a while ..... Done pingdirectory-1 Running the update tool ..... Done ... pingdirectory-1 Cleaning up after replace ..... Done pingdirectory-1 manage-profile replace-profile returned 0 This process requires having licenses for both server versions available and in the right location. If manage-profile replace-profile completes without error, you'll see the container continue the migration and eventually start up PingDirectory. If manage-profile replace-profile fails, an error is displayed and the container will exit. The errors will be due to some conflict in the server profile. The partition you set up previously provides an isolated environment for working out errors. Work through any errors until you can get manage-profile replace-profile to complete successfully before continuing. Rollout \u00b6 When you're confident your upgrade will occur smoothly, use the 4-rollout.yaml file in your local pingidentity-devops-getting-started/20-kubernetes/12-pingdirectory-upgrade directory to deploy the rollout to the remaining nodes. This will remove the partition. Enter: kubectl apply -f 4 -rollout.yaml","title":"Upgrading PingDirectory"},{"location":"how-to/upgradePingDirectory/#upgrading-pingdirectory","text":"Because PingDirectory is essentially a database, in it's container form each node in a cluster has its own persisted volume. Additionally, because PingDirectory is an application that focuses on state, rolling out an upgrade isn't really the same as any other configuration update. However, the product software, and scripts in the image provide a process through which upgrades are drastically simplified. This use case focuses on a PingDirectory upgrade in a default Kubernetes environment. You'll upgrade a PingDirectory StatefulSet 8.0.0.1 to 8.1.0.0 using an incremental canary roll-out.","title":"Upgrading PingDirectory"},{"location":"how-to/upgradePingDirectory/#tips","text":"To ensure a successful upgrade process: Avoid combining configuration changes and version upgrades in the same rollout. This adds unnecessary complexity to debugging errors during an upgrade. Successfully complete an upgrade in a proper Dev/QA environment before trying anything in production. Upgrades will happen on one server at a time. Ensure you have enough resources on the remaining machines to prevent client impact. Follow a canary deployment pattern to ensure the changes will be successful before doing a full rollout. There is no good way to roll back a completed upgrade, so take any necessary steps to avoid this.","title":"Tips"},{"location":"how-to/upgradePingDirectory/#prerequisites","text":"You've already been through Get Started to set up your DevOps environment and run a test deployment of the products. You've cloned or downloaded the pingidentity-devops-getting-started/20-kubernetes/12-pingdirectory-upgrade repository to your local ${HOME}/projects/devops directory. A good understanding of how to use our DevOps server profiles. Access to a Kubernetes cluster and a default StorageClass. An understanding of StatefulSets in Kubernetes is helpful. If you're upgrading in your own environment and using mounted licenses, the license for the existing version needs to be in the /opt/out persisted volume, and a license for the new version needs to be in /opt/in . The license locations are not an needed if you're using our DevOps credentials in an evaluation context.","title":"Prerequisites"},{"location":"how-to/upgradePingDirectory/#what-youll-do","text":"Start with a base stack. Set up a partition to make changes on just one node. Deploy changes to one node and fix any errors. Rollout changes to other nodes.","title":"What You'll Do"},{"location":"how-to/upgradePingDirectory/#summary","text":"The key functionality for PingDirectory upgrades is the relationship between the image hooks and the manage-profile command in the product. The upgrade is processed in this way: When a node starts for the first time it is in SETUP mode and runs manage-profile setup . When a node restarts (for whatever reason), it runs manage-profile replace-profile . This command compares the new profile to the old profile, and if there is a change, it tries standing up the server with the new profile. Errors will be thrown if there is a configuration in the new profile that prevents it from being applied. If manage-profile replace-profile detects a product version difference, it takes the same approach as any other restart. It will attempt to migrate PingDirectory to the new version, and the command will fail if it cannot. Because there is processing that happens automatically in the container during the upgrade, you want to roll the change out to a small partition first and test it thoroughly before rolling it out to all. This partition also gives us room to revert back without impacting traffic in case something is not as expected. This process in standard Kubernetes is called a Canary Rollout and is derived from Kubernetes documentation on StatefulSet update strategies . \"Canary Rollout\" in this scenario focuses only on an incremental rollout of containers, not actually separating traffic. That could be done with additional tools like Istio, or standing up another service and pairing separate labels and selectors.","title":"Summary"},{"location":"how-to/upgradePingDirectory/#setup","text":"The YAML configuration files for this use case are in your local Use the 1-initial.yaml file in your local pingidentity-devops-getting-started/20-kubernetes/12-pingdirectory-upgrade directory to start with a PingDirectory StatefulSet using persistent volumes. Enter: kubectl apply -f 1 -initial.yaml All kubectl commands for this use case need to be run from the pingidentity-devops-getting-started/20-kubernetes/12-pingdirectory-upgrade directory. This will stand up a two directory topology, each with its own Persistent Volume Claim using the default storage class. Wait for both nodes to be healthy before continuing. Enter: kubectl get pods pingdirectory-0 and pingdirectory-1 should show 1/1 in the READY column.","title":"Setup"},{"location":"how-to/upgradePingDirectory/#partition","text":"Use the 2-partition.yaml file in your local pingidentity-devops-getting-started/20-kubernetes/12-pingdirectory-upgrade directory to add a partition to StatefulSet for updateStrategy . Enter: kubectl apply -f 2 -partition.yaml This partition configuration signifies that any changes to spec.template will only be applied to nodes with a cardinal value higher than the partition definition.","title":"Partition"},{"location":"how-to/upgradePingDirectory/#stage","text":"Use the 3-staging.yaml file in your local pingidentity-devops-getting-started/20-kubernetes/12-pingdirectory-upgrade directory to stage the change. Enter: kubectl apply -f 3 -staging.yaml The only actual change is to the image tag. When this change is applied: The pingdirectory-1 pod will be terminated and a new one with the new image will started. The new PingDirectory container is based on a specific version of PingDirectory, found in the /opt/server volume. The manage-profile replace-profile command will eventually be triggered when the container detects PingDirectory is in a RESTART state. This command identifies the difference in the database version running, based on the persisted volume attached to /opt/out , and then attempts to upgrade. Information similar to the following will be displayed: ... pingdirectory-1 Validating source and existing servers ..... Done pingdirectory-1 Updating the server version from 8.0.0.1 to 8.1.0.0. Local database backends pingdirectory-1 will be exported before the update in case a revert is necessary pingdirectory-1 Exporting backend with backendID userRoot. This may take a while ..... Done pingdirectory-1 Running the update tool ..... Done ... pingdirectory-1 Cleaning up after replace ..... Done pingdirectory-1 manage-profile replace-profile returned 0 This process requires having licenses for both server versions available and in the right location. If manage-profile replace-profile completes without error, you'll see the container continue the migration and eventually start up PingDirectory. If manage-profile replace-profile fails, an error is displayed and the container will exit. The errors will be due to some conflict in the server profile. The partition you set up previously provides an isolated environment for working out errors. Work through any errors until you can get manage-profile replace-profile to complete successfully before continuing.","title":"Stage"},{"location":"how-to/upgradePingDirectory/#rollout","text":"When you're confident your upgrade will occur smoothly, use the 4-rollout.yaml file in your local pingidentity-devops-getting-started/20-kubernetes/12-pingdirectory-upgrade directory to deploy the rollout to the remaining nodes. This will remove the partition. Enter: kubectl apply -f 4 -rollout.yaml","title":"Rollout"},{"location":"how-to/upgradePingfederate/","text":"Upgrading PingFederate \u00b6 In a DevOps environment, upgrades are drastically simplified through automation, orchestration and separation of concerns. As a result, upgrading to a new version of PingFederate is much more like deploying any other configuration. The slight difference is that configuration updates can be achieved with zero downtime and no loss of state, whereas in version upgrades we consciously sacrifice state to maintain zero downtime overall. You can take a traditional upgrade approach to a containerized environment, but it provides no value above the process described here. As an example, we'll walk through upgrading a PingFederate deployment from 9.3.3 to 10.0.0 in a Kubernetes environment. However, the concepts should work with any container orchestrator. Prerequisites \u00b6 You've already been through Get started to set up your DevOps environment and run a test deployment of the products. A good understanding of how to use our DevOps server profiles. An understanding of blue-green deployments in Kubernetes is helpful. What You'll Do \u00b6 Setup and preparation Upgrade using a local profile Blue-Green Deployment Some final considerations Setup and Preparation \u00b6 The most important factor to a successful version upgrade is preparing an environment for success. This means using the DevOps process, and a blue-green deployment. For a blue-green Kubernetes deployment, we simply update a selector on a service. The DevOps process: All software features migrate through environments . You should have at least 2 environments (non-production and production). This gives you room to test everything before putting it into production. Environments are nearly identical . All deployments should be stringently validated before rolling into production. To be confident in your manual and automated tests, your environments need to be as close to identical as possible. In an ideal world, environments have dummy data, but function exactly the same. You know you're doing a good job when the only thing that changes (related to configuration) between environments is URLs, endpoints, and variable values. Containers in production are immutable . Nobody is perfect. So, we should never trust manual changes directly in production. You should disable all admin access to production. All configurations are maintained in source control . If you can roll it out, you better be able to roll it back too! Our example environment is set up with Apache JMeter throwing load to a Kubernetes service (which routes load to downstream PingFederate containers). This Kubernetes service is essentially a load balancer that follows a round-robin strategy with keep-alive. The key here is that the service is pointing to this deployment of PingFederate due to a selector defined on the service that matches a label on the PingFederate deployment. This is what will make the blue-green approach possible. A deployment in Kubernetes manages containers in pods, defining things like which containers to run, how many containers, the metadata labels, and the update strategy. Upgrade Using a Local Profile \u00b6 With your environment set up properly, you can do the product upgrade offline. Offline here means we'll pull the profile into a Docker container on our local workstation to upgrade it. Before going into the actual steps for a profile upgrade. It's very worth noting that if your profile is well-constructed, using the minimum number of files that are specific to the PingFederate configuration, you may be able to avoid this entire section. For example, the upgrade for our baseline profile (https://github.com/pingidentity/pingidentity-server-profiles/tree/master/baseline) worked almost flawlessly from 9.3.3 to 10.0.0. In fact the only file that required an update was adding a \"Pairwise definition\" to ./instance/server/default/conf/hivemodule.xml . This may very well be the case for you as well. If you want to try this, just use your PingFederate profile with the new version image tag, and watch the logs for errors. Some details of the upgrade process may be different for you, based on your PingFederate profile. Check out a PingFederate feature branch (such as, pf-10.0.0 ) off of the master of your current version of PingFederate (9.3.3 in our example). Spin up a PingFederate deployment based on this branch with the /opt/out volume mounted. For clarity, let's do this in Docker Compose and assume the mount looks something like this: ~/tmp/pf93/instance:/opt/out/instance . Once the container is healthy, stop the container. Download the latest version of the PingFederate Server distribution ZIP file from the Ping Identity website. Extract the distribution ZIP file into ~/tmp/pf93 . Go to the ~/tmp/pf93 directory, and run the PingFederate upgrade utility: ./upgrade.sh <pf_install_source> [ -l <newPingFederateLicense> ] [ -c ] . See Upgrading PingFederate on Linux Systems for more information. The result is an upgraded PingFederate profile with a lot of bloat. Let's clean this up so that we're able to run git diff and see only upgraded files. A good text editor (such as, Microsoft Visual Studio Code) with Git extensions is invaluable for this process. Copy over files from your new profile ~/tmp/pf93/instance on top of your current profile. Be careful not to directly copy over and replace .subst files. If you are using Visual Studio Code, you can right-click -> Select for compare on the old file and right-click >'Compare with selected' on the new file. This compares line-by-line diffs. If all you see is your variables, you can ignore the whole file. After you test the upgrade, push your changes to Git. Blue-Green Deployment \u00b6 Now that you have a new profile, you can stand up a new deployment that uses it and flip all of the traffic over to it. Stand up a new deployment using: The correct product image version. The new profile. A label on the deployment that distinguishes it from the prior deployment (such as, version: 10.0.0 ). When the deployment is healthy and ready to accept traffic, update the selector on the Kubernetes service. This routes all traffic to the new PingFederate deployment without downtime occurring. Some Final Considerations \u00b6 Using DevOps processes can mean that things like comfortable setup processes and admin UIs in production are sacrificed, but for most organizations, the resulting zero downtime for rollouts and rollbacks is easily worth it. Note that terms \"zero downtime\" and \"loss-of-state\" are significantly different. Zero downtime is what this upgrade process achieves: at no point in time will users experience a 500 bad gateway error. However, we are sacrificing state to achieve this. Because we are moving from one entire deployment to another, the new deployment does cannot have access to runtime state in the previous deployment. For this reason, it's critical to externalize state as much as possible.","title":"Upgrading PingFederate"},{"location":"how-to/upgradePingfederate/#upgrading-pingfederate","text":"In a DevOps environment, upgrades are drastically simplified through automation, orchestration and separation of concerns. As a result, upgrading to a new version of PingFederate is much more like deploying any other configuration. The slight difference is that configuration updates can be achieved with zero downtime and no loss of state, whereas in version upgrades we consciously sacrifice state to maintain zero downtime overall. You can take a traditional upgrade approach to a containerized environment, but it provides no value above the process described here. As an example, we'll walk through upgrading a PingFederate deployment from 9.3.3 to 10.0.0 in a Kubernetes environment. However, the concepts should work with any container orchestrator.","title":"Upgrading PingFederate"},{"location":"how-to/upgradePingfederate/#prerequisites","text":"You've already been through Get started to set up your DevOps environment and run a test deployment of the products. A good understanding of how to use our DevOps server profiles. An understanding of blue-green deployments in Kubernetes is helpful.","title":"Prerequisites"},{"location":"how-to/upgradePingfederate/#what-youll-do","text":"Setup and preparation Upgrade using a local profile Blue-Green Deployment Some final considerations","title":"What You'll Do"},{"location":"how-to/upgradePingfederate/#setup-and-preparation","text":"The most important factor to a successful version upgrade is preparing an environment for success. This means using the DevOps process, and a blue-green deployment. For a blue-green Kubernetes deployment, we simply update a selector on a service. The DevOps process: All software features migrate through environments . You should have at least 2 environments (non-production and production). This gives you room to test everything before putting it into production. Environments are nearly identical . All deployments should be stringently validated before rolling into production. To be confident in your manual and automated tests, your environments need to be as close to identical as possible. In an ideal world, environments have dummy data, but function exactly the same. You know you're doing a good job when the only thing that changes (related to configuration) between environments is URLs, endpoints, and variable values. Containers in production are immutable . Nobody is perfect. So, we should never trust manual changes directly in production. You should disable all admin access to production. All configurations are maintained in source control . If you can roll it out, you better be able to roll it back too! Our example environment is set up with Apache JMeter throwing load to a Kubernetes service (which routes load to downstream PingFederate containers). This Kubernetes service is essentially a load balancer that follows a round-robin strategy with keep-alive. The key here is that the service is pointing to this deployment of PingFederate due to a selector defined on the service that matches a label on the PingFederate deployment. This is what will make the blue-green approach possible. A deployment in Kubernetes manages containers in pods, defining things like which containers to run, how many containers, the metadata labels, and the update strategy.","title":"Setup and Preparation"},{"location":"how-to/upgradePingfederate/#upgrade-using-a-local-profile","text":"With your environment set up properly, you can do the product upgrade offline. Offline here means we'll pull the profile into a Docker container on our local workstation to upgrade it. Before going into the actual steps for a profile upgrade. It's very worth noting that if your profile is well-constructed, using the minimum number of files that are specific to the PingFederate configuration, you may be able to avoid this entire section. For example, the upgrade for our baseline profile (https://github.com/pingidentity/pingidentity-server-profiles/tree/master/baseline) worked almost flawlessly from 9.3.3 to 10.0.0. In fact the only file that required an update was adding a \"Pairwise definition\" to ./instance/server/default/conf/hivemodule.xml . This may very well be the case for you as well. If you want to try this, just use your PingFederate profile with the new version image tag, and watch the logs for errors. Some details of the upgrade process may be different for you, based on your PingFederate profile. Check out a PingFederate feature branch (such as, pf-10.0.0 ) off of the master of your current version of PingFederate (9.3.3 in our example). Spin up a PingFederate deployment based on this branch with the /opt/out volume mounted. For clarity, let's do this in Docker Compose and assume the mount looks something like this: ~/tmp/pf93/instance:/opt/out/instance . Once the container is healthy, stop the container. Download the latest version of the PingFederate Server distribution ZIP file from the Ping Identity website. Extract the distribution ZIP file into ~/tmp/pf93 . Go to the ~/tmp/pf93 directory, and run the PingFederate upgrade utility: ./upgrade.sh <pf_install_source> [ -l <newPingFederateLicense> ] [ -c ] . See Upgrading PingFederate on Linux Systems for more information. The result is an upgraded PingFederate profile with a lot of bloat. Let's clean this up so that we're able to run git diff and see only upgraded files. A good text editor (such as, Microsoft Visual Studio Code) with Git extensions is invaluable for this process. Copy over files from your new profile ~/tmp/pf93/instance on top of your current profile. Be careful not to directly copy over and replace .subst files. If you are using Visual Studio Code, you can right-click -> Select for compare on the old file and right-click >'Compare with selected' on the new file. This compares line-by-line diffs. If all you see is your variables, you can ignore the whole file. After you test the upgrade, push your changes to Git.","title":"Upgrade Using a Local Profile"},{"location":"how-to/upgradePingfederate/#blue-green-deployment","text":"Now that you have a new profile, you can stand up a new deployment that uses it and flip all of the traffic over to it. Stand up a new deployment using: The correct product image version. The new profile. A label on the deployment that distinguishes it from the prior deployment (such as, version: 10.0.0 ). When the deployment is healthy and ready to accept traffic, update the selector on the Kubernetes service. This routes all traffic to the new PingFederate deployment without downtime occurring.","title":"Blue-Green Deployment"},{"location":"how-to/upgradePingfederate/#some-final-considerations","text":"Using DevOps processes can mean that things like comfortable setup processes and admin UIs in production are sacrificed, but for most organizations, the resulting zero downtime for rollouts and rollbacks is easily worth it. Note that terms \"zero downtime\" and \"loss-of-state\" are significantly different. Zero downtime is what this upgrade process achieves: at no point in time will users experience a 500 bad gateway error. However, we are sacrificing state to achieve this. Because we are moving from one entire deployment to another, the new deployment does cannot have access to runtime state in the previous deployment. For this reason, it's critical to externalize state as much as possible.","title":"Some Final Considerations"},{"location":"how-to/usingVault/","text":"Using Hashicorp Vault \u00b6 This documentation provides details for using Hashicorp Vault and secrets with Ping Identity DevOps Images. What You'll Do \u00b6 The examples below will explain and show examples of: Using HashiCorp Vault Secrets in native PingIdentity DevOps Images Using HashiCorp Vault Injector in kubernetes deployments Prerequisites \u00b6 You've already been through Get Started to set up your DevOps environment and run a test deployment of the products. Have a running Hashicorp Vault instance. Refer to Deploy Hashicorp Vault for information on deploying a vault if you need one. Kubernetes - HashiCorp Vault Injector \u00b6 If you are using Kubernetes to deploy your containers, it's highly recommended to use the HashiCorp Vault Injector. The section below provides details on how to use secrets in a non-kubernetes deployment (i.e. docker-compose). If the HashiCorp Vault Injector Agent is installed, annotations can be added to the .yaml file of a Pod, Deployment, StatefulSet resource to pull in the secrets. The snippet below provides an example set of annotations (placed in to the metadata of the container) to pull in a pf.jwk secret into a container. Helm Chart Stateful Set This is an StatefulSet example created using the PingIdentity DevOps Helm Chart . apiVersion : apps/v1 kind : StatefulSet spec : template : metadata : annotations : vault.hashicorp.com/agent-init-first : \"true\" vault.hashicorp.com/agent-inject : \"true\" vault.hashicorp.com/agent-inject-secret-devops-secret.env.json : secret/.../devops-secret.env vault.hashicorp.com/agent-inject-template-devops-secret.env.json : | {{ with secret \"secret/.../devops-secret.env\" -}} {{ .Data.data | toJSONPretty }} {{- end }} vault.hashicorp.com/agent-inject-secret-devops-secret.env.json : secret/.../passwords vault.hashicorp.com/agent-inject-template-passwords.json : | {{ with secret \"secret/.../passwords\" -}} {{ .Data.data | toJSONPretty }} {{- end }} vault.hashicorp.com/agent-pre-populate-only : \"true\" vault.hashicorp.com/log-level : info vault.hashicorp.com/preserve-secret-case : \"true\" vault.hashicorp.com/role : k8s-default vault.hashicorp.com/secret-volume-path : /run/secrets Secrets - Variables \u00b6 Using example above, the value for secret secret/.../devops-secret.env json will be pulled into the container as /run/secrets/devops-secret.env.json . Because this secret ends in the value of .env , it will further be turned into a property file with NAME=VALUE pairs, and available to the container environment when starting up. Example of devops-secret.env transformed into files { \"PING_IDENTITY_DEVOPS_USER\" : \"jsmith@example.com\" , \"PING_IDENTITY_DEVOPS_KEY\" : \"xxxxx-xxxx-xxxxx-xxxxx-xxxx\" } creates the files File: /run/secrets/devops-secret.env Contents: PING_IDENTITY_DEVOPS_USER=\"jsmith@example.com\" PING_IDENTITY_DEVOPS_KEY=\"xxxxx-xxxx-xxxxx-xxxxx-xxxx\" Secret - Files \u00b6 Using example above, the value for secret secret/.../passwords json will be pulled into the container as /run/secrets/passwords.json and for every key/value in that secret a file will be created with the name of the key and contents of value . Example of /run/secrets/passwords.json transformed into files { \"root-user-password\" : \"secret-root-password\" , \"admin-password\" : \"secret-admin-password\" } creates the files File: /run/secrets/secret-root-password Contents: secret-root-password File: /run/secrets/secret-admin-password Contents: secret-admin-password Native DevOps HashiCorp Support \u00b6 Vault secrets can also be used in native PingIdentity DevOps Images regardless of the environment they are deployed in (i.e. kubernetes, docker, docker-compose). In these cases, there is no injector agent required. This does require some type of AuthN to your vault (i.e. USERNAME/PASSWORD or TOKEN). HashiCorp Injector method is recommended. The image below depicts the components and steps for pulling secrets into a container at start-up. The following variables can be used to deploy images that will pull secrets from the Vault. Variable Example Description SECRETS_DIR /run/secrets Location for storing secrets. See section below on using a tmpfs mounted filesystem to store secrets in a memory location. VAULT_TYPE hashicorp Type of vault used. Currently supporting hashicorp. VAULT_ADDR https://vault.example.com:8200 URL for the vault with secrets VAULT_TOKEN s.gvC3vd5aFz......JovV0b0A Active token used to authticate/authorize container to vault. Optional if VAULT_AUTH_USERNAME/VAULT_AUTH_PASSWORD are provided. VAULT_AUTH_USERNAME demo Username of internal vault identity. Optional if VAULT_TOKEN is provided. VAULT_AUTH_PASSWORD 2FederateM0re Password of internal vault identity. Optional if VAULT_TOKEN is provided. VAULT_SECRETS /pingfederate/encryption-keys A list of secrets to pull into the container. Must be the full secret path used in vault. Below is an example of how these would be used in an docker-compose.yaml file. Note that this example provides 2 secrets as denoted by the VAULT_SECRETS setting. services : pingfederate : image : pingidentity/pingfederate:edge environment : ... ################################################ # Vault Info ################################################ - VAULT_TYPE=hashicorp - VAULT_ADDR=https://vault.ping-devops.com:8200 - VAULT_AUTH_USERNAME=demo - VAULT_AUTH_PASSWORD=2FederateM0re - VAULT_SECRETS=/demo/passwords /demo/getting-started/pingfederated/pf-keys The secret types (Variables/Files) are processed the same way as with the HashiCorp Injector Method above. Secrets - Base64 \u00b6 Often, there are secrets that may be of a binary format (i.e. certificates). Special key name suffixes can be used to perform certain processing on the keys when the file is created. The following table provides examples of how keys with special suffixes. Key Suffix Description .b64 or .base64 Specifies that the value is base64 encoded and the resulting file should be decoded when written, without the suffix. There is a message that is base64 encoded and stored in the vault as secret /demo/b64-demo and key hello.b64 Secret: /demo/b64-demo KEY VALUE --- ----- hello.b64 SGVsbG8gV29ybGQhCg== would result in the following file: /run/secrets/hello CONTENTS -------- Hello World! Using tmpfs for Secrets \u00b6 It is best practice to place secrets in a volume that won't be persisted to storage with the possibility that it might be improperly accessed at any point in the future (i.e. backups, environment variables). Kubernetes automatically provides the default SECRETS_DIR of /run/secrets for this. If using docker, it's recommended to create a tmpfs type volume and size it to 32m and mount it to a path of /run/secrets . Requires docker-compose version 2.4 or later, due to the options provided to the tmpfs volumes definition. Creates a /run/secrets volume under tmpfs version : \"2.4\" services : pingfederate : image : pingidentity/pingfederate:edge environment : ... tmpfs : /run/secrets ---- or ----- volumes : - type : tmpfs target : /run/secrets tmpfs : size : 32m See this mount by exec'ing into the container and running: > df -k /run/secrets Filesystem 1K-blocks Used Available Use% Mounted on tmpfs 16384 0 16384 0 % /run/secrets","title":"Vault/Secrets"},{"location":"how-to/usingVault/#using-hashicorp-vault","text":"This documentation provides details for using Hashicorp Vault and secrets with Ping Identity DevOps Images.","title":"Using Hashicorp Vault"},{"location":"how-to/usingVault/#what-youll-do","text":"The examples below will explain and show examples of: Using HashiCorp Vault Secrets in native PingIdentity DevOps Images Using HashiCorp Vault Injector in kubernetes deployments","title":"What You'll Do"},{"location":"how-to/usingVault/#prerequisites","text":"You've already been through Get Started to set up your DevOps environment and run a test deployment of the products. Have a running Hashicorp Vault instance. Refer to Deploy Hashicorp Vault for information on deploying a vault if you need one.","title":"Prerequisites"},{"location":"how-to/usingVault/#kubernetes-hashicorp-vault-injector","text":"If you are using Kubernetes to deploy your containers, it's highly recommended to use the HashiCorp Vault Injector. The section below provides details on how to use secrets in a non-kubernetes deployment (i.e. docker-compose). If the HashiCorp Vault Injector Agent is installed, annotations can be added to the .yaml file of a Pod, Deployment, StatefulSet resource to pull in the secrets. The snippet below provides an example set of annotations (placed in to the metadata of the container) to pull in a pf.jwk secret into a container. Helm Chart Stateful Set This is an StatefulSet example created using the PingIdentity DevOps Helm Chart . apiVersion : apps/v1 kind : StatefulSet spec : template : metadata : annotations : vault.hashicorp.com/agent-init-first : \"true\" vault.hashicorp.com/agent-inject : \"true\" vault.hashicorp.com/agent-inject-secret-devops-secret.env.json : secret/.../devops-secret.env vault.hashicorp.com/agent-inject-template-devops-secret.env.json : | {{ with secret \"secret/.../devops-secret.env\" -}} {{ .Data.data | toJSONPretty }} {{- end }} vault.hashicorp.com/agent-inject-secret-devops-secret.env.json : secret/.../passwords vault.hashicorp.com/agent-inject-template-passwords.json : | {{ with secret \"secret/.../passwords\" -}} {{ .Data.data | toJSONPretty }} {{- end }} vault.hashicorp.com/agent-pre-populate-only : \"true\" vault.hashicorp.com/log-level : info vault.hashicorp.com/preserve-secret-case : \"true\" vault.hashicorp.com/role : k8s-default vault.hashicorp.com/secret-volume-path : /run/secrets","title":"Kubernetes - HashiCorp Vault Injector"},{"location":"how-to/usingVault/#secrets-variables","text":"Using example above, the value for secret secret/.../devops-secret.env json will be pulled into the container as /run/secrets/devops-secret.env.json . Because this secret ends in the value of .env , it will further be turned into a property file with NAME=VALUE pairs, and available to the container environment when starting up. Example of devops-secret.env transformed into files { \"PING_IDENTITY_DEVOPS_USER\" : \"jsmith@example.com\" , \"PING_IDENTITY_DEVOPS_KEY\" : \"xxxxx-xxxx-xxxxx-xxxxx-xxxx\" } creates the files File: /run/secrets/devops-secret.env Contents: PING_IDENTITY_DEVOPS_USER=\"jsmith@example.com\" PING_IDENTITY_DEVOPS_KEY=\"xxxxx-xxxx-xxxxx-xxxxx-xxxx\"","title":"Secrets - Variables"},{"location":"how-to/usingVault/#secret-files","text":"Using example above, the value for secret secret/.../passwords json will be pulled into the container as /run/secrets/passwords.json and for every key/value in that secret a file will be created with the name of the key and contents of value . Example of /run/secrets/passwords.json transformed into files { \"root-user-password\" : \"secret-root-password\" , \"admin-password\" : \"secret-admin-password\" } creates the files File: /run/secrets/secret-root-password Contents: secret-root-password File: /run/secrets/secret-admin-password Contents: secret-admin-password","title":"Secret - Files"},{"location":"how-to/usingVault/#native-devops-hashicorp-support","text":"Vault secrets can also be used in native PingIdentity DevOps Images regardless of the environment they are deployed in (i.e. kubernetes, docker, docker-compose). In these cases, there is no injector agent required. This does require some type of AuthN to your vault (i.e. USERNAME/PASSWORD or TOKEN). HashiCorp Injector method is recommended. The image below depicts the components and steps for pulling secrets into a container at start-up. The following variables can be used to deploy images that will pull secrets from the Vault. Variable Example Description SECRETS_DIR /run/secrets Location for storing secrets. See section below on using a tmpfs mounted filesystem to store secrets in a memory location. VAULT_TYPE hashicorp Type of vault used. Currently supporting hashicorp. VAULT_ADDR https://vault.example.com:8200 URL for the vault with secrets VAULT_TOKEN s.gvC3vd5aFz......JovV0b0A Active token used to authticate/authorize container to vault. Optional if VAULT_AUTH_USERNAME/VAULT_AUTH_PASSWORD are provided. VAULT_AUTH_USERNAME demo Username of internal vault identity. Optional if VAULT_TOKEN is provided. VAULT_AUTH_PASSWORD 2FederateM0re Password of internal vault identity. Optional if VAULT_TOKEN is provided. VAULT_SECRETS /pingfederate/encryption-keys A list of secrets to pull into the container. Must be the full secret path used in vault. Below is an example of how these would be used in an docker-compose.yaml file. Note that this example provides 2 secrets as denoted by the VAULT_SECRETS setting. services : pingfederate : image : pingidentity/pingfederate:edge environment : ... ################################################ # Vault Info ################################################ - VAULT_TYPE=hashicorp - VAULT_ADDR=https://vault.ping-devops.com:8200 - VAULT_AUTH_USERNAME=demo - VAULT_AUTH_PASSWORD=2FederateM0re - VAULT_SECRETS=/demo/passwords /demo/getting-started/pingfederated/pf-keys The secret types (Variables/Files) are processed the same way as with the HashiCorp Injector Method above.","title":"Native DevOps HashiCorp Support"},{"location":"how-to/usingVault/#secrets-base64","text":"Often, there are secrets that may be of a binary format (i.e. certificates). Special key name suffixes can be used to perform certain processing on the keys when the file is created. The following table provides examples of how keys with special suffixes. Key Suffix Description .b64 or .base64 Specifies that the value is base64 encoded and the resulting file should be decoded when written, without the suffix. There is a message that is base64 encoded and stored in the vault as secret /demo/b64-demo and key hello.b64 Secret: /demo/b64-demo KEY VALUE --- ----- hello.b64 SGVsbG8gV29ybGQhCg== would result in the following file: /run/secrets/hello CONTENTS -------- Hello World!","title":"Secrets - Base64"},{"location":"how-to/usingVault/#using-tmpfs-for-secrets","text":"It is best practice to place secrets in a volume that won't be persisted to storage with the possibility that it might be improperly accessed at any point in the future (i.e. backups, environment variables). Kubernetes automatically provides the default SECRETS_DIR of /run/secrets for this. If using docker, it's recommended to create a tmpfs type volume and size it to 32m and mount it to a path of /run/secrets . Requires docker-compose version 2.4 or later, due to the options provided to the tmpfs volumes definition. Creates a /run/secrets volume under tmpfs version : \"2.4\" services : pingfederate : image : pingidentity/pingfederate:edge environment : ... tmpfs : /run/secrets ---- or ----- volumes : - type : tmpfs target : /run/secrets tmpfs : size : 32m See this mount by exec'ing into the container and running: > df -k /run/secrets Filesystem 1K-blocks Used Available Use% Mounted on tmpfs 16384 0 16384 0 % /run/secrets","title":"Using tmpfs for Secrets"},{"location":"reference/addMOTD/","text":"Adding a MOTD \u00b6 You can create a message of the day (MOTD) JSON file to be used to provide an MOTD file to our product containers when they start. Prerequisites \u00b6 You've already been through Get Started to set up your DevOps environment and run a test deployment of the products. Use a MOTD File \u00b6 You can employ a MOTD file in these ways by editing our existing motd.json file used by our example use cases, or creating a motd.json file in the location of your server profile: To use the MOTD with our example uses cases, edit the motd/motd.json file located in your local pingidentity-devops-getting-started/motd . To use a MOTD file for your server profile, create a motd.json file in the directory where the docker-compose.yaml file you're using for the server profile is located. This motd.json file will be appended to the /etc/motd file used by the DevOps image. Test the MOTD File \u00b6 Test the new messages in the motd.json file using the test-motd.sh script. The script supplies the JQ_EXPR value used to pass the message data to the Devops image. To test the motd.json file locally for our example use cases, from the pingidentity-devops-getting-started/motd directory, enter: ./test-motd.sh local To test the motd.json file you created in your server profile directory: Copy the test-motd.sh script located in the pingidentity-devops-getting-started/motd directory to your server profile directory. Enter: ./test-motd.sh local To test the motd.json with a server profile located in a Github repository: Ensure the test-motd.sh script is located in the local, cloned repository. From the local, cloned repository, enter: ./test-motd.sh github Example motd.json \u00b6 The example below shows the messages that will be displayed for all product images. For this example, the messages will only be shown from the validFrom to validTo dates: { \"devops\" : [ { \"validFrom\" : 20190701 , \"validTo\" : 20190730 , \"subject\" : \"General Message 1\" , \"message\" : [ \"This is line # 1\" , \"\" , \"This is line # 3\" ,] }, { \"validFrom\" : 20190801 , \"validTo\" : 20190830 , \"subject\" : \"General Message 2\" , \"message\" : [ \"Message goes here\" ] } ], \"pingfederate\" : [ { \"validFrom\" : 20190701 , \"validTo\" : 20190830 , \"subject\" : \"PingFederate Message 1\" , \"message\" : [ \"Message goes here\" ] } ] }","title":"Adding a MOTD"},{"location":"reference/addMOTD/#adding-a-motd","text":"You can create a message of the day (MOTD) JSON file to be used to provide an MOTD file to our product containers when they start.","title":"Adding a MOTD"},{"location":"reference/addMOTD/#prerequisites","text":"You've already been through Get Started to set up your DevOps environment and run a test deployment of the products.","title":"Prerequisites"},{"location":"reference/addMOTD/#use-a-motd-file","text":"You can employ a MOTD file in these ways by editing our existing motd.json file used by our example use cases, or creating a motd.json file in the location of your server profile: To use the MOTD with our example uses cases, edit the motd/motd.json file located in your local pingidentity-devops-getting-started/motd . To use a MOTD file for your server profile, create a motd.json file in the directory where the docker-compose.yaml file you're using for the server profile is located. This motd.json file will be appended to the /etc/motd file used by the DevOps image.","title":"Use a MOTD File"},{"location":"reference/addMOTD/#test-the-motd-file","text":"Test the new messages in the motd.json file using the test-motd.sh script. The script supplies the JQ_EXPR value used to pass the message data to the Devops image. To test the motd.json file locally for our example use cases, from the pingidentity-devops-getting-started/motd directory, enter: ./test-motd.sh local To test the motd.json file you created in your server profile directory: Copy the test-motd.sh script located in the pingidentity-devops-getting-started/motd directory to your server profile directory. Enter: ./test-motd.sh local To test the motd.json with a server profile located in a Github repository: Ensure the test-motd.sh script is located in the local, cloned repository. From the local, cloned repository, enter: ./test-motd.sh github","title":"Test the MOTD File"},{"location":"reference/addMOTD/#example-motdjson","text":"The example below shows the messages that will be displayed for all product images. For this example, the messages will only be shown from the validFrom to validTo dates: { \"devops\" : [ { \"validFrom\" : 20190701 , \"validTo\" : 20190730 , \"subject\" : \"General Message 1\" , \"message\" : [ \"This is line # 1\" , \"\" , \"This is line # 3\" ,] }, { \"validFrom\" : 20190801 , \"validTo\" : 20190830 , \"subject\" : \"General Message 2\" , \"message\" : [ \"Message goes here\" ] } ], \"pingfederate\" : [ { \"validFrom\" : 20190701 , \"validTo\" : 20190830 , \"subject\" : \"PingFederate Message 1\" , \"message\" : [ \"Message goes here\" ] } ] }","title":"Example motd.json"},{"location":"reference/buildLocal/","text":"Build a Docker Product Image Locally \u00b6 This example describes how to build a Docker image or our products using the build tools found in our Docker Builds repo, and a local copy of a product zip file. Docker Builds Clone Build Repository \u00b6 Open a terminal and clone the pingidentity-docker-builds repo. Enter: git clone https://github.com/pingidentity/pingidentity-docker-builds.git Download a Product Zip File \u00b6 Go to Product Downloads and download the product you'd like to use to build a Docker image. Ensure you download the product distribution zip file and not the Windows installer. When the download has finished, rename it to product.zip. For example: mv pingfederate-10.1.0.zip product.zip Move product.zip to the Build Directory In the pingidentity-docker-builds repo directory for each product. Move the product.zip file to the <product>/tmp directory, where /<product> is the name of one of our available products. For example: mv ~/Downloads/product.zip \\ ~/pingidentity/devops/pingidentity-docker-builds/pingfederate/tmp Building the Docker Image \u00b6 Prior to building the image, display the versions.json file in the product directory. You'll need to specify a valid version for the build script. Since you're providing the product zip file, it doesn't really matter which version you select as long as it's valid. For example, you can see that 10.1.0 is a valid product version for PingFederate. Go to the base of the pingidentity-docker-builds repo. For example: cd ~/pingidentity/devops/pingidentity-docker-builds Our Docker images are built using common foundational layers that the product layer will need (such as, JVM, pingcommon, pingdatacommon). Because it's unlikely that you'll have the foundational layers locally, we'll build the product using the serial_build.sh script. Going forward, if you want to use the same foundational layers, you need only run the build_product.sh script to build the product layer. You'll need to specify the appropriate options when you run serial_build.sh . For PingFederate, the options might look like this: -p (Product): pingfederate -v (Version): 10.1.0 Note: this is the version retrieved from the versions.json file -s (Shim): alpine -j (Java): az11 To build the image, run the serial_build.sh script with the appropriate options. For example: ./ci_scripts/serial_build.sh \\ -p pingfederate \\ -v 10 .1 \\ -s alpine \\ -j az11 It's important that you build from the base of the repo as shown in the example. When the build is completed, the product and base images are displayed. For example: Re-Tagging the Local Image \u00b6 You can change the tag of the created image and push it to your own Docker registry using the docker tag command: docker tag [ image id ] \\ [ Docker Registry ] / [ Organization ] / [ Image Name ] : [ tag ] For example: docker tag a379dffedf13 \\ gcp.io/pingidentity/pingfederate:localbuild","title":"Build Local Images"},{"location":"reference/buildLocal/#build-a-docker-product-image-locally","text":"This example describes how to build a Docker image or our products using the build tools found in our Docker Builds repo, and a local copy of a product zip file. Docker Builds","title":"Build a Docker Product Image Locally"},{"location":"reference/buildLocal/#clone-build-repository","text":"Open a terminal and clone the pingidentity-docker-builds repo. Enter: git clone https://github.com/pingidentity/pingidentity-docker-builds.git","title":"Clone Build Repository"},{"location":"reference/buildLocal/#download-a-product-zip-file","text":"Go to Product Downloads and download the product you'd like to use to build a Docker image. Ensure you download the product distribution zip file and not the Windows installer. When the download has finished, rename it to product.zip. For example: mv pingfederate-10.1.0.zip product.zip Move product.zip to the Build Directory In the pingidentity-docker-builds repo directory for each product. Move the product.zip file to the <product>/tmp directory, where /<product> is the name of one of our available products. For example: mv ~/Downloads/product.zip \\ ~/pingidentity/devops/pingidentity-docker-builds/pingfederate/tmp","title":"Download a Product Zip File"},{"location":"reference/buildLocal/#building-the-docker-image","text":"Prior to building the image, display the versions.json file in the product directory. You'll need to specify a valid version for the build script. Since you're providing the product zip file, it doesn't really matter which version you select as long as it's valid. For example, you can see that 10.1.0 is a valid product version for PingFederate. Go to the base of the pingidentity-docker-builds repo. For example: cd ~/pingidentity/devops/pingidentity-docker-builds Our Docker images are built using common foundational layers that the product layer will need (such as, JVM, pingcommon, pingdatacommon). Because it's unlikely that you'll have the foundational layers locally, we'll build the product using the serial_build.sh script. Going forward, if you want to use the same foundational layers, you need only run the build_product.sh script to build the product layer. You'll need to specify the appropriate options when you run serial_build.sh . For PingFederate, the options might look like this: -p (Product): pingfederate -v (Version): 10.1.0 Note: this is the version retrieved from the versions.json file -s (Shim): alpine -j (Java): az11 To build the image, run the serial_build.sh script with the appropriate options. For example: ./ci_scripts/serial_build.sh \\ -p pingfederate \\ -v 10 .1 \\ -s alpine \\ -j az11 It's important that you build from the base of the repo as shown in the example. When the build is completed, the product and base images are displayed. For example:","title":"Building the Docker Image"},{"location":"reference/buildLocal/#re-tagging-the-local-image","text":"You can change the tag of the created image and push it to your own Docker registry using the docker tag command: docker tag [ image id ] \\ [ Docker Registry ] / [ Organization ] / [ Image Name ] : [ tag ] For example: docker tag a379dffedf13 \\ gcp.io/pingidentity/pingfederate:localbuild","title":"Re-Tagging the Local Image"},{"location":"reference/config/","text":"Introduction \u00b6 Image/Container Anatomy \u00b6 The diagram below shows the anatomy of a container with flows of data into the container and how it transitions to the eventual running state. Data Class Default Location Use Description VAULT ext Secret information from external Vault (i.e. HashiCorp Vault). Items like passwords, certificates, keys, etc... ORCH ext Environment variables from secrets, configmaps and/or env/envfile resources from orchestration (i.e. docker, k8s. SERVER PROFILE ext Product server profile from either an external repository (i.e. git) or external volume (i.e. aws s3). SERVER BITS /opt/server ro Uncompressed copy of the product software. Provided by image. SECRETS /run/secrets ro Read Only secrets residing on non-persistent storage (i.e. /run/secrets). IN /opt/in ro Volume intended to receive all incoming server-profile information. ENV /opt/staging/.env mem Environment variable settings used by hooks and product to configure container. STAGING /opt/staging tmp Temporary space used to prepare configuration and store variable settings before being moved to OUT OUT /opt/out rw Combo of product bits/configuration resulting in running container configuration. PERSISTENT VOLUME rw Persistent location of product bits/configuration in external storage (i.e. AWS EBS) Due to many factors of how an image is deployed: Deployment Environment - Kubernetes, Cloud Vendor, Local Docker CI/CD Tools - Kubectl, Helm, Kustomize, Terraform Source Maintenance - Git, Cloud Vendor Volumes Customer Environment - Development, Test, QA, Stage, Prod Security - Test/QA/Production Data, Secrets, Certificates, Secret Management Tools the options available and recommended for use of the elements above can vary greatly. Examples might look like: Production Example \u00b6 The diagram below shows an example in a high-level production scenario in an AWS EKS environment, where HashiCorp Vault is used to provide secrets to the container, Helm used to create k8s resources and deploy them, and AWS EBS volumes to persist the state of the container. Development Example \u00b6 The diagram below shows an example in a high-level development scenario in an Azure AKS environment, where no secrets management is used, simple kubectl is used to deploy k8s resources, and AWS EBS volumes persist the state of the container. Customizing the Containers \u00b6 You can customize our product containers by: Customizing server profiles The server profiles supply configuration, data, and environment information to the product containers at startup. You can use our server profiles, or use them as a baseline for creating your own. You'll find these in Baseline server profiles in our pingidentity-server-profiles repository. Customizing YAML files In the stack-related directories for the deployment examples, you'll find the YAML files used to configure the Docker stack deployment. The YAML files can contain startup configuration settings or references to startup configuration settings (such as, environment variables) for the stack. You can try different configuration settings using these YAML files, or use them as a baseline for creating your own. Using DevOps hooks Hooks are DevOps shell scripts, generally specific to a product, that you can use to automate certain operations. You'll find the hooks for our builds in the Docker builds product directories . Using release tags We use sets of tags for each released build image. These tags identify whether the image is a specific stable release, the latest stable release, or current (potentially unstable) builds. You'll find the release tag information in Docker images . You can try different tags in either the standalone startup scripts for the deployment examples, or the YAML files for the orchestrated deployment examples. Securing the containers By default, our Docker images run as root within the container. Refer to this topic for instructions in changing this. Adding a message of the day (MOTD) You can use a motd.json file to add message of the day information that will be used by the DevOps images.","title":"Introduction"},{"location":"reference/config/#introduction","text":"","title":"Introduction"},{"location":"reference/config/#imagecontainer-anatomy","text":"The diagram below shows the anatomy of a container with flows of data into the container and how it transitions to the eventual running state. Data Class Default Location Use Description VAULT ext Secret information from external Vault (i.e. HashiCorp Vault). Items like passwords, certificates, keys, etc... ORCH ext Environment variables from secrets, configmaps and/or env/envfile resources from orchestration (i.e. docker, k8s. SERVER PROFILE ext Product server profile from either an external repository (i.e. git) or external volume (i.e. aws s3). SERVER BITS /opt/server ro Uncompressed copy of the product software. Provided by image. SECRETS /run/secrets ro Read Only secrets residing on non-persistent storage (i.e. /run/secrets). IN /opt/in ro Volume intended to receive all incoming server-profile information. ENV /opt/staging/.env mem Environment variable settings used by hooks and product to configure container. STAGING /opt/staging tmp Temporary space used to prepare configuration and store variable settings before being moved to OUT OUT /opt/out rw Combo of product bits/configuration resulting in running container configuration. PERSISTENT VOLUME rw Persistent location of product bits/configuration in external storage (i.e. AWS EBS) Due to many factors of how an image is deployed: Deployment Environment - Kubernetes, Cloud Vendor, Local Docker CI/CD Tools - Kubectl, Helm, Kustomize, Terraform Source Maintenance - Git, Cloud Vendor Volumes Customer Environment - Development, Test, QA, Stage, Prod Security - Test/QA/Production Data, Secrets, Certificates, Secret Management Tools the options available and recommended for use of the elements above can vary greatly. Examples might look like:","title":"Image/Container Anatomy"},{"location":"reference/config/#production-example","text":"The diagram below shows an example in a high-level production scenario in an AWS EKS environment, where HashiCorp Vault is used to provide secrets to the container, Helm used to create k8s resources and deploy them, and AWS EBS volumes to persist the state of the container.","title":"Production Example"},{"location":"reference/config/#development-example","text":"The diagram below shows an example in a high-level development scenario in an Azure AKS environment, where no secrets management is used, simple kubectl is used to deploy k8s resources, and AWS EBS volumes persist the state of the container.","title":"Development Example"},{"location":"reference/config/#customizing-the-containers","text":"You can customize our product containers by: Customizing server profiles The server profiles supply configuration, data, and environment information to the product containers at startup. You can use our server profiles, or use them as a baseline for creating your own. You'll find these in Baseline server profiles in our pingidentity-server-profiles repository. Customizing YAML files In the stack-related directories for the deployment examples, you'll find the YAML files used to configure the Docker stack deployment. The YAML files can contain startup configuration settings or references to startup configuration settings (such as, environment variables) for the stack. You can try different configuration settings using these YAML files, or use them as a baseline for creating your own. Using DevOps hooks Hooks are DevOps shell scripts, generally specific to a product, that you can use to automate certain operations. You'll find the hooks for our builds in the Docker builds product directories . Using release tags We use sets of tags for each released build image. These tags identify whether the image is a specific stable release, the latest stable release, or current (potentially unstable) builds. You'll find the release tag information in Docker images . You can try different tags in either the standalone startup scripts for the deployment examples, or the YAML files for the orchestrated deployment examples. Securing the containers By default, our Docker images run as root within the container. Refer to this topic for instructions in changing this. Adding a message of the day (MOTD) You can use a motd.json file to add message of the day information that will be used by the DevOps images.","title":"Customizing the Containers"},{"location":"reference/dockerImageSecurity/","text":"Evaluation of Docker Base Image Security \u00b6 In CIS (Center for Internet Security) Docker Benchmark v1.2.0 , one of the recommendations says, \"4.3 Ensure that unnecessary packages are not installed in the container.\" It further states, \"You should consider using a minimal base image rather than the standard Red Hat/CentOS/Debian images if you can. Some of the options available include BusyBox and Alpine.\" Is Alpine Docker image really more secure than other, more popular Linux distributions? So, let's take a look at the security aspects of different Linux distributions. This doesn't necessarily mean that one is the best for Docker base images. Other factors such as usability and compatibility should also be considered when choosing the most suitable Docker image for an organization. Evaluation \u00b6 To evaluate Alpine\u2019s security, we'll compare it with the following popular Linux distros: Ubuntu, CentOS, and Red Hat Enterprise Linux 7. We'll use the latest version (as of March 12, 2020) of each distro\u2019s Docker image and compare them in four different areas: image size, number of packages installed by default, number of historical vulnerabilities reported on cvedetails.com , and the number of vulnerabilities reported by the Clair scan. This table summarizes the numbers for each distribution: Alpine Ubuntu CentOS RHEL7 Image Version alpine:3.11.3 ubuntu:18.04 centos:centos8.1.1911 rhel7:7.7-481 Image Size 5.59MB 64.2MB 237MB 205MB Number of Packages Installed 14 89 173 162 Number of Historical CVE*s 2 2007 2 662 Number of Vulnerabilities Reported by Clair 0 32 7 0 *CVE - Common Vulnerabilities and Exposures Image Size \u00b6 Alpine\u2019s advantage in image size is obvious. Although smaller size doesn\u2019t directly translate into better security, the smaller size does mean less code packed into the image, which means smaller attack surface. Number of Packages Installed \u00b6 Alpine has the fewest packages out of box. This is not a surprise given its tiny size. Fewer packages means lesser chance of having vulnerabilities in the dependencies - a plus for security. Number of Historical CVEs \u00b6 It\u2019s interesting to see Alpine and CentOS tie for the first place in this category, even though CentOS has a close relationship with RHEL7 and RHEL7 has 600+ reported vulnerabilities. Number of Vulnerabilities Reported by Clair \u00b6 There is a good chance that some vulnerabilities reported by Clair are not real issues, but their presence is also an issue. It means extra overhead for developers or security teams to triage these findings. This overhead can be avoided if unnecessary dependencies are excluded from the image in the first place. Result \u00b6 Admittedly, none of the four areas is perfect for evaluating the security of a Linux distro, but in combination, they provide a clear picture that Alpine is the winner in this comparison. DevOps Docker images \u00b6 For all of the reasons described this comparison of distro's and more, we've selected Alpine as the distro used for all of our Docker images. References \u00b6 CIS Docker Benchmarks Alpine CVEs Ubuntu CVEs CentOS CVEs Redhat Enterprise Linux CVEs Ping Identity's Docker Image Hardening Guide \u00b6 View Ping Identity's Hardening Guide which outlines best practices for securing your product Docker Image.","title":"DevOps Image Security"},{"location":"reference/dockerImageSecurity/#evaluation-of-docker-base-image-security","text":"In CIS (Center for Internet Security) Docker Benchmark v1.2.0 , one of the recommendations says, \"4.3 Ensure that unnecessary packages are not installed in the container.\" It further states, \"You should consider using a minimal base image rather than the standard Red Hat/CentOS/Debian images if you can. Some of the options available include BusyBox and Alpine.\" Is Alpine Docker image really more secure than other, more popular Linux distributions? So, let's take a look at the security aspects of different Linux distributions. This doesn't necessarily mean that one is the best for Docker base images. Other factors such as usability and compatibility should also be considered when choosing the most suitable Docker image for an organization.","title":"Evaluation of Docker Base Image Security"},{"location":"reference/dockerImageSecurity/#evaluation","text":"To evaluate Alpine\u2019s security, we'll compare it with the following popular Linux distros: Ubuntu, CentOS, and Red Hat Enterprise Linux 7. We'll use the latest version (as of March 12, 2020) of each distro\u2019s Docker image and compare them in four different areas: image size, number of packages installed by default, number of historical vulnerabilities reported on cvedetails.com , and the number of vulnerabilities reported by the Clair scan. This table summarizes the numbers for each distribution: Alpine Ubuntu CentOS RHEL7 Image Version alpine:3.11.3 ubuntu:18.04 centos:centos8.1.1911 rhel7:7.7-481 Image Size 5.59MB 64.2MB 237MB 205MB Number of Packages Installed 14 89 173 162 Number of Historical CVE*s 2 2007 2 662 Number of Vulnerabilities Reported by Clair 0 32 7 0 *CVE - Common Vulnerabilities and Exposures","title":"Evaluation"},{"location":"reference/dockerImageSecurity/#image-size","text":"Alpine\u2019s advantage in image size is obvious. Although smaller size doesn\u2019t directly translate into better security, the smaller size does mean less code packed into the image, which means smaller attack surface.","title":"Image Size"},{"location":"reference/dockerImageSecurity/#number-of-packages-installed","text":"Alpine has the fewest packages out of box. This is not a surprise given its tiny size. Fewer packages means lesser chance of having vulnerabilities in the dependencies - a plus for security.","title":"Number of Packages Installed"},{"location":"reference/dockerImageSecurity/#number-of-historical-cves","text":"It\u2019s interesting to see Alpine and CentOS tie for the first place in this category, even though CentOS has a close relationship with RHEL7 and RHEL7 has 600+ reported vulnerabilities.","title":"Number of Historical CVEs"},{"location":"reference/dockerImageSecurity/#number-of-vulnerabilities-reported-by-clair","text":"There is a good chance that some vulnerabilities reported by Clair are not real issues, but their presence is also an issue. It means extra overhead for developers or security teams to triage these findings. This overhead can be avoided if unnecessary dependencies are excluded from the image in the first place.","title":"Number of Vulnerabilities Reported by Clair"},{"location":"reference/dockerImageSecurity/#result","text":"Admittedly, none of the four areas is perfect for evaluating the security of a Linux distro, but in combination, they provide a clear picture that Alpine is the winner in this comparison.","title":"Result"},{"location":"reference/dockerImageSecurity/#devops-docker-images","text":"For all of the reasons described this comparison of distro's and more, we've selected Alpine as the distro used for all of our Docker images.","title":"DevOps Docker images"},{"location":"reference/dockerImageSecurity/#references","text":"CIS Docker Benchmarks Alpine CVEs Ubuntu CVEs CentOS CVEs Redhat Enterprise Linux CVEs","title":"References"},{"location":"reference/dockerImageSecurity/#ping-identitys-docker-image-hardening-guide","text":"View Ping Identity's Hardening Guide which outlines best practices for securing your product Docker Image.","title":"Ping Identity's Docker Image Hardening Guide"},{"location":"reference/dockerImagesRef/","text":"DevOps Docker Images Reference \u00b6 The reference documents for our Docker images include references to related Docker images, ports exposed for the container, environment variables for the image, and associated deployment information. The reference documents are auto-generated from each new build, so are always current.","title":"Introduction"},{"location":"reference/dockerImagesRef/#devops-docker-images-reference","text":"The reference documents for our Docker images include references to related Docker images, ports exposed for the container, environment variables for the image, and associated deployment information. The reference documents are auto-generated from each new build, so are always current.","title":"DevOps Docker Images Reference"},{"location":"reference/hooks/","text":"Using DevOps Hooks \u00b6 Our DevOps hooks are build-specific scripts that are called or can be called by the entrypoint.sh script that is used to start up our product containers. Use of our DevOps hooks is intended only for DevOps professionals. The available hooks are built with the DevOps images, and can be found in the hooks subdirectory of each product directory in the Docker Builds repository. In the entrypoint.sh startup script, there is an example (stub) provided for the available hooks for all products. Warning It's critical that the supplied hook names be used if you modify entrypoint.sh (for example, to make subtle changes to a server profile). Using .pre and .post Hooks \u00b6 When DevOps hooks are called during the entrypoint.sh script process, any corresponding .pre and .post hooks will also be called. The .pre and .post extensions allow you to define custom scripts to be executed before or after any hook that is run in the container. You can include any custom .pre and .post hooks in the hooks directory of your server profile. Hooks with a .pre extension are run before the corresponding hook, and hooks with a .post extension are run after the corresponding hook. For example, a script named 80-post-start.sh.pre will be run just before the 80-post-start.sh hook starts, and a script named 80-post-start.sh.post will be run just after that hook completes.","title":"DevOps Hooks"},{"location":"reference/hooks/#using-devops-hooks","text":"Our DevOps hooks are build-specific scripts that are called or can be called by the entrypoint.sh script that is used to start up our product containers. Use of our DevOps hooks is intended only for DevOps professionals. The available hooks are built with the DevOps images, and can be found in the hooks subdirectory of each product directory in the Docker Builds repository. In the entrypoint.sh startup script, there is an example (stub) provided for the available hooks for all products. Warning It's critical that the supplied hook names be used if you modify entrypoint.sh (for example, to make subtle changes to a server profile).","title":"Using DevOps Hooks"},{"location":"reference/hooks/#using-pre-and-post-hooks","text":"When DevOps hooks are called during the entrypoint.sh script process, any corresponding .pre and .post hooks will also be called. The .pre and .post extensions allow you to define custom scripts to be executed before or after any hook that is run in the container. You can include any custom .pre and .post hooks in the hooks directory of your server profile. Hooks with a .pre extension are run before the corresponding hook, and hooks with a .post extension are run after the corresponding hook. For example, a script named 80-post-start.sh.pre will be run just before the 80-post-start.sh hook starts, and a script named 80-post-start.sh.post will be run just after that hook completes.","title":"Using .pre and .post Hooks"},{"location":"reference/imageSupport/","text":"Ping Identity Docker Image Support Policy \u00b6 Overview \u00b6 Unlike software delivered as an archive, Docker Images include product artifacts, OS shim, an optimized JVM build and miscellaneous tools/libraries (Git, SSH, SSL) to run the software and automation scripts. Due to the number of dependency updates and to ensure all patches are kept up to date, Ping Identity actively maintains product images semi-weekly (edge), releasing a stable build each month (sprint and latest). The build process retrieves the latest versions of: Operating System Shim (Alpine) Optimized Java VM Product files Supporting tools/libraries Actively Maintained Images \u00b6 The DevOps program actively maintains docker images for: the two (2) most recent feature releases (major/minor) of each product the latest patch release for each minor version Examples: If we currently maintain images for PingFederate 10.0 and 10.1, when PingFederate 10.2 is released, docker images with PingFederate 10.0 will no longer be actively maintained. If a patch is released for 10.1, it supersedes the previous patch. In other words, if we currently maintain an image for PingFederate 10.1.2, when PingFederate 10.1.3 is released it replaces 10.1.2. Docker Hub Images Image versions that have fallen out of Ping's image active maintenance window will be removed from DockerHub 3 months after it was last actively maintained. Active Build Product Versions To view products and versions actively being built, navigate to the most recent Release Notes .","title":"Using Supported Images"},{"location":"reference/imageSupport/#ping-identity-docker-image-support-policy","text":"","title":"Ping Identity Docker Image Support Policy"},{"location":"reference/imageSupport/#overview","text":"Unlike software delivered as an archive, Docker Images include product artifacts, OS shim, an optimized JVM build and miscellaneous tools/libraries (Git, SSH, SSL) to run the software and automation scripts. Due to the number of dependency updates and to ensure all patches are kept up to date, Ping Identity actively maintains product images semi-weekly (edge), releasing a stable build each month (sprint and latest). The build process retrieves the latest versions of: Operating System Shim (Alpine) Optimized Java VM Product files Supporting tools/libraries","title":"Overview"},{"location":"reference/imageSupport/#actively-maintained-images","text":"The DevOps program actively maintains docker images for: the two (2) most recent feature releases (major/minor) of each product the latest patch release for each minor version Examples: If we currently maintain images for PingFederate 10.0 and 10.1, when PingFederate 10.2 is released, docker images with PingFederate 10.0 will no longer be actively maintained. If a patch is released for 10.1, it supersedes the previous patch. In other words, if we currently maintain an image for PingFederate 10.1.2, when PingFederate 10.1.3 is released it replaces 10.1.2. Docker Hub Images Image versions that have fallen out of Ping's image active maintenance window will be removed from DockerHub 3 months after it was last actively maintained. Active Build Product Versions To view products and versions actively being built, navigate to the most recent Release Notes .","title":"Actively Maintained Images"},{"location":"reference/ldapsdkUtil/","text":"The ldap-sdk-tools utility \u00b6 The ldap-sdk-tools Docker image gives you easy access to our LDAP Client SDK tools for use with PingDirectory. See the pingidentity/ldapsdk repository for complete documentation. Setup \u00b6 The first time you run the ldapsdk script, you'll be prompted to configure your settings. From your local pingidentity-devops-getting-started directory, enter: ./ldapsdk To edit the settings in the future, enter: ldapsdk configure You can then start the ldap-sdk-tools Docker image by entering: docker run -it --rm --network pingnet pingidentity/ldap-sdk-tools:latest Enter ls to list the available tools.","title":"ldap-sdk-tools"},{"location":"reference/ldapsdkUtil/#the-ldap-sdk-tools-utility","text":"The ldap-sdk-tools Docker image gives you easy access to our LDAP Client SDK tools for use with PingDirectory. See the pingidentity/ldapsdk repository for complete documentation.","title":"The ldap-sdk-tools utility"},{"location":"reference/ldapsdkUtil/#setup","text":"The first time you run the ldapsdk script, you'll be prompted to configure your settings. From your local pingidentity-devops-getting-started directory, enter: ./ldapsdk To edit the settings in the future, enter: ldapsdk configure You can then start the ldap-sdk-tools Docker image by entering: docker run -it --rm --network pingnet pingidentity/ldap-sdk-tools:latest Enter ls to list the available tools.","title":"Setup"},{"location":"reference/profileStructures/","text":"Server Profile Structures \u00b6 Each of the Docker images use a server profile structure that is specific to each product. The structure (directory paths and data) of the server profile differs between products. Depending on how you Deploy Your Server Profile , it is will be pulled/mounted into /opt/in on the container and used to stage your deployment. Below, you will find the server profile structures for each of our products with some example usages. To help with an example of the basics refer to the pingidentity-server-profiles/getting-started examples. Ignore .sec directories in examples For the getting-started profile examples, you should not use the practice of the .sec directory when providing passwords to your containers. These are intended for demonstration purposes. Rather, you should set an environment variable with your secrets or orchestration later: PING_IDENTITY_PASSWORD = \"secret\" PingFederate \u00b6 Example at getting-started/pingfederate . Path Location description instance Directories and files that you want to be used at product runtime, in accordance with the directory layout of the product. instance/server/default/data An unzipped configuration archive exported from PingFederate. instance/bulk-config/data.json A json export from the PingFed admin api /bulk/export . instance/server/default/deploy/OAuthPlayground.war Automatically deploy the OAuthPlayground web application. instance/server/default/conf/META-INF/hivemodule.xml Apply a Hive module config to the container. Used for persisting OAuth clients, grants, and sessions to an external DB. PingAccess \u00b6 Example at getting-started/pingaccess . Path Location description instance Directories and files that you want to be used at product runtime, in accordance with the directory layout of the product. instance/conf/pa.jwk Used to decrypt a data.json configuration upon import instance/data/data.json PA 6.1+ A config file that, if found by the container, is uploaded into the container instance/data/PingAccess.mv.db database binary that would be ingested at container startup if found. PingAccess Best Practices PingAccess profiles are typically minimalist. This is because the majority of PingAccess configurations can be found within a data.json or PingAccess.mv.db file. We highly recommend you only use data.json for configurations and only use PingAccess.mv.db if necessary. You can easily view and manipulate configurations directly in a JSON file as opposed to the binary PingAccess.mv.db file. This makes tracking changes in version control easier as well. PingAccess 6.1.x+ supports using only data.json , even when clustering. However on 6.1.0.3 make sure data.json is only supplied to the admin node. PingAccess 6.1.0+ PingAccess now supports native data.json ingestion. This is the recommended method . Place data.json or data.json.subst in instance/conf/data/start-up-deployer . The JSON configuration file for PingAccess must be named data.json . A data.json file that corresponds to earlier PingAccess versions might be accepted. However, once you're on version 6.1.x, the data.json file will be forward compatible. This means you're able to avoid upgrades for your deployments! PingAccess 6.0.x and prior The JSON configuration file for PingAccess must be named data.json and located in the instance/data directory. All PingAccess versions A corresponding file named pa.jwk must also exist in the instance/conf directory for the data.json file to be decrypted on import. To get a data.json and pa.jwk that work together, pull them both from the same running PingAccess instance. For example, if PingAccess is running in a local Docker container you can use these commands to export the data.json file and copy the pa.jwk file to your local Downloads directory: curl -k -u \"Administrator: ${ ADMIN_PASSWORD } \" -H \"X-Xsrf-Header: PingAccess\" https://localhost:9000/pa-admin-api/v3/config/export -o ~/Downloads/data.json docker cp <container_name>:/opt/out/instance/conf/pa.jwk ~/Downloads/pa.jwk Password Variables The PingAccess administrator password is not found in data.json , but in PingAccess.mv.db . For this reason, here are environment variables you can use to manage different scenarios: PING_IDENTITY_PASSWORD Use this variable if: You're starting a PingAccess container without any configurations. You're using only a data.json file for configurations. Your PingAccess.mv.db file has a password other than the default \"2Access\". The PING_IDENTITY_PASSWORD value will be used for all interactions with the PingAccess Admin API (such as, importing configurations, and creating clustering). PA_ADMIN_PASSWORD_INITIAL Use this in addition to PING_IDENTITY_PASSWORD to change the runtime admin password and override the password in PingAccess.mv.db . If you use only data.json and don't pass PING_IDENTITY_PASSWORD , the password will default to \"2FederateM0re\". So, always use PING_IDENTITY_PASSWORD . Ping Data Products \u00b6 The Ping Data Products (PingDirectory, PingDataSync, PingDataGovernance, PingDirectoryProxy) follow the same structure for server-profiles. Example at getting-started/pingdirectory . Path Location description pd.profile Server profile matching the structure as defined by PingDirectory Server Profiles instance Directories and files that you want to be used at product runtime, in accordance with the layout of the product. In general, this should be non existing or empty . env-vars You may set environment variables used during deployment. See Variables and Scope for more info. In general, this should be non existing or empty . Ping Data Server Profile Best Practices In most circumstances, the pd.profile should be on the only directory in the server profile. It is advised that all environment variables be provided via kubernetes configmaps/secrets and secret management tool. Be careful providing an env-vars and if you do, please review Variables and Scope Creating a pd.profile from scratch Use the manage-profile tool (found in product bin directory) to generate a pd.profile from an existing Ping Data 8.0+ deployment. An example on creating this pd.profile looks like: manage-profile generate-profile --profileRoot /tmp/pd.profile rm /tmp/pd.profile/setup-arguments.txt Follow instructions provided when you run the generate-profile to ensure that you include any additional components, such as encryption-settings","title":"Server Profile"},{"location":"reference/profileStructures/#server-profile-structures","text":"Each of the Docker images use a server profile structure that is specific to each product. The structure (directory paths and data) of the server profile differs between products. Depending on how you Deploy Your Server Profile , it is will be pulled/mounted into /opt/in on the container and used to stage your deployment. Below, you will find the server profile structures for each of our products with some example usages. To help with an example of the basics refer to the pingidentity-server-profiles/getting-started examples. Ignore .sec directories in examples For the getting-started profile examples, you should not use the practice of the .sec directory when providing passwords to your containers. These are intended for demonstration purposes. Rather, you should set an environment variable with your secrets or orchestration later: PING_IDENTITY_PASSWORD = \"secret\"","title":"Server Profile Structures"},{"location":"reference/profileStructures/#pingfederate","text":"Example at getting-started/pingfederate . Path Location description instance Directories and files that you want to be used at product runtime, in accordance with the directory layout of the product. instance/server/default/data An unzipped configuration archive exported from PingFederate. instance/bulk-config/data.json A json export from the PingFed admin api /bulk/export . instance/server/default/deploy/OAuthPlayground.war Automatically deploy the OAuthPlayground web application. instance/server/default/conf/META-INF/hivemodule.xml Apply a Hive module config to the container. Used for persisting OAuth clients, grants, and sessions to an external DB.","title":"PingFederate"},{"location":"reference/profileStructures/#pingaccess","text":"Example at getting-started/pingaccess . Path Location description instance Directories and files that you want to be used at product runtime, in accordance with the directory layout of the product. instance/conf/pa.jwk Used to decrypt a data.json configuration upon import instance/data/data.json PA 6.1+ A config file that, if found by the container, is uploaded into the container instance/data/PingAccess.mv.db database binary that would be ingested at container startup if found. PingAccess Best Practices PingAccess profiles are typically minimalist. This is because the majority of PingAccess configurations can be found within a data.json or PingAccess.mv.db file. We highly recommend you only use data.json for configurations and only use PingAccess.mv.db if necessary. You can easily view and manipulate configurations directly in a JSON file as opposed to the binary PingAccess.mv.db file. This makes tracking changes in version control easier as well. PingAccess 6.1.x+ supports using only data.json , even when clustering. However on 6.1.0.3 make sure data.json is only supplied to the admin node. PingAccess 6.1.0+ PingAccess now supports native data.json ingestion. This is the recommended method . Place data.json or data.json.subst in instance/conf/data/start-up-deployer . The JSON configuration file for PingAccess must be named data.json . A data.json file that corresponds to earlier PingAccess versions might be accepted. However, once you're on version 6.1.x, the data.json file will be forward compatible. This means you're able to avoid upgrades for your deployments! PingAccess 6.0.x and prior The JSON configuration file for PingAccess must be named data.json and located in the instance/data directory. All PingAccess versions A corresponding file named pa.jwk must also exist in the instance/conf directory for the data.json file to be decrypted on import. To get a data.json and pa.jwk that work together, pull them both from the same running PingAccess instance. For example, if PingAccess is running in a local Docker container you can use these commands to export the data.json file and copy the pa.jwk file to your local Downloads directory: curl -k -u \"Administrator: ${ ADMIN_PASSWORD } \" -H \"X-Xsrf-Header: PingAccess\" https://localhost:9000/pa-admin-api/v3/config/export -o ~/Downloads/data.json docker cp <container_name>:/opt/out/instance/conf/pa.jwk ~/Downloads/pa.jwk Password Variables The PingAccess administrator password is not found in data.json , but in PingAccess.mv.db . For this reason, here are environment variables you can use to manage different scenarios: PING_IDENTITY_PASSWORD Use this variable if: You're starting a PingAccess container without any configurations. You're using only a data.json file for configurations. Your PingAccess.mv.db file has a password other than the default \"2Access\". The PING_IDENTITY_PASSWORD value will be used for all interactions with the PingAccess Admin API (such as, importing configurations, and creating clustering). PA_ADMIN_PASSWORD_INITIAL Use this in addition to PING_IDENTITY_PASSWORD to change the runtime admin password and override the password in PingAccess.mv.db . If you use only data.json and don't pass PING_IDENTITY_PASSWORD , the password will default to \"2FederateM0re\". So, always use PING_IDENTITY_PASSWORD .","title":"PingAccess"},{"location":"reference/profileStructures/#ping-data-products","text":"The Ping Data Products (PingDirectory, PingDataSync, PingDataGovernance, PingDirectoryProxy) follow the same structure for server-profiles. Example at getting-started/pingdirectory . Path Location description pd.profile Server profile matching the structure as defined by PingDirectory Server Profiles instance Directories and files that you want to be used at product runtime, in accordance with the layout of the product. In general, this should be non existing or empty . env-vars You may set environment variables used during deployment. See Variables and Scope for more info. In general, this should be non existing or empty . Ping Data Server Profile Best Practices In most circumstances, the pd.profile should be on the only directory in the server profile. It is advised that all environment variables be provided via kubernetes configmaps/secrets and secret management tool. Be careful providing an env-vars and if you do, please review Variables and Scope Creating a pd.profile from scratch Use the manage-profile tool (found in product bin directory) to generate a pd.profile from an existing Ping Data 8.0+ deployment. An example on creating this pd.profile looks like: manage-profile generate-profile --profileRoot /tmp/pd.profile rm /tmp/pd.profile/setup-arguments.txt Follow instructions provided when you run the generate-profile to ensure that you include any additional components, such as encryption-settings","title":"Ping Data Products"},{"location":"reference/releaseTags/","text":"Using Release Tags \u00b6 Ping Identity uses multiple tags for each released build image. On our Docker Hub site, you can view the available tags for each image. All product containers in a stack should use the same release tag. Tagging Format \u00b6 The format used to specify a release tag for stacks is: image : pingidentity/<ping-product>:${PING_IDENTITY_DEVOPS_TAG} Where <ping-product> is the name of the product container and ${PING_IDENTITY_DEVOPS_TAG} is assigned the release tag value. The reference to the file containing the setting for ${PING_IDENTITY_DEVOPS_TAG} is ~/.pingidentity/devops by default. You can also specify the release tag explicitly in the YAML file. The release tag must be the same for each container in the stack. For example: image : pingidentity/<ping-product>:edge Base Release Tags \u00b6 The base release tags for a build are: edge latest sprint edge \u00b6 The edge release tag refers to \"bleeding edge\", indicating a build similar to an alpha release. This sliding tag includes the absolute latest hooks and scripts, but is considered highly unstable. The edge release is characterized by: Latest product version. Latest build image enhancements and fixes from our current sprint. Runs on the Linux Alpine OS. For example, pingaccess:edge . latest \u00b6 The latest release tag indicates the latest stable release. This is a sliding tag that marks the stable release for the latest sprint. The latest release is characterized by: Latest product version All completed and qualified enhacements and fixes from the prior monthly sprint. Runs on the Linux Alpine OS. For example, pingfederate:latest . sprint \u00b6 The sprint release tag is a build number and indicates a stable build that is guaranteed to not change. The sprint number uses the YYMM format. For example, 1909 = September 2020. Latest product version at the time the sprint ended. All completed and qualified enhacements and fixes from the specified monthly sprint. The Docker images are generated at the end of the specified monthly sprint. Runs on the Linux Alpine OS. For example, pingfederate:1909 . Which Release Tag To Use \u00b6 You should test all images in development before deploying to production. It's also best practice to use a full tag variation like pingaccess:5.3.0-alpine-edge , rather than pingaccess:edge to avoid dependency conflicts in server profiles. In general, we recommend: Use the edge release tag for demonstrations and testing latest features. edge is not suited for production use cases, because the underlying image is subject to change and backwards-compatibility is not guaranteed. Use the sprint release tag for development and production. The sprint tag is the only tag that is guaranteed to not change and as such provides the most stability for repeatable deployment in development and production environments. Use the latest in those rare scenarios that require stability between product sprints, but can accept a sliding tag. But what if you want bleeding edge features and a stable build image? For this, the best option is to periodically pull the Docker images having the base tag and store them in a local or private repository. Docker images produced before September 1, 2019 having a tag format of :product-edge or :productVersion:edge will not receive further updates. Determine Image Product Version \u00b6 If you're unsure of the product version for the container you are running, shell into the container, then echo the $IMAGE_VERSION environment variable. For example: docker container exec -it <container id> sh echo $IMAGE_VERSION The IMAGE_VERSION variable will return the version in this format: [product]-[container OS]-[jdk]-[product version]-[build date]-[git revision] . For example: IMAGE_VERSION = pingcentral-alpine-az11-1.3.0-200629-bc33 Where: Key Value Product pingcentral Container OS alpine JDK az11 Product Version 1.3.0 Build Date 200629* Git Revision bc33 * Date is in YYMMDD format","title":"Release Tags"},{"location":"reference/releaseTags/#using-release-tags","text":"Ping Identity uses multiple tags for each released build image. On our Docker Hub site, you can view the available tags for each image. All product containers in a stack should use the same release tag.","title":"Using Release Tags"},{"location":"reference/releaseTags/#tagging-format","text":"The format used to specify a release tag for stacks is: image : pingidentity/<ping-product>:${PING_IDENTITY_DEVOPS_TAG} Where <ping-product> is the name of the product container and ${PING_IDENTITY_DEVOPS_TAG} is assigned the release tag value. The reference to the file containing the setting for ${PING_IDENTITY_DEVOPS_TAG} is ~/.pingidentity/devops by default. You can also specify the release tag explicitly in the YAML file. The release tag must be the same for each container in the stack. For example: image : pingidentity/<ping-product>:edge","title":"Tagging Format"},{"location":"reference/releaseTags/#base-release-tags","text":"The base release tags for a build are: edge latest sprint","title":"Base Release Tags"},{"location":"reference/releaseTags/#edge","text":"The edge release tag refers to \"bleeding edge\", indicating a build similar to an alpha release. This sliding tag includes the absolute latest hooks and scripts, but is considered highly unstable. The edge release is characterized by: Latest product version. Latest build image enhancements and fixes from our current sprint. Runs on the Linux Alpine OS. For example, pingaccess:edge .","title":"edge"},{"location":"reference/releaseTags/#latest","text":"The latest release tag indicates the latest stable release. This is a sliding tag that marks the stable release for the latest sprint. The latest release is characterized by: Latest product version All completed and qualified enhacements and fixes from the prior monthly sprint. Runs on the Linux Alpine OS. For example, pingfederate:latest .","title":"latest"},{"location":"reference/releaseTags/#sprint","text":"The sprint release tag is a build number and indicates a stable build that is guaranteed to not change. The sprint number uses the YYMM format. For example, 1909 = September 2020. Latest product version at the time the sprint ended. All completed and qualified enhacements and fixes from the specified monthly sprint. The Docker images are generated at the end of the specified monthly sprint. Runs on the Linux Alpine OS. For example, pingfederate:1909 .","title":"sprint"},{"location":"reference/releaseTags/#which-release-tag-to-use","text":"You should test all images in development before deploying to production. It's also best practice to use a full tag variation like pingaccess:5.3.0-alpine-edge , rather than pingaccess:edge to avoid dependency conflicts in server profiles. In general, we recommend: Use the edge release tag for demonstrations and testing latest features. edge is not suited for production use cases, because the underlying image is subject to change and backwards-compatibility is not guaranteed. Use the sprint release tag for development and production. The sprint tag is the only tag that is guaranteed to not change and as such provides the most stability for repeatable deployment in development and production environments. Use the latest in those rare scenarios that require stability between product sprints, but can accept a sliding tag. But what if you want bleeding edge features and a stable build image? For this, the best option is to periodically pull the Docker images having the base tag and store them in a local or private repository. Docker images produced before September 1, 2019 having a tag format of :product-edge or :productVersion:edge will not receive further updates.","title":"Which Release Tag To Use"},{"location":"reference/releaseTags/#determine-image-product-version","text":"If you're unsure of the product version for the container you are running, shell into the container, then echo the $IMAGE_VERSION environment variable. For example: docker container exec -it <container id> sh echo $IMAGE_VERSION The IMAGE_VERSION variable will return the version in this format: [product]-[container OS]-[jdk]-[product version]-[build date]-[git revision] . For example: IMAGE_VERSION = pingcentral-alpine-az11-1.3.0-200629-bc33 Where: Key Value Product pingcentral Container OS alpine JDK az11 Product Version 1.3.0 Build Date 200629* Git Revision bc33 * Date is in YYMMDD format","title":"Determine Image Product Version"},{"location":"reference/troubleshooting/","text":"Troubleshooting \u00b6 Get started \u00b6 Examples Not Working \u00b6 One of the most common errors is due to having stale images. Our development is highly dynamic and Docker images can rapidly change. To avoid issues with stale images, you can have Docker pull the latest images by removing all the local. Enter: docker rmi $( docker images \"pingidentity/*\" -q ) Images tagged as \"latest\" locally does not mean they are the latest in the Docker hub registry. Misconfigured ~/.bash_profile file \u00b6 If your containers are not able to pull a license based on your DevOps user name and key, or running dhelp returns an error, there may be some misconfiguration in your ~/.bash_profile file. Possible solutions: If you have just run ./setup for the first time, make sure you are have done so in a fresh terminal, or have run source ~/.bash_profile . If running echo PING_IDENTITY_DEVOPS_USER returns nothing in a fresh terminal, it's likely your ~/.bash_profile file is misconfigured. There are two entries that need to be there: source <path>/pingidentity-devops-getting-started/bash_profile_devops Where <path> is the full path to the pingidentity-devops-getting-started directory. This entry sources our DevOps aliases. There also needs to be another entry for: sourcePingIdentityFiles This entry sources the Ping Identity file aliases. Make sure there are not old versions or duplicates of these entries. If you are running in Kubernetes, keep in mind that your PING_IDENTITY_DEVOPS_USER and key are local variables and need to be Passed as a Secret in your cluster. Unable To Retrieve Evaluation License \u00b6 If a product instance or instances are unable to get the evaluation license, an error similar to this may be generated: ----- Starting hook: /opt/staging/hooks/17-check-license.sh Pulling evaluation license from Ping Identity for : Prod License: PD - v7.3 DevOps User: some-devops-user@example.com... Unable to download evaluation product.lic ( 000 ) , most likely due to invalid PING_IDENTITY_DEVOPS_USER/PING_IDENTITY_DEVOPS_KEY ################################################################################## ############################ ALERT ################################# ################################################################################## # # No Ping Identity License File (PingDirectory.lic) was found in the server profile. # No Ping Identity DevOps User or Key was passed. # # # More info on obtaining your DevOps User and Key can be found at: # https://devops.pingidentity.com/get-started/devopsRegistration/ # ################################################################################## CONTAINER FAILURE: License File absent CONTAINER FAILURE: Error running 17 -check-license.sh CONTAINER FAILURE: Error running 10 -start-sequence.sh This can be caused by: An invalid DevOps user name or key (as noted in the error). This is usually caused by some issue with the variables being passed in. To verify the variables are available to the shell running (when running Docker commands), enter: echo $PING_IDENTITY_DEVOPS_USER $PING_IDENTITY_DEVOPS_KEY A bad Docker image. Pull the Docker image again to verify. Network connectivity to the license server is blocked. To test this, from the machine that is running the container, enter: curl -k https://license.pingidentity.com/devops/v2/license If the license server isn't accessible, an error similar to this is returned: { \"error\" : \"missing devops-user header\" }","title":"Troubleshooting"},{"location":"reference/troubleshooting/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"reference/troubleshooting/#get-started","text":"","title":"Get started"},{"location":"reference/troubleshooting/#examples-not-working","text":"One of the most common errors is due to having stale images. Our development is highly dynamic and Docker images can rapidly change. To avoid issues with stale images, you can have Docker pull the latest images by removing all the local. Enter: docker rmi $( docker images \"pingidentity/*\" -q ) Images tagged as \"latest\" locally does not mean they are the latest in the Docker hub registry.","title":"Examples Not Working"},{"location":"reference/troubleshooting/#misconfigured-bash_profile-file","text":"If your containers are not able to pull a license based on your DevOps user name and key, or running dhelp returns an error, there may be some misconfiguration in your ~/.bash_profile file. Possible solutions: If you have just run ./setup for the first time, make sure you are have done so in a fresh terminal, or have run source ~/.bash_profile . If running echo PING_IDENTITY_DEVOPS_USER returns nothing in a fresh terminal, it's likely your ~/.bash_profile file is misconfigured. There are two entries that need to be there: source <path>/pingidentity-devops-getting-started/bash_profile_devops Where <path> is the full path to the pingidentity-devops-getting-started directory. This entry sources our DevOps aliases. There also needs to be another entry for: sourcePingIdentityFiles This entry sources the Ping Identity file aliases. Make sure there are not old versions or duplicates of these entries. If you are running in Kubernetes, keep in mind that your PING_IDENTITY_DEVOPS_USER and key are local variables and need to be Passed as a Secret in your cluster.","title":"Misconfigured ~/.bash_profile file"},{"location":"reference/troubleshooting/#unable-to-retrieve-evaluation-license","text":"If a product instance or instances are unable to get the evaluation license, an error similar to this may be generated: ----- Starting hook: /opt/staging/hooks/17-check-license.sh Pulling evaluation license from Ping Identity for : Prod License: PD - v7.3 DevOps User: some-devops-user@example.com... Unable to download evaluation product.lic ( 000 ) , most likely due to invalid PING_IDENTITY_DEVOPS_USER/PING_IDENTITY_DEVOPS_KEY ################################################################################## ############################ ALERT ################################# ################################################################################## # # No Ping Identity License File (PingDirectory.lic) was found in the server profile. # No Ping Identity DevOps User or Key was passed. # # # More info on obtaining your DevOps User and Key can be found at: # https://devops.pingidentity.com/get-started/devopsRegistration/ # ################################################################################## CONTAINER FAILURE: License File absent CONTAINER FAILURE: Error running 17 -check-license.sh CONTAINER FAILURE: Error running 10 -start-sequence.sh This can be caused by: An invalid DevOps user name or key (as noted in the error). This is usually caused by some issue with the variables being passed in. To verify the variables are available to the shell running (when running Docker commands), enter: echo $PING_IDENTITY_DEVOPS_USER $PING_IDENTITY_DEVOPS_KEY A bad Docker image. Pull the Docker image again to verify. Network connectivity to the license server is blocked. To test this, from the machine that is running the container, enter: curl -k https://license.pingidentity.com/devops/v2/license If the license server isn't accessible, an error similar to this is returned: { \"error\" : \"missing devops-user header\" }","title":"Unable To Retrieve Evaluation License"},{"location":"reference/usingCertificates/","text":"Using Certificates with Images \u00b6 This provides details for using certificates with the Ping Identity images. Specifically, the preferred locations to place the certificate and pin/key files to provide best security practices and use by the underlying Ping Identity product. Currently, certificates can be provided to the PingData products when the containers are started. Prior to 2008 sprint release We encouraged these to be placed into the server profile (i.e. .../.sec/keystore). For security best practices, this is no longer a recommended approach. Rather a secret should be used to pass this material to the containers. What You'll Do \u00b6 The examples below will explain and show examples of: Deploying a certificate/pin combo to an image in a secure way Prerequisites \u00b6 You've already been through Get started to set up your DevOps environment and run a test deployment of the products. Preferably, have a secrets management system (i.e. Hashicorp Vault) that holds your certificate and places them into your SECRETS_DIR (i.e. /run/secrets). Refer to Using Hashicorp Vault for information on using a vault if you have one. PingData Image Certificates \u00b6 The PingData products (i.e. PingDirectory, PingDataSync, PingDataGovernance, PingDirectoryProxy) use a file location to determine certificates/pin files. It is best practice to use non-persistent location (i.e. /run/secrets) to store these files. If no certificate is provided, the container/product will generate a self-signed certificate. The default location for certificates and associated files are listed below (assumes a default SECRETS_DIR variable of /run/secrets ). Variable Used Default Location/Value /run/secrets... Notes Keystore (JKS) KEYSTORE_FILE keystore JKS Format. Set as default in absence of .p12 suffix Keystore (PKCS12) KEYSTORE_FILE keystore.p12 PKCS12 Format Keystore Type KEYSTORE_TYPE jks or pkcs12 Based on suffix of KEYSTORE_FILE Keystore PIN KEYSTORE_PIN_FILE keystore.pin Truststore (JKS) TRUSTSTORE_FILE truststore Set as default in absence of .p12 suffix Truststore (PKCS12) TRUSTSTORE_FILE truststore.p12 PKCS12 Format Truststore Type TRUSTSTORE_TYPE jks or pkcs12 Based on suffix of TRUSTSTORE_FILE Truststore PIN TRUSTSTORE_PIN_FILE truststore.pin Certificate Nickname CERTIFICATE_NICKNAME see below CERTIFICATE_NICKNAME Setting There is an additional certificate based variable used to identity the certificate alias used within the KEYSTORE_FILE . That variable is called CERTFICATE_NICKNAME which identifies the certificate to use by the server in the KEYSTORE_FILE . If a value isn't provided, the container will look a the list certs found in the KEYSTORE_FILE and if 1 an only 1 certificate is found of type PrivateKeyEntry , then that alias will be used. Specifying your own location for a certificate If you are relying on certificates to be mounted to a different locations than the SECRET_DIR location and/or name of the files, you can provide your own values to these variables identified above to specify those locations. As an example: KEYSTORE_FILE = /my/path/to/certs/cert-file KEYSTORE_PIN_FILE = /my/path/to/certs/cert.pin KEYSTORE_TYPE = jks CERTIFICATE_NICKNAME = development-cert Non PingData Image Cerfificates \u00b6 For non PingData images (i.e. PingAccess, PingFederate) the certificates are managed withing the product configs.","title":"Certificates"},{"location":"reference/usingCertificates/#using-certificates-with-images","text":"This provides details for using certificates with the Ping Identity images. Specifically, the preferred locations to place the certificate and pin/key files to provide best security practices and use by the underlying Ping Identity product. Currently, certificates can be provided to the PingData products when the containers are started. Prior to 2008 sprint release We encouraged these to be placed into the server profile (i.e. .../.sec/keystore). For security best practices, this is no longer a recommended approach. Rather a secret should be used to pass this material to the containers.","title":"Using Certificates with Images"},{"location":"reference/usingCertificates/#what-youll-do","text":"The examples below will explain and show examples of: Deploying a certificate/pin combo to an image in a secure way","title":"What You'll Do"},{"location":"reference/usingCertificates/#prerequisites","text":"You've already been through Get started to set up your DevOps environment and run a test deployment of the products. Preferably, have a secrets management system (i.e. Hashicorp Vault) that holds your certificate and places them into your SECRETS_DIR (i.e. /run/secrets). Refer to Using Hashicorp Vault for information on using a vault if you have one.","title":"Prerequisites"},{"location":"reference/usingCertificates/#pingdata-image-certificates","text":"The PingData products (i.e. PingDirectory, PingDataSync, PingDataGovernance, PingDirectoryProxy) use a file location to determine certificates/pin files. It is best practice to use non-persistent location (i.e. /run/secrets) to store these files. If no certificate is provided, the container/product will generate a self-signed certificate. The default location for certificates and associated files are listed below (assumes a default SECRETS_DIR variable of /run/secrets ). Variable Used Default Location/Value /run/secrets... Notes Keystore (JKS) KEYSTORE_FILE keystore JKS Format. Set as default in absence of .p12 suffix Keystore (PKCS12) KEYSTORE_FILE keystore.p12 PKCS12 Format Keystore Type KEYSTORE_TYPE jks or pkcs12 Based on suffix of KEYSTORE_FILE Keystore PIN KEYSTORE_PIN_FILE keystore.pin Truststore (JKS) TRUSTSTORE_FILE truststore Set as default in absence of .p12 suffix Truststore (PKCS12) TRUSTSTORE_FILE truststore.p12 PKCS12 Format Truststore Type TRUSTSTORE_TYPE jks or pkcs12 Based on suffix of TRUSTSTORE_FILE Truststore PIN TRUSTSTORE_PIN_FILE truststore.pin Certificate Nickname CERTIFICATE_NICKNAME see below CERTIFICATE_NICKNAME Setting There is an additional certificate based variable used to identity the certificate alias used within the KEYSTORE_FILE . That variable is called CERTFICATE_NICKNAME which identifies the certificate to use by the server in the KEYSTORE_FILE . If a value isn't provided, the container will look a the list certs found in the KEYSTORE_FILE and if 1 an only 1 certificate is found of type PrivateKeyEntry , then that alias will be used. Specifying your own location for a certificate If you are relying on certificates to be mounted to a different locations than the SECRET_DIR location and/or name of the files, you can provide your own values to these variables identified above to specify those locations. As an example: KEYSTORE_FILE = /my/path/to/certs/cert-file KEYSTORE_PIN_FILE = /my/path/to/certs/cert.pin KEYSTORE_TYPE = jks CERTIFICATE_NICKNAME = development-cert","title":"PingData Image Certificates"},{"location":"reference/usingCertificates/#non-pingdata-image-cerfificates","text":"For non PingData images (i.e. PingAccess, PingFederate) the certificates are managed withing the product configs.","title":"Non PingData Image Cerfificates"},{"location":"reference/variableScoping/","text":"Variables and Scope \u00b6 DevOps variables provide a way to store and reuse values with our Docker containers, ultimately used by our Docker Image hooks to customize configurations. It's important to understand the different levels at which variables can be set and how and where you should use them. The following diagram shows the different scopes in which variables can be set and applied: Assume that you're looking down at this diagram as a pyramid, where the container is the top. The order of precedence for variables is top down. Generally, you'll set variables having an orchestration scope. Image Scope \u00b6 Variables having an image scope are assigned using the values set for the Docker Image (for example, from Dockerfiles). These variables are often set as defaults, allowing scopes with a higher level of precedence to override them. To see the default environment variables available with any Docker image, you can enter: docker run pingidentity/<product-image>:<tag> env | sort Where <product-image> is the name of one of our products, and \\ is the release tag (such as, edge ). See our Docker Images Reference for the environment variables available or each product, as well as those available for all products (PingBase). Orchestration Scope \u00b6 Variables having orchestration scope are assigned at the orchestration layer. Typically, these are environment variables set using Docker commands, or Docker Compose or Kubernetes YAML configuration files. For example: Using docker run with --env: docker run --env SCOPE = env \\ pingidentity/pingdirectory:edge env | sort Using docker run with --env-file: echo \"SCOPE=env-file\" > /tmp/scope.properties docker run --env-file /tmp/scope.properties \\ pingidentity/pingdirectory:edge env | sort Using Docker Compose (docker-compose.yaml): environment : - SCOPE=compose env_file : - /tmp/scope.properties Using Kubernetes (kustomize.yaml) env : - name : SCOPE value : kubernetes Using Kubernetes configMapRef and secretRef (kustomize.yaml) - envFrom : - configMapRef : name : kubernetes-variables - secretRef : name : kubernetes-secret Server Profile Scope \u00b6 Variables having server profile scope are supplied using property files in the server-profile repo. You need to be careful setting variables in this scope as the settings can override variables having an image or orchestration scope. The following masthead can be used in your env_vars files to provide examples of setting variables and how they might override variables having a scope with a lower level of precedence. It will also suppress a warning when processing the env_vars file: # .suppress-container-warning # # NOTICE: Settings in this file will override values set at the # image or orchestration layers of the container. Examples # include variables that are specific to this server profile. # # Options include: # # ALWAYS OVERRIDE the value in the container # NAME=VAL # # SET TO DEFAULT VALUE if not already set # export NAME=${NAME:=myDefaultValue} # Sets to string of \"myDefaultValue\" # export NAME=${NAME:-OTHER_VAR} # Sets ot value of OTHER_VAR variable # Container Scope \u00b6 Variables having a container scope are assigned in the hook scripts, and will overwrite variables that are set elsewhere. Variables that need to be passed to other hook scripts be appended to the file assigned to ${CONTAINER_ENV} , (defaults to /opt/staging/.env ). This file will be sourced for every hook script. Scoping Example \u00b6","title":"Variables and Scope"},{"location":"reference/variableScoping/#variables-and-scope","text":"DevOps variables provide a way to store and reuse values with our Docker containers, ultimately used by our Docker Image hooks to customize configurations. It's important to understand the different levels at which variables can be set and how and where you should use them. The following diagram shows the different scopes in which variables can be set and applied: Assume that you're looking down at this diagram as a pyramid, where the container is the top. The order of precedence for variables is top down. Generally, you'll set variables having an orchestration scope.","title":"Variables and Scope"},{"location":"reference/variableScoping/#image-scope","text":"Variables having an image scope are assigned using the values set for the Docker Image (for example, from Dockerfiles). These variables are often set as defaults, allowing scopes with a higher level of precedence to override them. To see the default environment variables available with any Docker image, you can enter: docker run pingidentity/<product-image>:<tag> env | sort Where <product-image> is the name of one of our products, and \\ is the release tag (such as, edge ). See our Docker Images Reference for the environment variables available or each product, as well as those available for all products (PingBase).","title":"Image Scope"},{"location":"reference/variableScoping/#orchestration-scope","text":"Variables having orchestration scope are assigned at the orchestration layer. Typically, these are environment variables set using Docker commands, or Docker Compose or Kubernetes YAML configuration files. For example: Using docker run with --env: docker run --env SCOPE = env \\ pingidentity/pingdirectory:edge env | sort Using docker run with --env-file: echo \"SCOPE=env-file\" > /tmp/scope.properties docker run --env-file /tmp/scope.properties \\ pingidentity/pingdirectory:edge env | sort Using Docker Compose (docker-compose.yaml): environment : - SCOPE=compose env_file : - /tmp/scope.properties Using Kubernetes (kustomize.yaml) env : - name : SCOPE value : kubernetes Using Kubernetes configMapRef and secretRef (kustomize.yaml) - envFrom : - configMapRef : name : kubernetes-variables - secretRef : name : kubernetes-secret","title":"Orchestration Scope"},{"location":"reference/variableScoping/#server-profile-scope","text":"Variables having server profile scope are supplied using property files in the server-profile repo. You need to be careful setting variables in this scope as the settings can override variables having an image or orchestration scope. The following masthead can be used in your env_vars files to provide examples of setting variables and how they might override variables having a scope with a lower level of precedence. It will also suppress a warning when processing the env_vars file: # .suppress-container-warning # # NOTICE: Settings in this file will override values set at the # image or orchestration layers of the container. Examples # include variables that are specific to this server profile. # # Options include: # # ALWAYS OVERRIDE the value in the container # NAME=VAL # # SET TO DEFAULT VALUE if not already set # export NAME=${NAME:=myDefaultValue} # Sets to string of \"myDefaultValue\" # export NAME=${NAME:-OTHER_VAR} # Sets ot value of OTHER_VAR variable #","title":"Server Profile Scope"},{"location":"reference/variableScoping/#container-scope","text":"Variables having a container scope are assigned in the hook scripts, and will overwrite variables that are set elsewhere. Variables that need to be passed to other hook scripts be appended to the file assigned to ${CONTAINER_ENV} , (defaults to /opt/staging/.env ). This file will be sourced for every hook script.","title":"Container Scope"},{"location":"reference/variableScoping/#scoping-example","text":"","title":"Scoping Example"},{"location":"reference/yamlFiles/","text":"Customizing YAML files \u00b6 Docker Compose uses YAML files to configure a stack's containers on startup. You can customize our YAML files or use them as the basis for creating your own. See the Docker Compose File Reference for more information. Some examples of how you can customize our YAML files: Add or change environment variables to use different server profiles. Add references to a file or files containing environment variables to pass to the container on startup. Change the wait-for times used to control the startup sequence of containers. Change the port mappings for a container. Change the release tag used for the Docker images (all product containers in the stack must use the same release tag). See Using Release Tags for more information. You'll find the YAML files for the DevOps example stacks located in your ${HOME}/projects/devops/pingidentity-devops-getting-started/11-docker-compose subdirectories. YAML File Format \u00b6 Here's the format we use for our YAML files: version : \"2.4\" services : <ping-product> : image : pingidentity/<ping-product>:${PING_IDENTITY_DEVOPS_TAG} command : wait-for <another-ping-product>:<startup-port> -t <time-to-wait> -- entrypoint.sh start-server environment : - SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_PATH=baseline/<ping-product> - PING_IDENTITY_ACCEPT_EULA=YES env_file : - ~/.pingidentity/devops #volumes: # - ${HOME}/projects/devops/volumes/full-stack.<ping-product>:/opt/out # - ${HOME}/projects/devops/pingidentity-server-profiles/baseline/<ping-product>:/opt/in ports : - <host-port>:<container-port> - <host-port>:<container-port> networks : - pingnet-dmz networks : pingnet-internal : pingnet-dmz : Entry Description version The Docker Compose version used. <ping-product> The name of the Ping Identity product container. image The build image of the product used for the container, and the build tag to use (defaults to value assigned to PING_IDENTITY_DEVOPS_TAG in the ~/.pingidentity/devops file. command We use the wait-for script to control the startup order, where <startup-port> is the port to check for whether <another-ping-product> container has started. The <time-to-wait> argument is the number of seconds to wait before executing the entrypoint.sh script with the start-server command. If you find a container is timing out while waiting for another container to start, try increasing the <time-to-wait> value. environment The environment variables being set. See Customizing Server Profiles for more information. The PING_IDENTITY_ACCEPT_EULA environment variable is set to \"YES\" when you complete the DevOps registration. This variable assignment appears here by default, but may also be in your ~/.pingidentity/devops file. env_file A file or files containing environment variable settings. The DevOps environment settings are stored in your ~/.pingidentity/devops file. You also can specify additional files containing environment settings. See Customizing Server Profiles for more information. volumes Commented out by default. The location bind mounted to the /opt/out volume is used to persist product container state and data. The location bind mounted to the /opt/in volume is used to supply server profile information to the container on startup. See Modify a server profile using local directories in Customizing Server Profiles for more information. ports The port mappings between the host and the product container. See the Ports topic in the Docker Compose File Reference for more information. networks One or more of the networks listed under the top-level networks key that the product container can use for Docker network communications. top level networks The Docker networks available for assignment to the containers in the stack. Our stacks are built to use an internal Docker network for communications between product containers ( pingnet-internal ) and an external-facing DMZ for external network communications ( pingnet-dmz ).","title":"Docker Compose YAML"},{"location":"reference/yamlFiles/#customizing-yaml-files","text":"Docker Compose uses YAML files to configure a stack's containers on startup. You can customize our YAML files or use them as the basis for creating your own. See the Docker Compose File Reference for more information. Some examples of how you can customize our YAML files: Add or change environment variables to use different server profiles. Add references to a file or files containing environment variables to pass to the container on startup. Change the wait-for times used to control the startup sequence of containers. Change the port mappings for a container. Change the release tag used for the Docker images (all product containers in the stack must use the same release tag). See Using Release Tags for more information. You'll find the YAML files for the DevOps example stacks located in your ${HOME}/projects/devops/pingidentity-devops-getting-started/11-docker-compose subdirectories.","title":"Customizing YAML files"},{"location":"reference/yamlFiles/#yaml-file-format","text":"Here's the format we use for our YAML files: version : \"2.4\" services : <ping-product> : image : pingidentity/<ping-product>:${PING_IDENTITY_DEVOPS_TAG} command : wait-for <another-ping-product>:<startup-port> -t <time-to-wait> -- entrypoint.sh start-server environment : - SERVER_PROFILE_URL=https://github.com/pingidentity/pingidentity-server-profiles.git - SERVER_PROFILE_PATH=baseline/<ping-product> - PING_IDENTITY_ACCEPT_EULA=YES env_file : - ~/.pingidentity/devops #volumes: # - ${HOME}/projects/devops/volumes/full-stack.<ping-product>:/opt/out # - ${HOME}/projects/devops/pingidentity-server-profiles/baseline/<ping-product>:/opt/in ports : - <host-port>:<container-port> - <host-port>:<container-port> networks : - pingnet-dmz networks : pingnet-internal : pingnet-dmz : Entry Description version The Docker Compose version used. <ping-product> The name of the Ping Identity product container. image The build image of the product used for the container, and the build tag to use (defaults to value assigned to PING_IDENTITY_DEVOPS_TAG in the ~/.pingidentity/devops file. command We use the wait-for script to control the startup order, where <startup-port> is the port to check for whether <another-ping-product> container has started. The <time-to-wait> argument is the number of seconds to wait before executing the entrypoint.sh script with the start-server command. If you find a container is timing out while waiting for another container to start, try increasing the <time-to-wait> value. environment The environment variables being set. See Customizing Server Profiles for more information. The PING_IDENTITY_ACCEPT_EULA environment variable is set to \"YES\" when you complete the DevOps registration. This variable assignment appears here by default, but may also be in your ~/.pingidentity/devops file. env_file A file or files containing environment variable settings. The DevOps environment settings are stored in your ~/.pingidentity/devops file. You also can specify additional files containing environment settings. See Customizing Server Profiles for more information. volumes Commented out by default. The location bind mounted to the /opt/out volume is used to persist product container state and data. The location bind mounted to the /opt/in volume is used to supply server profile information to the container on startup. See Modify a server profile using local directories in Customizing Server Profiles for more information. ports The port mappings between the host and the product container. See the Ports topic in the Docker Compose File Reference for more information. networks One or more of the networks listed under the top-level networks key that the product container can use for Docker network communications. top level networks The Docker networks available for assignment to the containers in the stack. Our stacks are built to use an internal Docker network for communications between product containers ( pingnet-internal ) and an external-facing DMZ for external network communications ( pingnet-dmz ).","title":"YAML File Format"},{"location":"release-notes/relnotes-2003/","text":"Release Notes \u00b6 DevOps Docker Builds, Version 2003 \u00b6 New Features \u00b6 PingDirectoryProxy The PingDirectoryProxy Docker image is now available. See the Ping Identity Docker Hub PingCentral The PingCentral Docker image is now available. See the Ping Identity Docker Hub Docker Compose Port Mappings We now support the Docker Compose best practice of quoting all port mappings. Docker Images (Tag: edge) We've built a pipeline to support nightly public builds of all Ping Identity Docker images using the edge tag. PingDirectory We've upgraded the PingDirectory Docker image to the current product version 8.0.0.1. PingFederate Version 10.1.0 We've built a beta PingFederate 10.1.0 Docker image. PingAccess Version 6.1.0 We've built a beta PingAccess 6.1.0 Docker image. Ping Tool Kit The Ping Tool Kit Docker image is now available. See Ping Identity Docker Hub . Both kubectl and kustomize are supported in the image. PingFederate Version 9.3 We've updated the PingFederate 9.3 Docker image to include the latest product patches. The ping-devops Utility We've added Kubernetes license secret generation, and server profile generation for PingDirectory to the ping-devops utility. See The ping-devops utility . A New Hook We've added a security start-up hook notifying administrators of keys and secrets found in the server profile. DevOps Evaluation License We've added retry functionality to attempt getting the DevOps evaluation license if the initial request fails. Product Artifacts and Extensions We've created operations to retrieve product artifacts and extensions using the DevOps credentials. Java 11 We've migrated all Alpine-based Docker images to Java 11 (Azul). PingDirectory Replication Timing We've added a profile and reference example to test PingDirectory replication timing. See the pingidentity-devops-getting-started Repo . Docker Base Image Security We've documented an evaluation of Docker base image security. See Evaluation of Docker Base Image Security . Resolved Defects \u00b6 (GDO-85) Resolved an issue where PingAccess 6.0 loaded a 5.2 license. (GDO-87) Resolved an issue where Data Console wasn't allowing users to authenticate (edge tag). (GDO-124) Resolved an issue in with pipeline where starting containers using Docker-Compose timed out. (GDO-89) Resolved an issue where *.subst template files were able to overwrite the server profile configuration. (GDO-72) Resolved an issue where motd.json did not parse correctly when the product was missing. (GDO-88) Resolved an issue where PingFederate profile metadata did not expand hostname , breaking OAuth flows. Changed \u00b6 (GDO-97) Removed WebConsole HTTP servlet from the baseline server profile. See the pingidentity-server-profiles repo . Qualified \u00b6 (GDO-42) Verified the ability to run our Docker containers as a non-root user. See Securing the Containers .","title":"Version 2003"},{"location":"release-notes/relnotes-2003/#release-notes","text":"","title":"Release Notes"},{"location":"release-notes/relnotes-2003/#devops-docker-builds-version-2003","text":"","title":"DevOps Docker Builds, Version 2003"},{"location":"release-notes/relnotes-2003/#new-features","text":"PingDirectoryProxy The PingDirectoryProxy Docker image is now available. See the Ping Identity Docker Hub PingCentral The PingCentral Docker image is now available. See the Ping Identity Docker Hub Docker Compose Port Mappings We now support the Docker Compose best practice of quoting all port mappings. Docker Images (Tag: edge) We've built a pipeline to support nightly public builds of all Ping Identity Docker images using the edge tag. PingDirectory We've upgraded the PingDirectory Docker image to the current product version 8.0.0.1. PingFederate Version 10.1.0 We've built a beta PingFederate 10.1.0 Docker image. PingAccess Version 6.1.0 We've built a beta PingAccess 6.1.0 Docker image. Ping Tool Kit The Ping Tool Kit Docker image is now available. See Ping Identity Docker Hub . Both kubectl and kustomize are supported in the image. PingFederate Version 9.3 We've updated the PingFederate 9.3 Docker image to include the latest product patches. The ping-devops Utility We've added Kubernetes license secret generation, and server profile generation for PingDirectory to the ping-devops utility. See The ping-devops utility . A New Hook We've added a security start-up hook notifying administrators of keys and secrets found in the server profile. DevOps Evaluation License We've added retry functionality to attempt getting the DevOps evaluation license if the initial request fails. Product Artifacts and Extensions We've created operations to retrieve product artifacts and extensions using the DevOps credentials. Java 11 We've migrated all Alpine-based Docker images to Java 11 (Azul). PingDirectory Replication Timing We've added a profile and reference example to test PingDirectory replication timing. See the pingidentity-devops-getting-started Repo . Docker Base Image Security We've documented an evaluation of Docker base image security. See Evaluation of Docker Base Image Security .","title":"New Features"},{"location":"release-notes/relnotes-2003/#resolved-defects","text":"(GDO-85) Resolved an issue where PingAccess 6.0 loaded a 5.2 license. (GDO-87) Resolved an issue where Data Console wasn't allowing users to authenticate (edge tag). (GDO-124) Resolved an issue in with pipeline where starting containers using Docker-Compose timed out. (GDO-89) Resolved an issue where *.subst template files were able to overwrite the server profile configuration. (GDO-72) Resolved an issue where motd.json did not parse correctly when the product was missing. (GDO-88) Resolved an issue where PingFederate profile metadata did not expand hostname , breaking OAuth flows.","title":"Resolved Defects"},{"location":"release-notes/relnotes-2003/#changed","text":"(GDO-97) Removed WebConsole HTTP servlet from the baseline server profile. See the pingidentity-server-profiles repo .","title":"Changed"},{"location":"release-notes/relnotes-2003/#qualified","text":"(GDO-42) Verified the ability to run our Docker containers as a non-root user. See Securing the Containers .","title":"Qualified"},{"location":"release-notes/relnotes-2004/","text":"Release Notes \u00b6 DevOps Docker Builds, Version 2004 \u00b6 New Features \u00b6 Hashicorp Vault We've built an integration for Hashicorp Vault. See the Deploy Hashicorp Vault . PingCentral The PingCentral Docker image is now available. See the Ping Identity Docker hub . Docker Compose We've standardized our Docker Compose references. Performance We've built a performance framework. PingFederate version 10.0.2 We've updated the PingFederate 10 Docker image for the 10.0.2 release. The ping-devops utility We've added major enhancements to our ping-devops utility. See The ping-devops Utility . PingDirectory replication We've added support for PingDirectory replication using Docker Compose. Variables and scope We've added documentation to help with understanding the effective scope of variables. See Variables and Scope . Elasticsearch SIEM stack We've added documentation for our Elasticsearch SIEM stack. See Deploy an Elasticsearch SIEM Stack . Resolved Defects \u00b6 (GDO-1) Resolved issue where users were unable to override root and admin user passwords (PingDirectory). (GDO-129) Removed the console from Ping Data products when the server profile isn't specified. (GDO-54) Resolved PingDataGovernance issues within the baseline server profile. (GDO-138) Resolved issue regarding PingDataGovernance Policy Administration Point (PAP) launch. (GDO-189) Resolved issue with PingAccess heartbeat check. (GDO-196) Replaced nslookup with getent due to issues running in Alpine. (GDO-180) Resolved issue where extension signature verification may return a false positive. (GDO-169) Resolved issues with Ping Data Console by upgrading to Tomcat 9.0.34. (GDO-166) Resolved issue with make-ldif template processing.","title":"Version 2004"},{"location":"release-notes/relnotes-2004/#release-notes","text":"","title":"Release Notes"},{"location":"release-notes/relnotes-2004/#devops-docker-builds-version-2004","text":"","title":"DevOps Docker Builds, Version 2004"},{"location":"release-notes/relnotes-2004/#new-features","text":"Hashicorp Vault We've built an integration for Hashicorp Vault. See the Deploy Hashicorp Vault . PingCentral The PingCentral Docker image is now available. See the Ping Identity Docker hub . Docker Compose We've standardized our Docker Compose references. Performance We've built a performance framework. PingFederate version 10.0.2 We've updated the PingFederate 10 Docker image for the 10.0.2 release. The ping-devops utility We've added major enhancements to our ping-devops utility. See The ping-devops Utility . PingDirectory replication We've added support for PingDirectory replication using Docker Compose. Variables and scope We've added documentation to help with understanding the effective scope of variables. See Variables and Scope . Elasticsearch SIEM stack We've added documentation for our Elasticsearch SIEM stack. See Deploy an Elasticsearch SIEM Stack .","title":"New Features"},{"location":"release-notes/relnotes-2004/#resolved-defects","text":"(GDO-1) Resolved issue where users were unable to override root and admin user passwords (PingDirectory). (GDO-129) Removed the console from Ping Data products when the server profile isn't specified. (GDO-54) Resolved PingDataGovernance issues within the baseline server profile. (GDO-138) Resolved issue regarding PingDataGovernance Policy Administration Point (PAP) launch. (GDO-189) Resolved issue with PingAccess heartbeat check. (GDO-196) Replaced nslookup with getent due to issues running in Alpine. (GDO-180) Resolved issue where extension signature verification may return a false positive. (GDO-169) Resolved issues with Ping Data Console by upgrading to Tomcat 9.0.34. (GDO-166) Resolved issue with make-ldif template processing.","title":"Resolved Defects"},{"location":"release-notes/relnotes-2005/","text":"Release Notes \u00b6 DevOps Docker Builds, Version 2005 (May 2020) \u00b6 New Features \u00b6 PingDelegator Docker Image The PingDelegator Docker image is now available. View on Docker Hub for more information. Test drive PingDelegator using the supplied docker-compose file in our Simple-Stack example. PingAccess Image Version 6.0.2 We've updated the PingAccess Image to version 6.0.2. PingFederate Version 9.3.3 We've updated the PingFederate 9.3.3 Docker image to include patch 4. Docker Builds Pipeline We've made a number of CI/CD enhancements to improve Image qualification (smoke/integration tests). Image Enhancements Improved the wait-for command to optionally wait for a path or file to become available. Resolved Defects \u00b6 (GDO-187) Resolved issue where MAX_HEAP_SIZE wasn't applied during container restart. (GDO-220) Resolved issue where log message didn't contain log file source name. (GDO-238) Resolved issue where ping-devops kubernetes start fails if DNS_ZONE variable not set. (GDO-245) Resolved issue where PingAccess didn't exit when configuration import failed. (GDO-263) Resolved issue within deploy_docs.sh which had resulted in some documentation to not be pushed to GitHub. (GDO-278) Resolved issue with PingAccess clustering Server Profile.","title":"Version 2005"},{"location":"release-notes/relnotes-2005/#release-notes","text":"","title":"Release Notes"},{"location":"release-notes/relnotes-2005/#devops-docker-builds-version-2005-may-2020","text":"","title":"DevOps Docker Builds, Version 2005 (May 2020)"},{"location":"release-notes/relnotes-2005/#new-features","text":"PingDelegator Docker Image The PingDelegator Docker image is now available. View on Docker Hub for more information. Test drive PingDelegator using the supplied docker-compose file in our Simple-Stack example. PingAccess Image Version 6.0.2 We've updated the PingAccess Image to version 6.0.2. PingFederate Version 9.3.3 We've updated the PingFederate 9.3.3 Docker image to include patch 4. Docker Builds Pipeline We've made a number of CI/CD enhancements to improve Image qualification (smoke/integration tests). Image Enhancements Improved the wait-for command to optionally wait for a path or file to become available.","title":"New Features"},{"location":"release-notes/relnotes-2005/#resolved-defects","text":"(GDO-187) Resolved issue where MAX_HEAP_SIZE wasn't applied during container restart. (GDO-220) Resolved issue where log message didn't contain log file source name. (GDO-238) Resolved issue where ping-devops kubernetes start fails if DNS_ZONE variable not set. (GDO-245) Resolved issue where PingAccess didn't exit when configuration import failed. (GDO-263) Resolved issue within deploy_docs.sh which had resulted in some documentation to not be pushed to GitHub. (GDO-278) Resolved issue with PingAccess clustering Server Profile.","title":"Resolved Defects"},{"location":"release-notes/relnotes-2006/","text":"Release Notes \u00b6 DevOps Docker Builds, Version 2006 (June 2020) \u00b6 New Features \u00b6 Docker Compose Volumes Applications that create and manage configuration now have mounted volumes in Docker-Compose Examples , ensuring that your configuration changes are persisted across restarted. PingAccess Image Enhancements We've updated the PingAccess Image to support the new features available in version 6.1. Customer Support Data Collection Included in this release is the Java diagnostic tool to enable embedded customer support data collection. This tool set includes jstat , jmap and jhat . New Product Versions \u00b6 The following new product versions are available using edge , latest and 2006 image tags: PingFederate 10.1.0 PingAccess 6.1.0 PingDirectory 8.1.0.0 PingDirectoryProxy 8.1.0.0 PingDataGovernance 8.1.0.0 PingDataGovernance 8.1.0.0 PAP PingDataSync 8.1.0.0 PingCentral 1.4.0 Improvements \u00b6 Liveness Check We've made improvements to PingDirectory's liveness check to better inform dependant services on the status of the Directory service. Docker Build Pipeline We've published Documentation on how to build a Ping Identity Docker Image using a local zip artifact. We have improved our reference pipeline to allow for the build of a single product. We've made several CI/CD enhancements to improve Image qualification (smoke/integration tests). Configuration Substitution We've made enhancements to explicitly send the variables to be substituted. Resolved Defects \u00b6 (GDO-218) Resolved an issue where PingDirectory threw an error on manage-profile during setup. (GDO-289) Resolved an issue where Alpine based image couldn't install pip3. (GDO-329) Resolved an issue where PingCentral docs were not syncing to GitHub.","title":"Version 2006"},{"location":"release-notes/relnotes-2006/#release-notes","text":"","title":"Release Notes"},{"location":"release-notes/relnotes-2006/#devops-docker-builds-version-2006-june-2020","text":"","title":"DevOps Docker Builds, Version 2006 (June 2020)"},{"location":"release-notes/relnotes-2006/#new-features","text":"Docker Compose Volumes Applications that create and manage configuration now have mounted volumes in Docker-Compose Examples , ensuring that your configuration changes are persisted across restarted. PingAccess Image Enhancements We've updated the PingAccess Image to support the new features available in version 6.1. Customer Support Data Collection Included in this release is the Java diagnostic tool to enable embedded customer support data collection. This tool set includes jstat , jmap and jhat .","title":"New Features"},{"location":"release-notes/relnotes-2006/#new-product-versions","text":"The following new product versions are available using edge , latest and 2006 image tags: PingFederate 10.1.0 PingAccess 6.1.0 PingDirectory 8.1.0.0 PingDirectoryProxy 8.1.0.0 PingDataGovernance 8.1.0.0 PingDataGovernance 8.1.0.0 PAP PingDataSync 8.1.0.0 PingCentral 1.4.0","title":"New Product Versions"},{"location":"release-notes/relnotes-2006/#improvements","text":"Liveness Check We've made improvements to PingDirectory's liveness check to better inform dependant services on the status of the Directory service. Docker Build Pipeline We've published Documentation on how to build a Ping Identity Docker Image using a local zip artifact. We have improved our reference pipeline to allow for the build of a single product. We've made several CI/CD enhancements to improve Image qualification (smoke/integration tests). Configuration Substitution We've made enhancements to explicitly send the variables to be substituted.","title":"Improvements"},{"location":"release-notes/relnotes-2006/#resolved-defects","text":"(GDO-218) Resolved an issue where PingDirectory threw an error on manage-profile during setup. (GDO-289) Resolved an issue where Alpine based image couldn't install pip3. (GDO-329) Resolved an issue where PingCentral docs were not syncing to GitHub.","title":"Resolved Defects"},{"location":"release-notes/relnotes-2007/","text":"Release Notes \u00b6 DevOps Docker Builds, Version 2007 (July 2020) \u00b6 New Features \u00b6 Signed Docker Images All DockerHub Images are now signed and conform to the Docker Content Trust Specification . Variablize PingAccess Ports We've updated the PingAccess start up hooks to allow users to customize application ports. PingAccess Upgrade Utility The PingAccess upgrade utility is now part of Docker Image. Certificate Management Add consistency and flexibility with the injection of certs/pins. Docker Image Startup Flexibility We've added the ability for end users to customize the startup sequence for Docker Images using pre and post hooks. See our Documentation for implementation details. Improvements \u00b6 Docker Build Pipeline We've made several CI/CD enhancements to improve Image qualification (smoke/integration tests). Resolved Defects \u00b6 (GDO-345) Resolved issue where PingDelegator was using PRIVATE rather than PUBLIC hostnames. (GDO-346) Resolved issue regarding the default minimum heap for PingDirectory. (GDO-380) Resolved issue within PingAccess Clustering (Admin Console) Kubernetes examples. (GDO-371) Resolved issue where PingDelegator wouldn't start using non-privileged user.","title":"Version 2007"},{"location":"release-notes/relnotes-2007/#release-notes","text":"","title":"Release Notes"},{"location":"release-notes/relnotes-2007/#devops-docker-builds-version-2007-july-2020","text":"","title":"DevOps Docker Builds, Version 2007 (July 2020)"},{"location":"release-notes/relnotes-2007/#new-features","text":"Signed Docker Images All DockerHub Images are now signed and conform to the Docker Content Trust Specification . Variablize PingAccess Ports We've updated the PingAccess start up hooks to allow users to customize application ports. PingAccess Upgrade Utility The PingAccess upgrade utility is now part of Docker Image. Certificate Management Add consistency and flexibility with the injection of certs/pins. Docker Image Startup Flexibility We've added the ability for end users to customize the startup sequence for Docker Images using pre and post hooks. See our Documentation for implementation details.","title":"New Features"},{"location":"release-notes/relnotes-2007/#improvements","text":"Docker Build Pipeline We've made several CI/CD enhancements to improve Image qualification (smoke/integration tests).","title":"Improvements"},{"location":"release-notes/relnotes-2007/#resolved-defects","text":"(GDO-345) Resolved issue where PingDelegator was using PRIVATE rather than PUBLIC hostnames. (GDO-346) Resolved issue regarding the default minimum heap for PingDirectory. (GDO-380) Resolved issue within PingAccess Clustering (Admin Console) Kubernetes examples. (GDO-371) Resolved issue where PingDelegator wouldn't start using non-privileged user.","title":"Resolved Defects"},{"location":"release-notes/relnotes-2008/","text":"Release Notes \u00b6 DevOps Docker Builds, Version 2008 (August 2020) \u00b6 New Features \u00b6 Secret Management A number of key enhancements have been made to natively support secret management within our Docker Images. See Documentation for implementation details. DevOps Development Mode We've added a 'Continue on Failure' option to all Docker Images. This allows the Container to say alive while any potential issues are being investigated. DevOps Program Registration Signing up for the Ping DevOps program is now self-service! Simply follow the instructions found Here . Improvements \u00b6 Ping-DevOps Utility We've added secret management commands to ping-devops, allowing you to quickly integrate secrets into your deployments. Image Restart State A number of enhancements have been made to improve the overall restart flow in our Docker Images. Resolved Defects \u00b6 (GDO-352) Resolved restart issue in PingDataGovernance PAP. (GDO-392) Resolved issue within PingDelegator when DS_PORT variable was undefined. (GDO-395) Resolved issue within PingDirectory restart when Java versions changed. (GDO-397) Resolved issue where PingFederate failed to start in Kubernetes using the full-stack example. (GDO-404) Resolved issue where some users were unable to log into the PingAccess console using the Image edge tag and Baseline server profile.","title":"Version 2008"},{"location":"release-notes/relnotes-2008/#release-notes","text":"","title":"Release Notes"},{"location":"release-notes/relnotes-2008/#devops-docker-builds-version-2008-august-2020","text":"","title":"DevOps Docker Builds, Version 2008 (August 2020)"},{"location":"release-notes/relnotes-2008/#new-features","text":"Secret Management A number of key enhancements have been made to natively support secret management within our Docker Images. See Documentation for implementation details. DevOps Development Mode We've added a 'Continue on Failure' option to all Docker Images. This allows the Container to say alive while any potential issues are being investigated. DevOps Program Registration Signing up for the Ping DevOps program is now self-service! Simply follow the instructions found Here .","title":"New Features"},{"location":"release-notes/relnotes-2008/#improvements","text":"Ping-DevOps Utility We've added secret management commands to ping-devops, allowing you to quickly integrate secrets into your deployments. Image Restart State A number of enhancements have been made to improve the overall restart flow in our Docker Images.","title":"Improvements"},{"location":"release-notes/relnotes-2008/#resolved-defects","text":"(GDO-352) Resolved restart issue in PingDataGovernance PAP. (GDO-392) Resolved issue within PingDelegator when DS_PORT variable was undefined. (GDO-395) Resolved issue within PingDirectory restart when Java versions changed. (GDO-397) Resolved issue where PingFederate failed to start in Kubernetes using the full-stack example. (GDO-404) Resolved issue where some users were unable to log into the PingAccess console using the Image edge tag and Baseline server profile.","title":"Resolved Defects"},{"location":"release-notes/relnotes-2009/","text":"Release Notes \u00b6 Devops Docker Builds, Version 2009 (September 2020) \u00b6 New Features \u00b6 PingFederate Multi-region Clustering We've published our Reference Architecture for deploying PingFederate across multiple AWS regions using Kubernetes. PingDataSync Clustering Within PingDataSync 8.2.0.0-EA we've introduced clustering, ensuring your deployment is highly available. Certificate Management Usage We've added documentation for DevOps Certificate Management . PingAccess Release \u00b6 PingAccess 6.1.2 is now available using edge , latest and 2009 image tags Product Betas and Release Candidates \u00b6 Looking to see what the next official product release will contain? Start using the beta and early access builds today. PingFederate 10.2.0-Beta PingAccess 6.2.0-Beta PingDirectory 8.2.0.0-EA PingDirectoryProxy 8.2.0.0-EA PingDataGovernance 8.2.0.0-EA PingDataGovernance 8.2.0.0-EA PAP PingDataSync 8.2.0.0-EA Improvements \u00b6 Image Hardening We've updated our Image hardening Guide to help secure your production deployments.","title":"Version 2009"},{"location":"release-notes/relnotes-2009/#release-notes","text":"","title":"Release Notes"},{"location":"release-notes/relnotes-2009/#devops-docker-builds-version-2009-september-2020","text":"","title":"Devops Docker Builds, Version 2009 (September 2020)"},{"location":"release-notes/relnotes-2009/#new-features","text":"PingFederate Multi-region Clustering We've published our Reference Architecture for deploying PingFederate across multiple AWS regions using Kubernetes. PingDataSync Clustering Within PingDataSync 8.2.0.0-EA we've introduced clustering, ensuring your deployment is highly available. Certificate Management Usage We've added documentation for DevOps Certificate Management .","title":"New Features"},{"location":"release-notes/relnotes-2009/#pingaccess-release","text":"PingAccess 6.1.2 is now available using edge , latest and 2009 image tags","title":"PingAccess Release"},{"location":"release-notes/relnotes-2009/#product-betas-and-release-candidates","text":"Looking to see what the next official product release will contain? Start using the beta and early access builds today. PingFederate 10.2.0-Beta PingAccess 6.2.0-Beta PingDirectory 8.2.0.0-EA PingDirectoryProxy 8.2.0.0-EA PingDataGovernance 8.2.0.0-EA PingDataGovernance 8.2.0.0-EA PAP PingDataSync 8.2.0.0-EA","title":"Product Betas and Release Candidates"},{"location":"release-notes/relnotes-2009/#improvements","text":"Image Hardening We've updated our Image hardening Guide to help secure your production deployments.","title":"Improvements"},{"location":"release-notes/relnotes-2010/","text":"Release Notes \u00b6 Devops Docker Builds, Version 2010 (October 2020) \u00b6 New Features \u00b6 PingIdentity Helm Charts Looking to deploy the PingDevOps stack into your Kubernetes cluster? We've published our Helm Charts to help streamline deployment. PingIntelligence (ASE) Docker Image PingIntelligence (ASE) is now available on DockerHub! Pull the 4.3 ASE image Here . PingFederate Bulk API Configuration Management We've added tooling and documentation for managing PingFederate configuration using the build API export and import. View the latest documentation Here . Enhancements \u00b6 PingFederate Version 10.0.6 now available. Image now includes tcp.xml.subst for cluster parameterization. Updated image to support easier enablement/use of Bouncy Castle FIPS provider with PingFederate. PingAccess Version 6.1.3 is now available. LDAP SDK Updated to version 5.1.1 ping-devops CLI Added functionality to generate K8s license and version secret directly from the evaluation license service. Added ACCEPT_EULA value to K8s devops-secret. Resolved Defects \u00b6 (GDO-411) Resolved issue where access token was logged when using private Git repository. (GDO-444) Resolved PingDirectory issue with keystore exception on restart. (GDO-491) Removed GPG from base Docker image. (GDO-495) Removed gosu from base Docker image. (GDO-513) Resolved issue with replication topology list on PingDirectory restart.","title":"Version 2010"},{"location":"release-notes/relnotes-2010/#release-notes","text":"","title":"Release Notes"},{"location":"release-notes/relnotes-2010/#devops-docker-builds-version-2010-october-2020","text":"","title":"Devops Docker Builds, Version 2010 (October 2020)"},{"location":"release-notes/relnotes-2010/#new-features","text":"PingIdentity Helm Charts Looking to deploy the PingDevOps stack into your Kubernetes cluster? We've published our Helm Charts to help streamline deployment. PingIntelligence (ASE) Docker Image PingIntelligence (ASE) is now available on DockerHub! Pull the 4.3 ASE image Here . PingFederate Bulk API Configuration Management We've added tooling and documentation for managing PingFederate configuration using the build API export and import. View the latest documentation Here .","title":"New Features"},{"location":"release-notes/relnotes-2010/#enhancements","text":"PingFederate Version 10.0.6 now available. Image now includes tcp.xml.subst for cluster parameterization. Updated image to support easier enablement/use of Bouncy Castle FIPS provider with PingFederate. PingAccess Version 6.1.3 is now available. LDAP SDK Updated to version 5.1.1 ping-devops CLI Added functionality to generate K8s license and version secret directly from the evaluation license service. Added ACCEPT_EULA value to K8s devops-secret.","title":"Enhancements"},{"location":"release-notes/relnotes-2010/#resolved-defects","text":"(GDO-411) Resolved issue where access token was logged when using private Git repository. (GDO-444) Resolved PingDirectory issue with keystore exception on restart. (GDO-491) Removed GPG from base Docker image. (GDO-495) Removed gosu from base Docker image. (GDO-513) Resolved issue with replication topology list on PingDirectory restart.","title":"Resolved Defects"},{"location":"release-notes/relnotes-2011/","text":"Release Notes \u00b6 Devops Docker Builds, Version 2011 (November 2020) \u00b6 New Features \u00b6 Internal XRay Scanning We've automated the process to scan all Sprint Release Docker Images for CVE's Enhancements \u00b6 PingFederate Version 10.1.3 now available. Parameterized run.properties, ldap.properties and tcp.xml now included in Docker Image Helm Charts We added a number of enhancements to our Helm charts. See the Helm Release Notes for details. Misc. Updated EULA check to be case insensitive Add Java back into pingtoolkit Image Updated example docker run commands in Dockerfile documentation Info message when Server Profile URLs are not present Resolved Defects \u00b6 (GDO-549) - Resolved issue where SCIM Swagger test pages don't work in PingDataGovernance Docker Image (GDO-567) - Resolved issue where changes made to PingDirectory's java.properties were erased on container restart (GDO-599) - Change wait-for localhost to use IP address (GDO-604) - Modified simple-sync server profile to work in Kubernetes environment with different service names (GDO-606) - Resolved issue where copy of server bits throws errors when running under non-root security context","title":"Version 2011"},{"location":"release-notes/relnotes-2011/#release-notes","text":"","title":"Release Notes"},{"location":"release-notes/relnotes-2011/#devops-docker-builds-version-2011-november-2020","text":"","title":"Devops Docker Builds, Version 2011 (November 2020)"},{"location":"release-notes/relnotes-2011/#new-features","text":"Internal XRay Scanning We've automated the process to scan all Sprint Release Docker Images for CVE's","title":"New Features"},{"location":"release-notes/relnotes-2011/#enhancements","text":"PingFederate Version 10.1.3 now available. Parameterized run.properties, ldap.properties and tcp.xml now included in Docker Image Helm Charts We added a number of enhancements to our Helm charts. See the Helm Release Notes for details. Misc. Updated EULA check to be case insensitive Add Java back into pingtoolkit Image Updated example docker run commands in Dockerfile documentation Info message when Server Profile URLs are not present","title":"Enhancements"},{"location":"release-notes/relnotes-2011/#resolved-defects","text":"(GDO-549) - Resolved issue where SCIM Swagger test pages don't work in PingDataGovernance Docker Image (GDO-567) - Resolved issue where changes made to PingDirectory's java.properties were erased on container restart (GDO-599) - Change wait-for localhost to use IP address (GDO-604) - Modified simple-sync server profile to work in Kubernetes environment with different service names (GDO-606) - Resolved issue where copy of server bits throws errors when running under non-root security context","title":"Resolved Defects"},{"location":"release-notes/relnotes-2012/","text":"Release Notes \u00b6 Devops Docker Builds, Version 2012 (December 2020) \u00b6 New Features \u00b6 DevOps Documentation We've moved from GitBook to MKDocs to provide a richer DevOps documentation experience. Enhancements \u00b6 PingFederate Version 10.2 now available. PingAccess Version 6.2 is now available. PingDirectory Version 8.2.0 is now available. PingDataGovernance Version 8.2.0 is now available. PingDataSync Version 8.2.0 is now available. PingCentral Version 1.6.0 is now available. LDAP SDK Version 5.1.3 is now available. Updated to latest Tomcat version. PingData Console SSO Example We've provided an example of running the Admin Console in Docker with SSO configured. Resolved Defects \u00b6 (GDO-362) Resolved issue where PingDirectory instances become active prior to being fully synchronized. (GDO-502) Resolved potential vulnerability by updating Ping Data products to Spring Framework v4.3.29. (GDO-544) Resolved issue where PingDataGovernance PAP images' MAX_HEAP_SIZE variable had no effect. (GDO-618) Resolved issue where base layer was missing JMX agent. (GDO-640) Resolved issue where wait-for command didn't honor timeout when waiting for host:port. Product Build Matrix \u00b6 The following table includes product versions and their accompanying Image build status for this release. Product Active Build Build EOL PingAccess 6.2.0 6.1.3 6.0.4 PingCentral 1.6.0 1.5.0 PingDataConsole 8.2.0.0 8.1.0.0 8.0.0.1 PingDataGovernance 8.2.0.0 8.1.0.0 8.0.0.1 PingDataGovernance PAP 8.2.0.0 8.1.0.0 8.0.0.1 PingDataSync 8.2.0.0 8.1.0.0 8.0.0.1 PingDelegator 4.4.0 4.2.1 PingDirectory 8.2.0.0 8.1.0.0 8.0.0.1 PingDirectoryProxy 8.2.0.0 8.1.0.0 8.0.0.1 PingFederate 10.2.0 10.1.3 10.1.2 PingIntelligence 4.4 4.3 Build Matrix Info Bolded product version number is version within 'latest' image tag. Build EOL denotes product versions that are no longer built as of this release.","title":"Version 2012"},{"location":"release-notes/relnotes-2012/#release-notes","text":"","title":"Release Notes"},{"location":"release-notes/relnotes-2012/#devops-docker-builds-version-2012-december-2020","text":"","title":"Devops Docker Builds, Version 2012 (December 2020)"},{"location":"release-notes/relnotes-2012/#new-features","text":"DevOps Documentation We've moved from GitBook to MKDocs to provide a richer DevOps documentation experience.","title":"New Features"},{"location":"release-notes/relnotes-2012/#enhancements","text":"PingFederate Version 10.2 now available. PingAccess Version 6.2 is now available. PingDirectory Version 8.2.0 is now available. PingDataGovernance Version 8.2.0 is now available. PingDataSync Version 8.2.0 is now available. PingCentral Version 1.6.0 is now available. LDAP SDK Version 5.1.3 is now available. Updated to latest Tomcat version. PingData Console SSO Example We've provided an example of running the Admin Console in Docker with SSO configured.","title":"Enhancements"},{"location":"release-notes/relnotes-2012/#resolved-defects","text":"(GDO-362) Resolved issue where PingDirectory instances become active prior to being fully synchronized. (GDO-502) Resolved potential vulnerability by updating Ping Data products to Spring Framework v4.3.29. (GDO-544) Resolved issue where PingDataGovernance PAP images' MAX_HEAP_SIZE variable had no effect. (GDO-618) Resolved issue where base layer was missing JMX agent. (GDO-640) Resolved issue where wait-for command didn't honor timeout when waiting for host:port.","title":"Resolved Defects"},{"location":"release-notes/relnotes-2012/#product-build-matrix","text":"The following table includes product versions and their accompanying Image build status for this release. Product Active Build Build EOL PingAccess 6.2.0 6.1.3 6.0.4 PingCentral 1.6.0 1.5.0 PingDataConsole 8.2.0.0 8.1.0.0 8.0.0.1 PingDataGovernance 8.2.0.0 8.1.0.0 8.0.0.1 PingDataGovernance PAP 8.2.0.0 8.1.0.0 8.0.0.1 PingDataSync 8.2.0.0 8.1.0.0 8.0.0.1 PingDelegator 4.4.0 4.2.1 PingDirectory 8.2.0.0 8.1.0.0 8.0.0.1 PingDirectoryProxy 8.2.0.0 8.1.0.0 8.0.0.1 PingFederate 10.2.0 10.1.3 10.1.2 PingIntelligence 4.4 4.3 Build Matrix Info Bolded product version number is version within 'latest' image tag. Build EOL denotes product versions that are no longer built as of this release.","title":"Product Build Matrix"},{"location":"release-notes/relnotes-2101/","text":"Release Notes \u00b6 Devops Docker Builds, Version 2101 (January 2021) \u00b6 Enhancements \u00b6 PingFederate Versions 10.2.1 and 10.1.4 are now available. PingDirectory Versions 8.2.0.1 and 8.1.0.3 are now available. PingDirectory now delays its readiness state until replication has completed (Kubernetes). Improved container restart time by regenerating java.properties only when changes are made to JVM or JVM options. PingDataGovernance Versions 8.2.0.1 and 8.1.0.3 are now available. PingDataSync Versions 8.2.0.1 and 8.1.0.3 are now available. PingDelegator 4.4.1 Version 4.4.1 is now available. LDAP SDK Version 5.1.3 is now available. Container Secrets Sourcing of secret_envs is now recursive. Resolved Defects \u00b6 (GDO-577) - Resolved issue to suppress environment variables in cn=monitor for PingData products. (GDO-658) - Enhanced error messages returned by the evaluation license service. (GDO-659) - Resolved issue where evaluation license server used incorrect calculation for checking image expiration. (GDO-668) - Resolved issue where remnants of previous server profile remained in place when restarting a container. (GDO-674) - Resolved issue where hashing contents of the SECRETS_DIR risked leaving passwords stored insecurely on the container filesystem. Product Build Matrix \u00b6 The following table includes product versions and their accompanying Image build status for this release. Product Active Build Build EOL PingAccess 6.2.0 6.1.3 PingCentral 1.6.0 1.5.0 PingDataConsole 8.2.0.1 8.1.0.3 8.2.0.0 8.1.0.0 PingDataGovernance 8.2.0.1 8.1.0.3 8.2.0.0 8.1.0.0 PingDataGovernance PAP 8.2.0.1 8.1.0.3 8.2.0.0 8.1.0.0 PingDataSync 8.2.0.1 8.1.0.3 8.2.0.0 8.1.0.0 PingDelegator 4.4.0 4.2.1 PingDirectory 8.2.0.1 8.1.0.3 8.2.0.0 8.1.0.0 PingDirectoryProxy 8.2.0.1 8.1.0.3 8.2.0.0 8.1.0.0 PingFederate 10.2.1 10.1.4 10.2.0 10.1.3 PingIntelligence 4.4 Build Matrix Info Bolded product version number is version within 'latest' image tag. Build EOL denotes product versions that are no longer built as of this release.","title":"Version 2101"},{"location":"release-notes/relnotes-2101/#release-notes","text":"","title":"Release Notes"},{"location":"release-notes/relnotes-2101/#devops-docker-builds-version-2101-january-2021","text":"","title":"Devops Docker Builds, Version 2101 (January 2021)"},{"location":"release-notes/relnotes-2101/#enhancements","text":"PingFederate Versions 10.2.1 and 10.1.4 are now available. PingDirectory Versions 8.2.0.1 and 8.1.0.3 are now available. PingDirectory now delays its readiness state until replication has completed (Kubernetes). Improved container restart time by regenerating java.properties only when changes are made to JVM or JVM options. PingDataGovernance Versions 8.2.0.1 and 8.1.0.3 are now available. PingDataSync Versions 8.2.0.1 and 8.1.0.3 are now available. PingDelegator 4.4.1 Version 4.4.1 is now available. LDAP SDK Version 5.1.3 is now available. Container Secrets Sourcing of secret_envs is now recursive.","title":"Enhancements"},{"location":"release-notes/relnotes-2101/#resolved-defects","text":"(GDO-577) - Resolved issue to suppress environment variables in cn=monitor for PingData products. (GDO-658) - Enhanced error messages returned by the evaluation license service. (GDO-659) - Resolved issue where evaluation license server used incorrect calculation for checking image expiration. (GDO-668) - Resolved issue where remnants of previous server profile remained in place when restarting a container. (GDO-674) - Resolved issue where hashing contents of the SECRETS_DIR risked leaving passwords stored insecurely on the container filesystem.","title":"Resolved Defects"},{"location":"release-notes/relnotes-2101/#product-build-matrix","text":"The following table includes product versions and their accompanying Image build status for this release. Product Active Build Build EOL PingAccess 6.2.0 6.1.3 PingCentral 1.6.0 1.5.0 PingDataConsole 8.2.0.1 8.1.0.3 8.2.0.0 8.1.0.0 PingDataGovernance 8.2.0.1 8.1.0.3 8.2.0.0 8.1.0.0 PingDataGovernance PAP 8.2.0.1 8.1.0.3 8.2.0.0 8.1.0.0 PingDataSync 8.2.0.1 8.1.0.3 8.2.0.0 8.1.0.0 PingDelegator 4.4.0 4.2.1 PingDirectory 8.2.0.1 8.1.0.3 8.2.0.0 8.1.0.0 PingDirectoryProxy 8.2.0.1 8.1.0.3 8.2.0.0 8.1.0.0 PingFederate 10.2.1 10.1.4 10.2.0 10.1.3 PingIntelligence 4.4 Build Matrix Info Bolded product version number is version within 'latest' image tag. Build EOL denotes product versions that are no longer built as of this release.","title":"Product Build Matrix"},{"location":"release-notes/relnotes-2102/","text":"Release Notes \u00b6 Devops Docker Builds, Version 2102 (February 2021) \u00b6 Enhancements \u00b6 PingFederate Support for creation and loading of certificates for admin. Version 10.2.2 is now available. PingAccess Baseline now has clustering support. Version 6.1.4 is now available. PingDirectory Improve speed of replace-profile process during PingDirectory restart. Indexes are automatically rebuilt upon server restart. Version 8.2.0.2 is now available. PingDataGovernance Helm charts have been added for the PingDataGovernance policy editor. Version 8.2.0.2 is now available. PingDataSync Version 8.2.0.2 is now available. Resolved Defects \u00b6 (GDO-382) - Resolved issue where PingDirectory is unable to restart when upgrading 7.3 to 8.1 due to a license error. (GDO-543) - Updated \"Related Docker Images\" documentation in PAP Dockerfile. (GDO-672) - Resolved issue with 'manage-profile setup' signaling a dsconfig error. (GDO-680) - Resolved issue with PingDirectory set_server_available and set_server_unavailable methods being very. (GDO-311) - Updated 05-expand-templates.sh to no longer build data.zip if a data.zip directory is found in the profile. Product Build Matrix \u00b6 The following table includes product versions and their accompanying Image build status for this release. Product Active Build Build EOL PingAccess 6.2.0 6.1.4 6.1.3 PingCentral 1.6.0 1.5.0 PingDataConsole 8.2.0.2 8.1.0.3 8.2.0.1 PingDataGovernance 8.2.0.2 8.1.0.3 8.2.0.1 PingDataGovernance PAP 8.2.0.2 8.1.0.3 8.2.0.1 PingDataSync 8.2.0.2 8.1.0.3 8.2.0.1 PingDelegator 4.4.0 4.2.1 PingDirectory 8.2.0.2 8.1.0.3 8.2.0.1 PingDirectoryProxy 8.2.0.2 8.1.0.3 8.2.0.1 PingFederate 10.2.2 10.1.4 10.2.1 PingIntelligence 4.4 Build Matrix Info Bolded product version number is version within 'latest' image tag. Build EOL denotes product versions that are no longer built as of this release.","title":"Current"},{"location":"release-notes/relnotes-2102/#release-notes","text":"","title":"Release Notes"},{"location":"release-notes/relnotes-2102/#devops-docker-builds-version-2102-february-2021","text":"","title":"Devops Docker Builds, Version 2102 (February 2021)"},{"location":"release-notes/relnotes-2102/#enhancements","text":"PingFederate Support for creation and loading of certificates for admin. Version 10.2.2 is now available. PingAccess Baseline now has clustering support. Version 6.1.4 is now available. PingDirectory Improve speed of replace-profile process during PingDirectory restart. Indexes are automatically rebuilt upon server restart. Version 8.2.0.2 is now available. PingDataGovernance Helm charts have been added for the PingDataGovernance policy editor. Version 8.2.0.2 is now available. PingDataSync Version 8.2.0.2 is now available.","title":"Enhancements"},{"location":"release-notes/relnotes-2102/#resolved-defects","text":"(GDO-382) - Resolved issue where PingDirectory is unable to restart when upgrading 7.3 to 8.1 due to a license error. (GDO-543) - Updated \"Related Docker Images\" documentation in PAP Dockerfile. (GDO-672) - Resolved issue with 'manage-profile setup' signaling a dsconfig error. (GDO-680) - Resolved issue with PingDirectory set_server_available and set_server_unavailable methods being very. (GDO-311) - Updated 05-expand-templates.sh to no longer build data.zip if a data.zip directory is found in the profile.","title":"Resolved Defects"},{"location":"release-notes/relnotes-2102/#product-build-matrix","text":"The following table includes product versions and their accompanying Image build status for this release. Product Active Build Build EOL PingAccess 6.2.0 6.1.4 6.1.3 PingCentral 1.6.0 1.5.0 PingDataConsole 8.2.0.2 8.1.0.3 8.2.0.1 PingDataGovernance 8.2.0.2 8.1.0.3 8.2.0.1 PingDataGovernance PAP 8.2.0.2 8.1.0.3 8.2.0.1 PingDataSync 8.2.0.2 8.1.0.3 8.2.0.1 PingDelegator 4.4.0 4.2.1 PingDirectory 8.2.0.2 8.1.0.3 8.2.0.1 PingDirectoryProxy 8.2.0.2 8.1.0.3 8.2.0.1 PingFederate 10.2.2 10.1.4 10.2.1 PingIntelligence 4.4 Build Matrix Info Bolded product version number is version within 'latest' image tag. Build EOL denotes product versions that are no longer built as of this release.","title":"Product Build Matrix"}]}